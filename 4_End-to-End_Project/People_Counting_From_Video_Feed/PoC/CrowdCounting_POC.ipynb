{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Crowds with Deep Learning\n",
    "## Proof Of Concept\n",
    "\n",
    "The notebook will implement various papers for the puprose of crowd counting\n",
    "* [Dense Scale Networks](https://arxiv.org/pdf/1906.09707.pdf)\n",
    "* [CSRNet: Dilated Convolutional Neural Networks](https://arxiv.org/pdf/1802.10062.pdf)\n",
    "\n",
    "The goal - to find the best approach to teach a model to count crowds, based on input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import h5py\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from skimage import exposure, img_as_float\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting module\n",
    "from ovreport.report import report_to_overwatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Globals\n",
    "A number of parameters on top of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "train_path_UCF_QNRF = 'training_dataset/UCF-QNRF_ECCV18/Train_h5/'\n",
    "test_path_UCF_QNRF = 'training_dataset/UCF-QNRF_ECCV18/Test_h5/'\n",
    "train_path_SGH = 'training_dataset/ShanghaiTech/train_h5/'\n",
    "test_path_SGH = 'training_dataset/ShanghaiTech/test_h5/'\n",
    "# TODO: Add More ... add more\n",
    "\n",
    "# Target Image Size\n",
    "TARGET_SHAPE = (720, 480)\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_SAVE_PATH = 'models/best/'\n",
    "CKP_MODEL_SAVE_PATH = 'models/checkpoints/'\n",
    "# Training Details\n",
    "PRETRAINED_BACKBONE = True\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dealing with Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset(Dataset):\n",
    "    def __init__(self, root, transform, ratio=8, output_shape=False, aug=False):\n",
    "        self.nsamples = len(root)\n",
    "        self.aug = aug\n",
    "        self.output_shape = output_shape\n",
    "        self.root = root\n",
    "        self.ratio = ratio\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __augment(image, target, count, seed):\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # apply random crop\n",
    "        if random.random() < 0.5:\n",
    "            crop_size = (img.size[0]//2, img.size[1]//2)\n",
    "        \n",
    "            if random.random() <=0.44:\n",
    "                # 4 non-overlapping patches\n",
    "                dx = int(random.randint(0,1) * crop_size[0])\n",
    "                dy = int(random.randint(0,1) * crop_size[1])\n",
    "            else:\n",
    "                # 5 random patches\n",
    "                # set seed to ensure for each image the random patches are certain\n",
    "                # if not set, the crop will be online which means the patches change every time loading, leading to a dynamic training set.\n",
    "                patch_id = random.randint(0, 4)\n",
    "                random.seed(index + patch_id * 0.1)\n",
    "                dx = int(random.random() * crop_size[0])\n",
    "                random.seed(index + 0.5 + patch_id * 0.1)\n",
    "                dy = int(random.random() * crop_size[1])\n",
    "            # crop\n",
    "            img = img.crop((dx, dy, crop_size[0]+dx, crop_size[1]+dy))\n",
    "            target = target[dy:crop_size[1]+dy, dx:crop_size[0]+dx]\n",
    "            count = float(target.sum())\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            target = np.fliplr(target)\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        if random.random() > 0.7:\n",
    "            img = img_as_float(image)\n",
    "            # gamma_img: np.array(dtype=float64) ranging [0,1]\n",
    "            if random.random() > 0.5:\n",
    "                gamma_img = exposure.adjust_gamma(img, 1.5)\n",
    "            else:\n",
    "                gamma_img = exposure.adjust_gamma(img, 0.5)\n",
    "            gamma_img = gamma_img * 255\n",
    "            gamma_img = np.uint8(gamma_img)\n",
    "            image = Image.fromarray(gamma_img)\n",
    "        \n",
    "        return image, target, count\n",
    "    \n",
    "    def __resize_to_target(self, img, target_shape):\n",
    "        return cv2.resize(img, target_shape, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    def __load_data(self, path, ratio=8, output_shape=None, aug=False, index=None):\n",
    "        src_h5 = h5py.File(path, 'r')\n",
    "        img = src_h5['image_array'].value\n",
    "        output = src_h5['density_map'].value\n",
    "        count = float(src_h5['count'].value)\n",
    "\n",
    "\n",
    "        if len(img.shape) < 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        if output_shape is not None:\n",
    "            img = self.__resize_to_target(img, output_shape)\n",
    "            output = self.__resize_to_target(output, output_shape)\n",
    "\n",
    "        if aug:\n",
    "            # TODO: Implement augumentation\n",
    "            img, output, count = self.__augment(img, output, count, 42)\n",
    "\n",
    "        if self.ratio>1:\n",
    "            output = cv2.resize(output, \n",
    "                                (int(output.shape[1]/ratio),int(output.shape[0]/ratio)), \n",
    "                                interpolation=cv2.INTER_CUBIC) * (ratio**2)\n",
    "\n",
    "        output = np.reshape(output, (1, ) + output.shape)\n",
    "\n",
    "        return img, output, count        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, target, count = self.__load_data(self.root[index], output_shape=self.output_shape, aug=self.aug)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, target, count\n",
    "    def __len__(self):\n",
    "        return self.nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(train_path, test_path, output_shape, ratio=8):\n",
    "    train_img_paths = glob.glob(os.path.join(train_path, '*.h5'))\n",
    "    test_img_paths = glob.glob(os.path.join(test_path, '*.h5'))\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    train_dataset = RawDataset(train_img_paths, transform, ratio=ratio, output_shape=output_shape, aug=False)\n",
    "    test_dataset = RawDataset(test_img_paths, transform, ratio=ratio, output_shape=output_shape, aug=False)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=TEST_BATCH_SIZE)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_loaders(train_path_SGH, test_path_SGH, output_shape=TARGET_SHAPE)\n",
    "train_loader2, test_loader2 = get_loaders(train_path_SGH, test_path_SGH, output_shape=TARGET_SHAPE, ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader, test_loader = get_loaders(train_path_UCF_QNRF, test_path_UCF_QNRF, output_shape=TARGET_SHAPE)\n",
    "# train_loader2, test_loader2 = get_loaders(train_path_UCF_QNRF, test_path_UCF_QNRF, output_shape=TARGET_SHAPE, ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Model Checkpoints\n",
    "Save model checkpoints, which are - epoch number, model state and optimizer state. A model checkpoint will be saved only whenever a better MAE is achieved or each 5 epochs, only if the previous loss has been improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model_filename, model_state_dict, optimizer_state_dict, save_path):\n",
    "    '''\n",
    "        Saves the model, as well as the optimizer state in the predefined place.\n",
    "    '''\n",
    "    print('Saving Model!')\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model_state_dict,\n",
    "        'optimizer': optimizer_state_dict\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, os.path.join(save_path, model_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_filepath):\n",
    "    '''\n",
    "        Loads the model from checkpoint_filepath.\n",
    "    '''    \n",
    "    return torch.load(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dense Scale Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDCB(nn.Module):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    def __init__(self, in_planes):\n",
    "        super(DDCB, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_planes, 256, 1), nn.ReLU(True), \n",
    "                                   nn.Conv2d(256, 64, 3, padding=1), nn.ReLU(True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(in_planes+64, 256, 1), nn.ReLU(True), \n",
    "                                   nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(in_planes+128, 256, 1), nn.ReLU(True), \n",
    "                                   nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(in_planes+128, 512, 3, padding=1), nn.ReLU(True))\n",
    "    def forward(self, x):\n",
    "        x1_raw = self.conv1(x)\n",
    "        x1 = torch.cat([x, x1_raw], 1)\n",
    "        x2_raw = self.conv2(x1)\n",
    "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
    "        x3_raw = self.conv3(x2)\n",
    "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
    "        output = self.conv4(x3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseScaleNet(nn.Module):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    def __init__(self, model_state='', pretrained_backbone=False, trainable_backbone=False):\n",
    "        super(DenseScaleNet, self).__init__()\n",
    "        self.model_state = model_state\n",
    "        self.pretrained_backbone = pretrained_backbone\n",
    "        self.trainable_backbone = trainable_backbone\n",
    "        # network\n",
    "        self.features = self.__get_backbone()\n",
    "        self.DDCB1 = DDCB(512)\n",
    "        self.DDCB2 = DDCB(512)\n",
    "        self.DDCB3 = DDCB(512)\n",
    "        self.output_layers = nn.Sequential(nn.Conv2d(512, 128, 3, padding=1), \n",
    "                                           nn.ReLU(True), \n",
    "                                           nn.Conv2d(128, 64, 3, padding=1), \n",
    "                                           nn.ReLU(True), \n",
    "                                           nn.Conv2d(64, 1, 1))\n",
    "        self.__initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x1_raw = self.DDCB1(x)\n",
    "        x1 = x1_raw + x\n",
    "        x2_raw = self.DDCB2(x1)\n",
    "        x2 = x2_raw + x1_raw + x\n",
    "        x3_raw = self.DDCB3(x2)\n",
    "        x3 = x3_raw + x2_raw + x1_raw + x\n",
    "        output = self.output_layers(x3)\n",
    "        return output\n",
    "    \n",
    "    def __get_backbone(self):\n",
    "        self.features_cfg = [64, 64, 'M', \n",
    "                             128, 128, 'M', \n",
    "                             256, 256, 256, 'M', \n",
    "                             512, 512, 512,]\n",
    "        return self.__make_layers(self.features_cfg)\n",
    "    \n",
    "    def __make_layers(self, cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "        if dilation:\n",
    "            d_rate = 2\n",
    "        else:\n",
    "            d_rate = 1\n",
    "        layers = []\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)   \n",
    "    \n",
    "    def __initialize_weights(self):\n",
    "        self_dict = self.state_dict()\n",
    "        pretrained_dict = dict()\n",
    "        self.__random_initialize_weights()\n",
    "        if not self.model_state:\n",
    "            # load vgg16\n",
    "            vgg16 = torchvision.models.vgg16(pretrained=self.pretrained_backbone)\n",
    "            # check if the backbone should be trainable or not\n",
    "            if not self.trainable_backbone:\n",
    "                for param in vgg16.parameters():\n",
    "                    param.requires_grad = False\n",
    "            # copy over the items that match        \n",
    "            for k, v in vgg16.state_dict().items():\n",
    "                if k in self_dict and self_dict[k].size() == v.size():\n",
    "                    pretrained_dict[k] = v\n",
    "            self_dict.update(pretrained_dict)\n",
    "            self.load_state_dict(self_dict)\n",
    "        else:\n",
    "            self_dict.update(self.model_state)\n",
    "            self.load_state_dict(self_dict)\n",
    "            \n",
    "    def __random_initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.normal_(module.weight, std=0.01)\n",
    "                #nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Criterion, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_lc_loss(output, target, sizes=(1,2,4)):\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "    Lc_loss = None\n",
    "    for s in sizes:\n",
    "        pool = nn.AdaptiveAvgPool2d(s)\n",
    "        est = pool(output)\n",
    "        gt = pool(target)\n",
    "        if Lc_loss:\n",
    "            Lc_loss += criterion_L1(est, gt)\n",
    "        else:\n",
    "            Lc_loss = criterion_L1(est, gt)\n",
    "    return Lc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(output, target):\n",
    "    Le_Loss = criterion(output, target)\n",
    "    Lc_Loss = cal_lc_loss(output, target)\n",
    "    loss = Le_Loss + 1000 * Lc_Loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, test_loader):\n",
    "    model.eval()\n",
    "    mae = 0.0\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, target, count in test_loader:\n",
    "            img = img.cuda()\n",
    "            output = model(img)\n",
    "            est_count = output.sum().item()\n",
    "            mae += abs(est_count - count)\n",
    "            mse += (est_count - count)**2\n",
    "    mae /= len(test_loader)\n",
    "    mse /= len(test_loader)\n",
    "    mse = mse**0.5\n",
    "    return float(mae), float(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Init Model\n",
    "Or load a pretrained one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkp = load_checkpoint('models/best/DenseScaleNet_noaug_1e-5.pth')\n",
    "dsn_net_weights = chkp['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseScaleNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "  )\n",
       "  (DDCB1): DDCB(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (DDCB2): DDCB(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (DDCB3): DDCB(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layers): Sequential(\n",
       "    (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsn_net = DenseScaleNet('', pretrained_backbone=True, trainable_backbone=True)\n",
    "dsn_net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(dsn_net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    best_mae, _  = val(model, test_loader)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for img, target, count in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            img = img.cuda()\n",
    "            target = target.float()\n",
    "            target = target.cuda()\n",
    "            output = model(img)\n",
    "\n",
    "            loss = calc_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        mae, mse = val(model, test_loader)\n",
    "\n",
    "        print('Epoch {}/{} Loss:{:.3f}, MAE:{:.2f}, MSE:{:.2f}, Best MAE:{:.2f}'.format(epoch+1, \n",
    "                                                                                        EPOCHS, \n",
    "                                                                                        train_loss/len(train_loader), \n",
    "                                                                                        mae, \n",
    "                                                                                        mse, \n",
    "                                                                                        best_mae))\n",
    "        if epoch + 1 % 5 == 0:\n",
    "            save_checkpoint(epoch, f'DenseScaleNet_noaug_e{epoch}_{LEARNING_RATE}.pth', \n",
    "                            model.state_dict(), \n",
    "                            optimizer.state_dict(), \n",
    "                            CKP_MODEL_SAVE_PATH)          \n",
    "        \n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            print(f'New best mae: {best_mae}. Saving model!')\n",
    "            # report best model\n",
    "            report_to_overwatch('VM:ML:P', 'Atlas', f'Epoch {epoch} recorded {best_mae}!')\n",
    "            \n",
    "            save_checkpoint(epoch, 'DenseScaleNet_noaug_1e-5.pth', \n",
    "                            model.state_dict(), \n",
    "                            optimizer.state_dict(), \n",
    "                            BEST_MODEL_SAVE_PATH)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:56<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Loss:41.357, MAE:73.62, MSE:135.67, Best MAE:96.37\n",
      "New best mae: 73.6196076145655. Saving model!\n",
      "200\n",
      "Report sent\n",
      "Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:56<00:00,  3.42it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Loss:41.316, MAE:87.38, MSE:145.30, Best MAE:73.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:56<00:00,  3.42it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 Loss:36.088, MAE:98.41, MSE:150.70, Best MAE:73.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:56<00:00,  3.42it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 Loss:36.800, MAE:88.79, MSE:141.81, Best MAE:73.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:56<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 Loss:33.338, MAE:115.02, MSE:169.71, Best MAE:73.62\n",
      "200\n",
      "Report sent\n"
     ]
    }
   ],
   "source": [
    "dsn_net = train_model(dsn_net, train_loader, test_loader, optimizer)\n",
    "report_to_overwatch('VM:ML:P', 'Atlas', 'Training of model done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_h5 = h5py.File('training_dataset/ShanghaiTech/test_h5/train_h5GT_IMG_3.mat.h5', 'r')\n",
    "img = src_h5['image_array'].value\n",
    "output = src_h5['density_map'].value\n",
    "count = float(src_h5['count'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f244004a5f8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.resize(img, (720, 480), interpolation=cv2.INTER_CUBIC)\n",
    "img = np.reshape(img, (1, 3, 480, 720))\n",
    "timg = torch.from_numpy(img).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dsn_net(timg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463.291259765625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Congested Scene Recognition Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False, trainable_backbone=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.trainable_backbone = trainable_backbone\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128,\n",
    "                              'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat = [512, 512, 512, 256, 128, 64]\n",
    "        self.frontend = self.__make_layers(self.frontend_feat, batch_norm=True)\n",
    "        self.backend = self.__make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        self.__initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = nn.functional.interpolate(x, scale_factor=8)\n",
    "        return x\n",
    "\n",
    "    def __initialize_weights(self):\n",
    "        self_dict = self.state_dict()\n",
    "        pretrained_dict = dict()\n",
    "        # load vgg16\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "        # check if the backbone should be trainable or not\n",
    "        if self.trainable_backbone:\n",
    "            for param in vgg16.parameters():\n",
    "                param.requires_grad = False\n",
    "        # copy over the items that match        \n",
    "        for k, v in vgg16.state_dict().items():\n",
    "            if k in self_dict and self_dict[k].size() == v.size():\n",
    "                pretrained_dict[k] = v\n",
    "        self_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(self_dict)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "    def __make_layers(self, cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "        if dilation:\n",
    "            d_rate = 2\n",
    "        else:\n",
    "            d_rate = 1\n",
    "        layers = []\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3,\n",
    "                                   padding=d_rate, dilation=d_rate)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Criterion\n",
    "Same as Dense Scale Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CSRNet(\n",
       "  (frontend): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "  )\n",
       "  (backend): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "    (11): ReLU(inplace=True)\n",
       "  )\n",
       "  (output_layer): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_net = CSRNet()\n",
    "csr_net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(csr_net.parameters(), lr=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_dataloader, test_dataloader):\n",
    "    min_mae = 999999\n",
    "    min_mae_epoch = -1\n",
    "    for epoch in range(0, EPOCHS):                          # start training\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for img, target, count in tqdm(train_dataloader):\n",
    "            image = img.cuda()\n",
    "            gt_densitymap = target.float()\n",
    "            gt_densitymap = gt_densitymap.cuda()\n",
    "            et_densitymap = model(image)  \n",
    "            loss = criterion(et_densitymap,gt_densitymap)       # calculate loss\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()                                     # back propagation\n",
    "            optimizer.step()                                    # update network parameters\n",
    "            \n",
    "        print('Epoch {}/{} Loss: {:.3f}'.format(epoch, EPOCHS, epoch_loss/len(train_loader)))\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_mae = 0.0\n",
    "            for img, target, count in tqdm(test_dataloader):\n",
    "                image = img.cuda()\n",
    "                target = target.float()\n",
    "                gt_densitymap = target.cuda()\n",
    "                et_densitymap = model(image).detach()           # forward propagation\n",
    "                mae = abs(et_densitymap.data.sum()-gt_densitymap.data.sum())\n",
    "                epoch_mae += mae.item()\n",
    "            epoch_mae /= len(test_dataloader)\n",
    "            print('Epoch {}/{} MAE: {:.3f}'.format(epoch, EPOCHS, epoch_mae))\n",
    "            if epoch_mae < min_mae:\n",
    "                min_mae, min_mae_epoch = epoch_mae, epoch\n",
    "                report_to_overwatch('VM:ML:P', 'Atlas', f'Epoch {epoch} recorded {min_mae}!')\n",
    "                torch.save(model.state_dict(), os.path.join(BEST_MODEL_SAVE_PATH,'CSRNet_'+str(epoch)+\".pth\")) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 231/400 [04:07<03:02,  1.08s/it]"
     ]
    }
   ],
   "source": [
    "csr_net = train_model(csr_net, optimizer, criterion, train_loader2, test_loader2)\n",
    "report_to_overwatch('VM:ML:P', 'Atlas', 'Training of CSRNet model done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_h5 = h5py.File('training_dataset/ShanghaiTech/test_h5/train_h5GT_IMG_3.mat.h5', 'r')\n",
    "img = src_h5['image_array'].value\n",
    "output = src_h5['density_map'].value\n",
    "count = float(src_h5['count'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f35e954d588>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25950.5234375"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.resize(img, (720, 480), interpolation=cv2.INTER_CUBIC)\n",
    "img = np.reshape(img, (1, 3, 480, 720))\n",
    "timg = torch.from_numpy(img).float().cuda()\n",
    "out = csr_net(timg)\n",
    "out.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 Pytorch",
   "language": "python",
   "name": "torchpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

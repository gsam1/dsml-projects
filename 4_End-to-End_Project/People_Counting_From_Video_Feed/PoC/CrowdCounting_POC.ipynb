{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Crowds with Deep Learning\n",
    "## Proof Of Concept\n",
    "\n",
    "The notebook will implement various papers for the puprose of crowd counting\n",
    "* [Dense Scale Networks](https://arxiv.org/pdf/1906.09707.pdf)\n",
    "* [CSRNet: Dilated Convolutional Neural Networks](https://arxiv.org/pdf/1802.10062.pdf)\n",
    "\n",
    "The goal - to find the best approach to teach a model to count crowds, based on input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from skimage import exposure, img_as_float\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting module\n",
    "from ovreport.report import report_to_overwatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Globals\n",
    "A number of parameters on top of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "train_path_UCF_QNRF = 'training_dataset/UCF-QNRF_ECCV18/Train_h5/'\n",
    "test_path_UCF_QNRF = 'training_dataset/UCF-QNRF_ECCV18/Test_h5/'\n",
    "# TODO: Add More ... add more\n",
    "\n",
    "# Target Image Size\n",
    "TARGET_SHAPE = (720, 480)\n",
    "\n",
    "# Model save paths\n",
    "BEST_MODEL_SAVE_PATH = 'models/best/'\n",
    "CKP_MODEL_SAVE_PATH = 'models/checkpoints/'\n",
    "# Training Details\n",
    "PRETRAINED_BACKBONE = True\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dealing with Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset(Dataset):\n",
    "    def __init__(self, root, transform, ratio=8, output_shape=False, aug=False):\n",
    "        self.nsamples = len(root)\n",
    "        self.aug = aug\n",
    "        self.output_shape = output_shape\n",
    "        self.root = root\n",
    "        self.ratio = ratio\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __augment(image, target, count, seed):\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # apply random crop\n",
    "        if random.random() < 0.5:\n",
    "            crop_size = (img.size[0]//2, img.size[1]//2)\n",
    "        \n",
    "            if random.random() <=0.44:\n",
    "                # 4 non-overlapping patches\n",
    "                dx = int(random.randint(0,1) * crop_size[0])\n",
    "                dy = int(random.randint(0,1) * crop_size[1])\n",
    "            else:\n",
    "                # 5 random patches\n",
    "                # set seed to ensure for each image the random patches are certain\n",
    "                # if not set, the crop will be online which means the patches change every time loading, leading to a dynamic training set.\n",
    "                patch_id = random.randint(0, 4)\n",
    "                random.seed(index + patch_id * 0.1)\n",
    "                dx = int(random.random() * crop_size[0])\n",
    "                random.seed(index + 0.5 + patch_id * 0.1)\n",
    "                dy = int(random.random() * crop_size[1])\n",
    "            # crop\n",
    "            img = img.crop((dx, dy, crop_size[0]+dx, crop_size[1]+dy))\n",
    "            target = target[dy:crop_size[1]+dy, dx:crop_size[0]+dx]\n",
    "            count = float(target.sum())\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            target = np.fliplr(target)\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        if random.random() > 0.7:\n",
    "            img = img_as_float(image)\n",
    "            # gamma_img: np.array(dtype=float64) ranging [0,1]\n",
    "            if random.random() > 0.5:\n",
    "                gamma_img = exposure.adjust_gamma(img, 1.5)\n",
    "            else:\n",
    "                gamma_img = exposure.adjust_gamma(img, 0.5)\n",
    "            gamma_img = gamma_img * 255\n",
    "            gamma_img = np.uint8(gamma_img)\n",
    "            image = Image.fromarray(gamma_img)\n",
    "        \n",
    "        return image, target, count\n",
    "    \n",
    "    def __resize_to_target(self, img, target_shape):\n",
    "        return cv2.resize(img, target_shape, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    def __load_data(self, path, ratio=8, output_shape=None, aug=False, index=None):\n",
    "        src_h5 = h5py.File(path, 'r')\n",
    "        img = src_h5['image_array'].value\n",
    "        output = src_h5['density_map'].value\n",
    "        count = float(src_h5['count'].value)\n",
    "\n",
    "\n",
    "        if len(img.shape) < 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        if output_shape is not None:\n",
    "            img = self.__resize_to_target(img, output_shape)\n",
    "            output = self.__resize_to_target(output, output_shape)\n",
    "\n",
    "        if aug:\n",
    "            # TODO: Implement augumentation\n",
    "            img, output, count = self.__augment(img, output, count, 42)\n",
    "\n",
    "        if ratio>1:\n",
    "            output = cv2.resize(output, \n",
    "                                (int(output.shape[1]/ratio),int(output.shape[0]/ratio)), \n",
    "                                interpolation=cv2.INTER_CUBIC) * (ratio**2)\n",
    "\n",
    "        output = np.reshape(output, (1, ) + output.shape)\n",
    "\n",
    "        return img, output, count        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, target, count = self.__load_data(self.root[index], output_shape=self.output_shape, aug=self.aug)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, target, count\n",
    "    def __len__(self):\n",
    "        return self.nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(train_path, test_path, output_shape, ratio=8):\n",
    "    train_img_paths = glob.glob(os.path.join(train_path, '*.h5'))\n",
    "    test_img_paths = glob.glob(os.path.join(test_path, '*.h5'))\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    train_dataset = RawDataset(train_img_paths, transform, ratio=ratio, output_shape=output_shape, aug=False)\n",
    "    test_dataset = RawDataset(test_img_paths, transform, ratio=1, output_shape=output_shape, aug=False)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=TEST_BATCH_SIZE)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_loaders(train_path_UCF_QNRF, test_path_UCF_QNRF, output_shape=TARGET_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Model Checkpoints\n",
    "Save model checkpoints, which are - epoch number, model state and optimizer state. A model checkpoint will be saved only whenever a better MAE is achieved or each 5 epochs, only if the previous loss has been improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model_filename, model_state_dict, optimizer_state_dict, save_path):\n",
    "    '''\n",
    "        Saves the model, as well as the optimizer state in the predefined place.\n",
    "    '''\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model_state_dict,\n",
    "        'optimizer': optimizer_state_dict\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, os.path.join(save_path, model_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_filepath, model, optimizer):\n",
    "    '''\n",
    "        Loads the model from checkpoint_filepath.\n",
    "    '''\n",
    "    checkpoint = torch.load(checkpoint_filepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    return model, optimizer, checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dense Scale Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDCB(nn.Module):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    def __init__(self, in_planes):\n",
    "        super(DDCB, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_planes, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1), nn.ReLU(True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(in_planes+64, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(in_planes+128, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(in_planes+128, 512, 3, padding=1), nn.ReLU(True))\n",
    "    def forward(self, x):\n",
    "        x1_raw = self.conv1(x)\n",
    "        x1 = torch.cat([x, x1_raw], 1)\n",
    "        x2_raw = self.conv2(x1)\n",
    "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
    "        x3_raw = self.conv3(x2)\n",
    "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
    "        output = self.conv4(x3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseScaleNet(nn.Module):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    def __init__(self, load_model='', pretrained_backbone=False, trainable_backbone=False):\n",
    "        super(DenseScaleNet, self).__init__()\n",
    "        self.load_model = load_model\n",
    "        self.pretrained_backbone = pretrained_backbone\n",
    "        self.trainable_backbone = trainable_backbone\n",
    "        # network\n",
    "        self.features = self.__get_backbone()\n",
    "        self.DDCB1 = DDCB(512)\n",
    "        self.DDCB2 = DDCB(512)\n",
    "        self.DDCB3 = DDCB(512)\n",
    "        self.output_layers = nn.Sequential(nn.Conv2d(512, 128, 3, padding=1), \n",
    "                                           nn.ReLU(True), \n",
    "                                           nn.Conv2d(128, 64, 3, padding=1), \n",
    "                                           nn.ReLU(True), \n",
    "                                           nn.Conv2d(64, 1, 1))\n",
    "        self.__initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x1_raw = self.DDCB1(x)\n",
    "        x1 = x1_raw + x\n",
    "        x2_raw = self.DDCB2(x1)\n",
    "        x2 = x2_raw + x1_raw + x\n",
    "        x3_raw = self.DDCB3(x2)\n",
    "        x3 = x3_raw + x2_raw + x1_raw + x\n",
    "        output = self.output_layers(x3)\n",
    "        return output\n",
    "    \n",
    "    def __get_backbone(self):\n",
    "#         if self.pretrained_backbone:\n",
    "#             vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "#             # only get the leayers we are interested in\n",
    "#             backbone = vgg16.features[:23] # to match the layers in the paper\n",
    "#             # make them untrainable if desired\n",
    "#             if not self.trainable_backbone:\n",
    "#                 for param in backbone.parameters():\n",
    "#                     param.requires_grad = False\n",
    "\n",
    "#             return backbone\n",
    "#         else:\n",
    "        self.features_cfg = [64, 64, 'M', \n",
    "                             128, 128, 'M', \n",
    "                             256, 256, 256, 'M', \n",
    "                             512, 512, 512,]\n",
    "        return self.__make_layers(self.features_cfg)\n",
    "    \n",
    "    def __make_layers(self, cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "        if dilation:\n",
    "            d_rate = 2\n",
    "        else:\n",
    "            d_rate = 1\n",
    "        layers = []\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)   \n",
    "    \n",
    "    def __initialize_weights(self):\n",
    "        self_dict = self.state_dict()\n",
    "        pretrained_dict = dict()\n",
    "        self.__random_initialize_weights()\n",
    "        if not self.load_model:\n",
    "            # load vgg16\n",
    "            vgg16 = torchvision.models.vgg16(pretrained=self.pretrained_backbone)\n",
    "            # check if the backbone should be trainable or not\n",
    "            if not self.trainable_backbone:\n",
    "                for param in vgg16.parameters():\n",
    "                    param.requires_grad = False\n",
    "            # copy over the items that match        \n",
    "            for k, v in vgg16.state_dict().items():\n",
    "                if k in self_dict and self_dict[k].size() == v.size():\n",
    "                    pretrained_dict[k] = v\n",
    "            self_dict.update(pretrained_dict)\n",
    "            self.load_state_dict(self_dict)\n",
    "        else:\n",
    "            self.load_state_dict(self.load_model)\n",
    "            \n",
    "    def __random_initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.normal_(module.weight, std=0.01)\n",
    "                #nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Criterion, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_lc_loss(output, target, sizes=(1,2,4)):\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "    Lc_loss = None\n",
    "    for s in sizes:\n",
    "        pool = nn.AdaptiveAvgPool2d(s)\n",
    "        est = pool(output)\n",
    "        gt = pool(target)\n",
    "        if Lc_loss:\n",
    "            Lc_loss += criterion_L1(est, gt)\n",
    "        else:\n",
    "            Lc_loss = criterion_L1(est, gt)\n",
    "    return Lc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(output, target):\n",
    "    Le_Loss = criterion(output, target)\n",
    "    Lc_Loss = cal_lc_loss(output, target)\n",
    "    loss = Le_Loss + 1000 * Lc_Loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, test_loader):\n",
    "    model.eval()\n",
    "    mae = 0.0\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, target, count in test_loader:\n",
    "            img = img.cuda()\n",
    "            output = model(img)\n",
    "            est_count = output.sum().item()\n",
    "            mae += abs(est_count - count)\n",
    "            mse += (est_count - count)**2\n",
    "    mae /= len(test_loader)\n",
    "    mse /= len(test_loader)\n",
    "    mse = mse**0.5\n",
    "    return float(mae), float(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Init Model\n",
    "Or load a pretrained one if exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model=None, **options):\n",
    "    '''\n",
    "        Initializes the Model.\n",
    "        If the model is \n",
    "        If new is true it starts with a new model, else loads the one in the Model Save Path.\n",
    "    '''\n",
    "    if options['new'] and options['trainable_backbone']:\n",
    "        dsn_net = DenseScaleNet('', pretrained_backbone=PRETRAINED_BACKBONE, trainable_backbone=True)\n",
    "    elif options['new']:\n",
    "        dsn_net = DenseScaleNet('', pretrained_backbone=PRETRAINED_BACKBONE)\n",
    "    else:\n",
    "        dsn_net = DenseScaleNet(options['model'])\n",
    "    \n",
    "    return dsn_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseScaleNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "  )\n",
       "  (DDCB1): DDCB(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (DDCB2): DDCB(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (DDCB3): DDCB(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layers): Sequential(\n",
       "    (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsn_net = init_model(new=True, trainable_backbone=True)\n",
    "dsn_net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(dsn_net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    best_mae, _  = val(model, test_loader)\n",
    "    best_loss = 9999.0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for img, target, count in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            img = img.cuda()\n",
    "            target = target.float()\n",
    "            target = target.cuda()\n",
    "            output = model(img)\n",
    "\n",
    "            loss = calc_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        mae, mse = val(model, test_loader)\n",
    "\n",
    "        print('Epoch {}/{} Loss:{:.3f}, MAE:{:.2f}, MSE:{:.2f}, Best MAE:{:.2f}'.format(epoch+1, \n",
    "                                                                                        EPOCHS, \n",
    "                                                                                        train_loss/len(train_loader), \n",
    "                                                                                        mae, \n",
    "                                                                                        mse, \n",
    "                                                                                        best_mae))\n",
    "        if epoch % 5 == 0 and train_loss < best_loss:\n",
    "            save_checkpoint(epoch, f'DenseScaleNet_noaug_e{epoch}_{LEARNING_RATE}.pth', \n",
    "                            model.state_dict(), \n",
    "                            optimizer.state_dict(), \n",
    "                            CKP_MODEL_SAVE_PATH)          \n",
    "        \n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            print(f'New best mae: {best_mae}. Saving model!')\n",
    "            # report best model\n",
    "            report_to_overwatch('VM:ML:P', 'Atlas', f'Epoch {epoch} recorded {best_mae}!')\n",
    "            \n",
    "            save_checkpoint(epoch, 'DenseScaleNet_noaug_1e-5.pth', \n",
    "                            model.state_dict(), \n",
    "                            optimizer.state_dict(), \n",
    "                            BEST_MODEL_SAVE_PATH)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [11:33<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Loss:68.091, MAE:682.38, MSE:1005.10, Best MAE:764.81\n",
      "New best mae: 682.383870290425. Saving model!\n",
      "200\n",
      "Report sent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [11:33<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Loss:57.625, MAE:639.43, MSE:933.25, Best MAE:682.38\n",
      "New best mae: 639.4251550937038. Saving model!\n",
      "200\n",
      "Report sent\n",
      "200\n",
      "Report sent\n"
     ]
    }
   ],
   "source": [
    "dsn_net = train_model(dsn_net, train_loader, test_loader, optimizer)\n",
    "report_to_overwatch('VM:ML:P', 'Atlas', 'Training of model done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target, count = next(iter(test_loader))\n",
    "img = img.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dsn_net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5622602701187134"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([332.], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 Pytorch",
   "language": "python",
   "name": "torchpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

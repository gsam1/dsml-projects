{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Crowds with Deep Learning\n",
    "## Proof Of Concept\n",
    "\n",
    "The notebook will implement various papers for the puprose of crowd counting\n",
    "* [Dense Scale Networks](https://arxiv.org/pdf/1906.09707.pdf)\n",
    "* [CSRNet: Dilated Convolutional Neural Networks](https://arxiv.org/pdf/1802.10062.pdf)\n",
    "\n",
    "The goal - to find the best approach to teach a model to count crowds, based on input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting module\n",
    "from ovreport.report import report_to_overwatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Globals\n",
    "A number of parameters on top of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "train_path_UCF_QNRF = 'training_dataset/UCF-QNRF_ECCV18/Train_h5/'\n",
    "test_path_UCF_QNRF = 'training_dataset/UCF-QNRF_ECCV18/Test_h5/'\n",
    "# TODO: Add More ... add more\n",
    "\n",
    "# Target Image Size\n",
    "TARGET_SHAPE = (720, 480)\n",
    "\n",
    "\n",
    "MODEL_SAVE_PATH = 'models/best/DenseScaleNet_ucf_qnrf_1e-5.pth'\n",
    "# Training Details\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dealing with Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset(Dataset):\n",
    "    def __init__(self, root, transform, ratio=8, output_shape=False, aug=False):\n",
    "        self.nsamples = len(root)\n",
    "        self.aug = aug\n",
    "        self.output_shape = output_shape\n",
    "        self.root = root\n",
    "        self.ratio = ratio\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __resize_to_target(self, img, target_shape):\n",
    "        return cv2.resize(img, target_shape, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    def __load_data(self, path, ratio=8, output_shape=None, aug=False, index=None):\n",
    "        src_h5 = h5py.File(path, 'r')\n",
    "        img = src_h5['image_array'].value\n",
    "        output = src_h5['density_map'].value\n",
    "        count = float(src_h5['count'].value)\n",
    "\n",
    "\n",
    "        if len(img.shape) < 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        if output_shape is not None:\n",
    "            img = self.__resize_to_target(img, output_shape)\n",
    "            output = self.__resize_to_target(output, output_shape)\n",
    "\n",
    "        if aug:\n",
    "            # TODO: Implement augumentation\n",
    "            pass\n",
    "\n",
    "        if ratio>1:\n",
    "            output = cv2.resize(output, \n",
    "                                (int(output.shape[1]/ratio),int(output.shape[0]/ratio)), \n",
    "                                interpolation=cv2.INTER_CUBIC) * (ratio**2)\n",
    "\n",
    "        output = np.reshape(output, (1, ) + output.shape)\n",
    "\n",
    "        return img, output, count        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, target, count = self.__load_data(self.root[index], output_shape=self.output_shape, aug=self.aug)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, target, count\n",
    "    def __len__(self):\n",
    "        return self.nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(train_path, test_path, output_shape, ratio=8):\n",
    "    train_img_paths = glob.glob(os.path.join(train_path, '*.h5'))\n",
    "    test_img_paths = glob.glob(os.path.join(test_path, '*.h5'))\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    train_dataset = RawDataset(train_img_paths, transform, ratio=ratio, output_shape=output_shape, aug=False)\n",
    "    test_dataset = RawDataset(test_img_paths, transform, ratio=1, output_shape=output_shape, aug=False)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=TEST_BATCH_SIZE)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_loaders(train_path_UCF_QNRF, test_path_UCF_QNRF, output_shape=TARGET_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dense Scale Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDCB(nn.Module):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    def __init__(self, in_planes):\n",
    "        super(DDCB, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_planes, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1), nn.ReLU(True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(in_planes+64, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(in_planes+128, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(in_planes+128, 512, 3, padding=1), nn.ReLU(True))\n",
    "    def forward(self, x):\n",
    "        x1_raw = self.conv1(x)\n",
    "        x1 = torch.cat([x, x1_raw], 1)\n",
    "        x2_raw = self.conv2(x1)\n",
    "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
    "        x3_raw = self.conv3(x2)\n",
    "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
    "        output = self.conv4(x3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseScaleNet(nn.Module):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    def __init__(self, load_model=''):\n",
    "        super(DenseScaleNet, self).__init__()\n",
    "        self.load_model = load_model\n",
    "        self.features_cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512,]\n",
    "        self.features = self.__make_layers(self.features_cfg)\n",
    "        self.DDCB1 = DDCB(512)\n",
    "        self.DDCB2 = DDCB(512)\n",
    "        self.DDCB3 = DDCB(512)\n",
    "        self.output_layers = nn.Sequential(nn.Conv2d(512, 128, 3, padding=1), \n",
    "                                           nn.ReLU(True), \n",
    "                                           nn.Conv2d(128, 64, 3, padding=1), \n",
    "                                           nn.ReLU(True), \n",
    "                                           nn.Conv2d(64, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x1_raw = self.DDCB1(x)\n",
    "        x1 = x1_raw + x\n",
    "        x2_raw = self.DDCB2(x1)\n",
    "        x2 = x2_raw + x1_raw + x\n",
    "        x3_raw = self.DDCB3(x2)\n",
    "        x3 = x3_raw + x2_raw + x1_raw + x\n",
    "        output = self.output_layers(x3)\n",
    "        return output\n",
    "    \n",
    "    def __make_layers(self, cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "        if dilation:\n",
    "            d_rate = 2\n",
    "        else:\n",
    "            d_rate = 1\n",
    "        layers = []\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)   \n",
    "    \n",
    "    def __initialize_weights(self):\n",
    "        self_dict = self.state_dict()\n",
    "        pretrained_dict = dict()\n",
    "        self.__random_initialize_weights()\n",
    "        if not self.load_model:\n",
    "            vgg16 = torch.load(os.path.join(MODEL_SAVE_PATH, 'vgg16.pth'))\n",
    "            for k, v in vgg16.items():\n",
    "                if k in self_dict and self_dict[k].size() == v.size():\n",
    "                    pretrained_dict[k] = v\n",
    "            self_dict.update(pretrained_dict)\n",
    "            self.load_state_dict(self_dict)\n",
    "        else:\n",
    "            self.load_state_dict(torch.load(self.load_model))\n",
    "            \n",
    "    def __random_initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.normal_(module.weight, std=0.01)\n",
    "                #nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Criterion, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_lc_loss(output, target, sizes=(1,2,4)):\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "    Lc_loss = None\n",
    "    for s in sizes:\n",
    "        pool = nn.AdaptiveAvgPool2d(s)\n",
    "        est = pool(output)\n",
    "        gt = pool(target)\n",
    "        if Lc_loss:\n",
    "            Lc_loss += criterion_L1(est, gt)\n",
    "        else:\n",
    "            Lc_loss = criterion_L1(est, gt)\n",
    "    return Lc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(output, target):\n",
    "    Le_Loss = criterion(output, target)\n",
    "    Lc_Loss = cal_lc_loss(output, target)\n",
    "    loss = Le_Loss + 1000 * Lc_Loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, test_loader):\n",
    "    model.eval()\n",
    "    mae = 0.0\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, target, count in test_loader:\n",
    "            img = img.cuda()\n",
    "            output = model(img)\n",
    "            est_count = output.sum().item()\n",
    "            mae += abs(est_count - count)\n",
    "            mse += (est_count - count)**2\n",
    "    mae /= len(test_loader)\n",
    "    mse /= len(test_loader)\n",
    "    mse = mse**0.5\n",
    "    return float(mae), float(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Init Model\n",
    "Or load a pretrained one if exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseScaleNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "  )\n",
       "  (DDCB1): DDCB(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (DDCB2): DDCB(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (DDCB3): DDCB(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layers): Sequential(\n",
       "    (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    dsn_net = DenseScaleNet('')\n",
    "    dsn_net.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "else:\n",
    "    dsn_net = DenseScaleNet('')\n",
    "\n",
    "dsn_net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(dsn_net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer, model_save_path):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    best_mae, _  = val(model, test_loader)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for img, target, count in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            img = img.cuda()\n",
    "            target = target.float()\n",
    "            target = target.cuda()\n",
    "            output = model(img)\n",
    "\n",
    "            loss = calc_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        mae, mse = val(model, test_loader)\n",
    "\n",
    "        print('Epoch {}/{} Loss:{:.3f}, MAE:{:.2f}, MSE:{:.2f}, Best MAE:{:.2f}'.format(epoch+1, \n",
    "                                                                                        EPOCHS, \n",
    "                                                                                        train_loss/len(train_loader), \n",
    "                                                                                        mae, \n",
    "                                                                                        mse, \n",
    "                                                                                        best_mae))\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 Loss:63.413, MAE:615.19, MSE:947.65, Best MAE:588.56\n",
      "Epoch 2/30 Loss:62.755, MAE:640.70, MSE:959.84, Best MAE:588.56\n",
      "Epoch 3/30 Loss:60.739, MAE:668.55, MSE:974.62, Best MAE:588.56\n",
      "Epoch 4/30 Loss:62.883, MAE:666.09, MSE:978.54, Best MAE:588.56\n",
      "Epoch 5/30 Loss:62.852, MAE:632.43, MSE:945.47, Best MAE:588.56\n",
      "Epoch 6/30 Loss:61.348, MAE:667.62, MSE:979.62, Best MAE:588.56\n",
      "Epoch 7/30 Loss:62.322, MAE:650.71, MSE:962.87, Best MAE:588.56\n",
      "Epoch 8/30 Loss:60.064, MAE:656.68, MSE:968.32, Best MAE:588.56\n",
      "Epoch 9/30 Loss:60.771, MAE:624.85, MSE:943.96, Best MAE:588.56\n",
      "Epoch 10/30 Loss:61.141, MAE:639.48, MSE:957.22, Best MAE:588.56\n",
      "Epoch 11/30 Loss:59.337, MAE:594.84, MSE:905.80, Best MAE:588.56\n",
      "Epoch 12/30 Loss:60.132, MAE:634.74, MSE:942.98, Best MAE:588.56\n",
      "Epoch 13/30 Loss:60.041, MAE:635.01, MSE:946.06, Best MAE:588.56\n",
      "Epoch 14/30 Loss:59.364, MAE:621.39, MSE:933.34, Best MAE:588.56\n",
      "Epoch 15/30 Loss:57.705, MAE:653.10, MSE:970.35, Best MAE:588.56\n",
      "Epoch 16/30 Loss:60.634, MAE:606.70, MSE:923.66, Best MAE:588.56\n",
      "Epoch 17/30 Loss:57.849, MAE:698.94, MSE:1010.45, Best MAE:588.56\n",
      "Epoch 18/30 Loss:59.455, MAE:665.88, MSE:979.36, Best MAE:588.56\n",
      "Epoch 19/30 Loss:58.030, MAE:647.12, MSE:961.90, Best MAE:588.56\n",
      "Epoch 20/30 Loss:57.731, MAE:641.92, MSE:956.33, Best MAE:588.56\n",
      "Epoch 21/30 Loss:56.997, MAE:611.78, MSE:925.41, Best MAE:588.56\n",
      "Epoch 22/30 Loss:59.023, MAE:635.53, MSE:945.61, Best MAE:588.56\n",
      "Epoch 23/30 Loss:57.643, MAE:616.80, MSE:936.46, Best MAE:588.56\n",
      "Epoch 24/30 Loss:58.546, MAE:642.58, MSE:958.63, Best MAE:588.56\n",
      "Epoch 25/30 Loss:56.431, MAE:639.61, MSE:949.94, Best MAE:588.56\n",
      "Epoch 26/30 Loss:57.735, MAE:619.11, MSE:917.05, Best MAE:588.56\n",
      "Epoch 27/30 Loss:56.327, MAE:640.58, MSE:948.73, Best MAE:588.56\n",
      "Epoch 28/30 Loss:55.966, MAE:646.39, MSE:959.62, Best MAE:588.56\n",
      "Epoch 29/30 Loss:55.219, MAE:667.88, MSE:984.83, Best MAE:588.56\n",
      "Epoch 30/30 Loss:55.363, MAE:593.48, MSE:904.97, Best MAE:588.56\n",
      "200\n",
      "Report sent\n"
     ]
    }
   ],
   "source": [
    "dsn_net = train_model(dsn_net, train_loader, test_loader, optimizer, MODEL_SAVE_PATH)\n",
    "report_to_overwatch('VM:ML:P', 'Atlas', 'Training of model done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target, count = next(iter(test_loader))\n",
    "img = img.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dsn_net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.673754692077637"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([332.], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 Pytorch",
   "language": "python",
   "name": "torchpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Crowds with DL\n",
    "## Proof Of Concept\n",
    "The notebook will implement [Dense Scale Networks](https://arxiv.org/pdf/1906.09707.pdf) for the purpose of counting crowds for images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 1. Imports and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import scipy\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mse, mae\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting module\n",
    "from ovreport.report import report_to_overwatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 1.1 Getting Data to Train the model.\n",
    "A couple of datasets are available to use for the purpose of training. My goal is to try to combine them all and train the dense scale network on all of them and evaluate them on all tests set.\n",
    "\n",
    "*NOTE*: Provide List of more famous Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 UCF-QNRF_ECCV18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Exploring the data format and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO:* Learn images original size and aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucf_qnrf_example = scipy.io.loadmat('training_dataset/UCF-QNRF_ECCV18/Train/img_0001_ann.mat')\n",
    "print(ucf_qnrf_example.keys())\n",
    "print(ucf_qnrf_example['annPoints'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the keys of the example it seems, that the .mat files contain the annotation points. From the annotation points a density map will be generated, so that the model can be evaluated against it.\n",
    "\n",
    "*TODO*: Explain why a k-nearest density map was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Density Map Generation\n",
    "Below is an example of how a single density is created. For further information refer to the data_exploration notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_knk_density(image, points):\n",
    "    image_h = image.shape[0]\n",
    "    image_w = image.shape[1]\n",
    "\n",
    "    # coordinate of heads in the image\n",
    "    points_coordinate = points\n",
    "    # quantity of heads in the image\n",
    "    points_quantity = len(points_coordinate)\n",
    "\n",
    "    # generate ground truth density map\n",
    "    densitymap = np.zeros((image_h, image_w))\n",
    "    if points_quantity == 0:\n",
    "        return densitymap\n",
    "    else:\n",
    "        # build kdtree\n",
    "        tree = scipy.spatial.KDTree(points_coordinate.copy(), leafsize=2048)\n",
    "        # query kdtree\n",
    "        distances, _ = tree.query(points_coordinate, k=4)\n",
    "        for i, pt in enumerate(points_coordinate):\n",
    "            pt2d = np.zeros((image_h,image_w), dtype=np.float32)\n",
    "            \n",
    "            if int(pt[1])<image_h and int(pt[0])<image_w:\n",
    "                pt2d[int(pt[1]),int(pt[0])] = 1.\n",
    "\n",
    "            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1\n",
    "            \n",
    "            densitymap += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')\n",
    "        return densitymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucf_qnrf_ex_img = plt.imread('training_dataset/UCF-QNRF_ECCV18/Train/img_0001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_map = gen_knk_density(ucf_qnrf_ex_img, ucf_qnrf_example['annPoints'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ucf_qnrf_ex_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucf_qnrf_ex_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(dataset_path):\n",
    "    '''\n",
    "        Gets the density/count vector by reading the .mat file \n",
    "        and getting the shape of the file. \n",
    "        The function only requires the path to the dataset.\n",
    "    '''\n",
    "    file_contents = os.listdir(dataset_path)\n",
    "    only_mat_files = list(filter(lambda x: '.mat' in x, file_contents))\n",
    "    \n",
    "    densities = {'image_name':[], 'count': [], 'density': []}\n",
    "    for mat in only_mat_files:\n",
    "        filepath = os.path.join(dataset_path, mat)\n",
    "        mat_loaded = scipy.io.loadmat(filepath)\n",
    "        # we attach the .jpg at the end for the respective image\n",
    "        image_name = mat.split('_a')[0] + '.jpg'\n",
    "        densities['image_name'].append(image_name)\n",
    "        densities['density'].append(mat_loaded['annPoints'])\n",
    "        densities['count'].append(mat_loaded['annPoints'].shape[0])      \n",
    "    \n",
    "    # return it as a pandas dataframe\n",
    "    return pd.DataFrame(densities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = build_dataframe('training_dataset/UCF-QNRF_ECCV18/Train/')\n",
    "test_df = build_dataframe('training_dataset/UCF-QNRF_ECCV18/Test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These densities have all been converted previously to a h5 file, along with the image data. The whole process is explained in the *Data_Exploration* notebook. Let me load a single one to detail the structure of one such file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = h5py.File('training_dataset/UCF-QNRF_ECCV18/Train_h5/img_0001.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the count from the density_map and count align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tysample_file['count'].value, np.sum(sample_file['density_map'].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference, but in general they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Data Loading Aproaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* Creating a master_df is not feasible, as pandas is memory consuming and I do only have 32GB, thus limiting the amount data such dataframe could hold. That is why the aproach will be looking into custom keras data loader to be able to serve this h5 files. I leave the code below just as an example that this is an option too, given enough ram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_master_df(dataset_path):\n",
    "    '''\n",
    "        Creates a dataset that contains for each row the name of the image, the image,\n",
    "        represented as a numpy arrray, as well as the generated density map (the label).\n",
    "        NOT \n",
    "    '''\n",
    "    dir_contents = os.listdir(dataset_path)\n",
    "    master_df = pd.DataFrame(columns=['name', 'image', 'density_map', 'count'])\n",
    "    \n",
    "    for i, file in tqdm(enumerate(dir_contents)):\n",
    "        if '.h5' in file:\n",
    "            current_file = h5py.File(os.path.join(dataset_path, file), 'r')\n",
    "            # i could write a one liner to compile the row, but i opted out for code comprehension\n",
    "            count = current_file['count'].value\n",
    "            image = current_file['image_array'].value\n",
    "            density_map = current_file['density_map'].value\n",
    "            name = current_file['image_name'].value\n",
    "\n",
    "            master_df.loc[i] = [name, image, density_map, count]\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Custom Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5DataLoader:\n",
    "    def __init__(self, filepath, batch_size=32, validation_split=None, resize=None, ratio=8, normalize=False):\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        self.ratio = ratio\n",
    "        self.filepath = filepath\n",
    "        self.files = sorted(glob.glob(os.path.join(filepath, '*.h5')))\n",
    "        self.validation_split = validation_split\n",
    "        self.batch_size = batch_size\n",
    "        self.resize = resize\n",
    "        self.normalize = normalize\n",
    "        self.train_files = []\n",
    "        self.validation_files = []\n",
    "        \n",
    "        if validation_split is not None:\n",
    "            files_v = np.random.choice(self.files, int(self.validation_split * len(self.files)))\n",
    "            self.validation_files = list(files_v)\n",
    "            self.train_files = [file for file in self.files if file not in self.validation_files]\n",
    "        else:\n",
    "            self.train_files = self.files\n",
    "    \n",
    "    def __check_if_rgb(self, image):\n",
    "        '''\n",
    "            Checks if the image is RGB - if it is grayscale the image will have less than\n",
    "            3 dimensions.\n",
    "        '''\n",
    "        if len(image.shape) < 3:\n",
    "            return np.array(Image.fromarray(image).convert('RGB'))\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "    def __apply_resize(self, image):\n",
    "        return np.array(Image.fromarray(image).resize(self.resize))\n",
    "    \n",
    "    def __apply_resize_by_ratio(self, image):\n",
    "        return np.array(Image.fromarray(image).resize((self.resize[0] // self.ratio, self.resize[1] // self.ratio)))\n",
    "    \n",
    "    def __flip_dims(self, image):\n",
    "        return np.reshape(image, (image.shape[1], image.shape[0], image.shape[2]))\n",
    "        \n",
    "    def __load_img_and_density(self, input_path):\n",
    "        src_h5 = h5py.File(input_path, 'r')\n",
    "        image = src_h5['image_array'].value\n",
    "        output = src_h5['density_map'].value\n",
    "        count = src_h5['count'].value\n",
    "        \n",
    "        return image, output, count\n",
    "    \n",
    "    def __normalize(self, arr):\n",
    "        \n",
    "        for i in range(3):\n",
    "            minval = arr[...,i].min()\n",
    "            maxval = arr[...,i].max()\n",
    "        \n",
    "        if minval != maxval:\n",
    "            arr[...,i] -= minval\n",
    "            arr[...,i] *= (255.0/(maxval-minval))\n",
    "        \n",
    "        return arr\n",
    "    \n",
    "    def __apply_transformation(self, image):\n",
    "        '''\n",
    "            TODO: Implement Transformation\n",
    "        '''\n",
    "        transformed_image = image\n",
    "        if self.normalize:\n",
    "            transformed_image = self.__normalize(transformed_image)\n",
    "        \n",
    "        # TODO: chain other transformations\n",
    "        \n",
    "        return transformed_image\n",
    "        \n",
    "    \n",
    "    def __flow(self, files, batch_size):\n",
    "        while True:\n",
    "            batch_paths = np.random.choice(files, batch_size)\n",
    "            batch_input = []\n",
    "            batch_output = []\n",
    "\n",
    "            for input_path in batch_paths:\n",
    "                \n",
    "                input_img, output, count = self.__load_img_and_density(input_path)\n",
    "                if self.resize is not None:\n",
    "                    input_img = self.__apply_resize(input_img)\n",
    "                    # check if image is rgb, if not convert it to one\n",
    "                    input_img = self.__check_if_rgb(input_img)\n",
    "                    # transform image if needed\n",
    "                    input_img = self.__apply_transformation(input_img)\n",
    "                    # flip dims\n",
    "                    # input_img = self.__flip_dims(input_img)\n",
    "                    \n",
    "                    # first scale the output approriatly to the input\n",
    "                    output = self.__apply_resize(output)\n",
    "                    # then scale it again to match the network's output\n",
    "                    output = self.__apply_resize_by_ratio(output)\n",
    "                    # finally add the channel to the output\n",
    "                    output = np.reshape(output, (output.shape + (-1,)))\n",
    "                    # flip dims\n",
    "                    # output = self.__flip_dims(output)\n",
    "                \n",
    "                batch_input += [input_img]\n",
    "                batch_output += [output]\n",
    "\n",
    "            batch_x = np.array(batch_input)\n",
    "            batch_y = np.array(batch_output)\n",
    "\n",
    "            yield (batch_x, batch_y) \n",
    "    \n",
    "    def get_file_list(self):\n",
    "        return self.files\n",
    "    \n",
    "    def training_flow(self):\n",
    "        return self.__flow(self.train_files, batch_size=self.batch_size)\n",
    "    \n",
    "    def validation_flow(self):\n",
    "        return self.__flow(self.validation_files, batch_size=1)\n",
    "\n",
    "    def next(self):\n",
    "        data = next(self.training_flow())\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 480)\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL PARAMETERS\n",
    "train_batch_size = 1\n",
    "test_batch_size = 1\n",
    "\n",
    "aspect_ratio = 1.5\n",
    "target_widgth = 720\n",
    "target_height = int(target_widgth / aspect_ratio)\n",
    "\n",
    "input_shape = (target_widgth, target_height)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data exploration notebook we saw that the median aspect ratio is 1.5, thus we need to resize accordingly. The paper recomends a batch size of 1, even during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Training & Validation [.h5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_h5_path = 'training_dataset/UCF-QNRF_ECCV18/Train_h5/'\n",
    "test_h5_path = 'training_dataset/UCF-QNRF_ECCV18/Test_h5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = H5DataLoader(train_h5_path, \\\n",
    "                         train_batch_size, \\\n",
    "                         resize=input_shape, \\\n",
    "                         #validation_split=0.1, \\\n",
    "                         normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training itterators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = train_gen.training_flow()\n",
    "# valid_iter = train_gen.validation_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Test [.h5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = H5DataLoader(test_h5_path, \\\n",
    "                        test_batch_size, \\\n",
    "                        resize=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = test_gen.training_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Training Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[0, :, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = test_gen.next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[0, :, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Model\n",
    "In order to build ithe Deep scale network first the Dense Dialated Convolution Block has to be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2.1 Network Architecture\n",
    "*TODO*: Show pictures from paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.0 Subclassing API Example\n",
    "Had some issues with it so I will leave it here for a later point where I can revisit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(keras.models.Model):\n",
    "    '''\n",
    "        TODO: Add Docstring\n",
    "    '''\n",
    "    def __init__(self, activation='relu', padding=1, dilation_rate=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv2D(256, (1, 1), activation=activation)\n",
    "        self.padding = keras.layers.ZeroPadding2D(padding=(padding, padding))\n",
    "        self.conv2 = keras.layers.Conv2D(64, (3, 3), padding='valid', dilation_rate=dilation_rate, activation=activation)\n",
    "    def call(self, inputs):\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.padding(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDCB(keras.models.Model):\n",
    "    '''\n",
    "        TODO: Add Docstring\n",
    "    '''\n",
    "    def __init__(self, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.convBlock1 = ConvBlock(**kwargs)\n",
    "        self.convBlock2 = ConvBlock(padding=2, dilation_rate=2, **kwargs)\n",
    "        self.convBlock3 = ConvBlock(padding=3, dilation_rate=3, **kwargs)\n",
    "        self.paddingOut = keras.layers.ZeroPadding2D(padding=(1,1))\n",
    "        self.convOut = keras.layers.Conv2D(512, (3, 3), padding='valid', activation='relu')\n",
    "    \n",
    "    def call(inputs):\n",
    "        out1 = self.convBlock1(inputs)\n",
    "        out2 = keras.layers.Concatenate([inputs, out1])\n",
    "        out3 = self.convBlock2(out2)\n",
    "        out4 = keras.layers.Concatenate([inputs, out1, out3])\n",
    "        out5 = self.convBlock3(out4)\n",
    "        out6 = keras.layers.Concatenate([inputs, out3, out5])\n",
    "        out7 = self.paddingOut(out6)\n",
    "        out8 = self.convOut(out7)\n",
    "        return out8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseScaleNet(keras.models.Model):\n",
    "    '''\n",
    "        TODO: Add Docstring\n",
    "    '''\n",
    "    def __init__(self, model=None, input_shape=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model is None:\n",
    "            if input_shape is None:\n",
    "                raise Exception('A model could not have an input shape set to None.')\n",
    "                \n",
    "            model = self.__create_backbone(input_shape)\n",
    "        \n",
    "        self.model = model\n",
    "        self.DDCB1 = DDCB(**kwargs)\n",
    "        self.DDCB2 = DDCB(**kwargs)\n",
    "        self.DDCB3 = DDCB(**kwargs)\n",
    "        self.padding1 = keras.layers.ZeroPadding2D(padding=(1,1))\n",
    "        self.conv1 = keras.layers.Conv2D(128, (3, 3), padding='valid', activation='relu')\n",
    "        self.padding2 = keras.layers.ZeroPadding2D(padding=(1,1))\n",
    "        self.conv2 = keras.layers.Conv2D(64, (3,3), padding='valid', activation='relu')\n",
    "        self.outconv = keras.layers.Conv2D(1, (1, 1), activation='relu')\n",
    "    \n",
    "    def __create_backbone(self, input_shape):\n",
    "        '''\n",
    "            TODO: Add Doctstring\n",
    "        '''\n",
    "        vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_shape=input_shape)\n",
    "        model = keras.models.Sequential()\n",
    "        \n",
    "        # we want to copy the first ten layers from VGG16\n",
    "        for layer in vgg16_model.layers[:11]:\n",
    "            model.add(layer)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        out1 = self.model(inputs)\n",
    "        out2 = self.DDCB1(out1)\n",
    "        out3 = out2 + out3\n",
    "        out4 = self.DDCB2(out3)\n",
    "        out5 = out4 + out2 + out1\n",
    "        out6 = self.DDCB3(out5)\n",
    "        out7 = out6 + out4 + out2 + out1\n",
    "        out = self.padding1(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.padding2(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.outconv(out)\n",
    "        \n",
    "        return out           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Functional API Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, activation='relu', padding=1, dilation_rate=1):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    x = keras.layers.Conv2D(256, (1, 1), padding='same', activation=activation)(x)\n",
    "#     x = keras.layers.ZeroPadding2D(padding=(padding, padding))(x)\n",
    "    out = keras.layers.Conv2D(64, (3, 3), padding='same', dilation_rate=dilation_rate, activation=activation)(x)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def ddcb(inputs):\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    \n",
    "    x1 = conv_block(inputs)\n",
    "    x2 = keras.layers.concatenate([inputs, x1])\n",
    "    x3 = conv_block(x2, padding=2, dilation_rate=2)\n",
    "    x4 = keras.layers.concatenate([inputs, x1, x3])\n",
    "    x5 = conv_block(x4, padding=3, dilation_rate=3)\n",
    "    x6 = keras.layers.concatenate([inputs, x3, x5])\n",
    "#     x7 = keras.layers.ZeroPadding2D(padding=(1,1))(x6)\n",
    "    out = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x6)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseScaleNetF:\n",
    "    '''\n",
    "        TODO: Docstring\n",
    "    '''\n",
    "    def __init__(self, input_shape, pretrained=False):\n",
    "        self.input_shape = input_shape\n",
    "        self.backbone = self.__create_backbone(input_shape, pretrained)\n",
    "#         self.padding1 = keras.layers.ZeroPadding2D(padding=(1,1))\n",
    "        self.conv1 = keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\n",
    "#         self.padding2 = keras.layers.ZeroPadding2D(padding=(1,1))\n",
    "        self.conv2 = keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')\n",
    "        self.outconv = keras.layers.Conv2D(1, (1, 1), activation='relu')\n",
    "    \n",
    "    def __create_backbone(self, input_shape, pretrained):\n",
    "        '''\n",
    "            TODO: Add Doctstring\n",
    "        '''\n",
    "        weights = None\n",
    "        if pretrained:\n",
    "            weights = 'imagenet'\n",
    "        \n",
    "        vgg16_model = keras.applications.vgg16.VGG16(weights=weights, \n",
    "                                                     input_shape=input_shape,\n",
    "                                                    include_top=False)\n",
    "        model = keras.models.Sequential()\n",
    "        \n",
    "        # we want to copy the first ten layers from VGG16\n",
    "        for layer in vgg16_model.layers[:14]:\n",
    "            model.add(layer)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_model(self):\n",
    "        '''\n",
    "            TODO: Docstring\n",
    "        '''\n",
    "        \n",
    "        # funny input names to be able to track input and output\n",
    "        inputs = keras.layers.Input(shape=self.input_shape)\n",
    "        bckb_x = self.backbone(inputs)\n",
    "        ddcb1_x = ddcb(bckb_x)\n",
    "        add1_x = keras.layers.add([ddcb1_x, bckb_x])\n",
    "        ddcb2_x = ddcb(add1_x)\n",
    "        add2_x = keras.layers.add([ddcb2_x, ddcb1_x, bckb_x])\n",
    "        ddcb3_x = ddcb(add2_x)\n",
    "        add3_x = keras.layers.add([ddcb3_x, ddcb2_x, ddcb1_x, bckb_x])\n",
    "#         pad1_x = self.padding1(add3_x)\n",
    "        conv1_x = self.conv1(add3_x)\n",
    "#         pad2_x = self.padding2(conv1_x)\n",
    "        conv2_x = self.conv2(conv1_x)\n",
    "        out = self.outconv(conv2_x)\n",
    "        # create the model\n",
    "        model = keras.Model(inputs=inputs, outputs=out)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Neccessary Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.mae_logs = []\n",
    "        self.mse_logs = []\n",
    "\n",
    "    def set_dataset_length(self, ds_lenght):\n",
    "        self.dslength = ds_lenght    \n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.acutal_mae = 0\n",
    "        self.actual_mse = 0\n",
    "        self.sum_mae = 0\n",
    "        self.sum_mse = 0        \n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # update the metrics\n",
    "        if logs.get('mae') is None:\n",
    "            current_mae = 0\n",
    "            current_mse = 0\n",
    "        else:\n",
    "            current_mae = logs.get('mae')\n",
    "            current_mse = logs.get('mse')\n",
    "\n",
    "        self.sum_mae += current_mae\n",
    "        self.sum_mse += current_mse\n",
    "        # update the actual metrics\n",
    "        self.actual_mae = self.sum_mae #/ (logs.get('batch') + 1)\n",
    "        self.actual_mse = self.sum_mse #/ (logs.get('batch') + 1)\n",
    "        # print the results\n",
    "        \n",
    "        # finally append the logs, just for safe keeping\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.mae_logs.append(logs.get('mae'))\n",
    "        self.mse_logs.append(logs.get('mse'))\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        agg_mae = self.sum_mae / self.dslength\n",
    "        agg_mse = self.sum_mse / self.dslength\n",
    "        print(f'Aggregated: {round(agg_mae, 4)}, {round(agg_mse, 4)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = LossHistory()\n",
    "history.set_dataset_length(len(train_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath='checkpoints/all/weights.{mae:02f}-{epoch:02d}.hdf5', \n",
    "                                             monitor='mae',\n",
    "                                             verbose=1,\n",
    "                                             save_best_only=False,\n",
    "                                             mode='min',\n",
    "                                             period=1\n",
    "                                            )\n",
    "\n",
    "class BestCheckPoint(keras.callbacks.ModelCheckpoint):\n",
    "    def __init__(self, best_init=None, *arg, **kwagrs):\n",
    "        super().__init__(*arg, **kwagrs)\n",
    "        if best_init is not None:\n",
    "            self.best = best_init\n",
    "\n",
    "\n",
    "best_checkpoint = BestCheckPoint(filepath='checkpoints/best/weights_best.h5',\n",
    "                                 best_init=9999,\n",
    "                                 monitor='mae',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True\n",
    "                                )            \n",
    "\n",
    "# Tensorboard\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir='log/',\n",
    "    write_graph=True,\n",
    "    batch_size=train_batch_size,\n",
    "    write_images=True,\n",
    "    update_freq='batch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "calback_list = [\n",
    "    checkpoint,\n",
    "    best_checkpoint,\n",
    "    tensorboard,\n",
    "    history\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2.3 The Optimizer\n",
    "The paper uses an Adam optimizer with a learning rate of 5e-6.\n",
    "-- add weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Adding L2 Penalty\n",
    "As the original paper uses L2 regularization of the cost function to assist with model convergence I will leave it as an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_l2_reg(model, weight_decay):\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            layer.kernel_regularizer=keras.regularizers.l2(weight_decay)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Custom Loss\n",
    "\n",
    "**TODO:** Explanation for the custom loss from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    '''\n",
    "        Euclidean distance loss\n",
    "        https://en.wikipedia.org/wiki/Euclidean_distance\n",
    "    '''\n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true)))\n",
    "\n",
    "def average_pooling(input_tensor, shape):\n",
    "    '''\n",
    "        Pooling the input_layer according to the output size.\n",
    "    '''\n",
    "    \n",
    "    # 8 is the factor with which the model scales the input image\n",
    "    pool_shape_x = input_shape[1] // 8 // shape \n",
    "    pool_shape_y = input_shape[0] // 8 // shape\n",
    "\n",
    "    pool_shape = (pool_shape_x, pool_shape_y)\n",
    "    pool = K.pool2d(input_tensor, pool_shape, strides=pool_shape, pool_mode='avg')\n",
    "    \n",
    "    return pool\n",
    "    \n",
    "def mean_abs_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "    \n",
    "def multiscale_dens_scale_loss(y_true, y_pred):\n",
    "    '''\n",
    "        Calculate the multiscale density loss as defined in the paper.\n",
    "    '''\n",
    "    loss = 0\n",
    "    for i in [1, 2, 4]:\n",
    "        pool_y_true = average_pooling(y_true, i)\n",
    "        pool_y_pred = average_pooling(y_pred, i)\n",
    "        loss += mean_abs_error(pool_y_true, pool_y_pred)\n",
    "        \n",
    "    return loss\n",
    "    \n",
    "    \n",
    "def dsn_loss(y_true, y_pred):\n",
    "    Le = euclidean_distance_loss(y_true, y_pred)\n",
    "    Lc = multiscale_dens_scale_loss(y_true, y_pred)\n",
    "    lambd = 1000 # coming straight from the paper for the UCF dataset will move it to a global\n",
    "    return Le + lambd * Lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Custom Metrics\n",
    "Nothing out of the ordinary - just the metrics have to be evaluated on the count, not on the density maps themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    return K.abs(K.sum(y_pred) - K.sum(y_true))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return K.square(K.sum(y_pred) - K.sum(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Model Compilation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fury/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 480, 720, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 60, 90, 512)  7635264     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 60, 90, 256)  131328      sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 60, 90, 64)   147520      conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 60, 90, 576)  0           sequential_2[1][0]               \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 60, 90, 256)  147712      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 60, 90, 64)   147520      conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 60, 90, 640)  0           sequential_2[1][0]               \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 60, 90, 256)  164096      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 60, 90, 64)   147520      conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 60, 90, 640)  0           sequential_2[1][0]               \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 60, 90, 512)  2949632     concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 60, 90, 512)  0           conv2d_34[0][0]                  \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 60, 90, 256)  131328      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 60, 90, 64)   147520      conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 60, 90, 576)  0           add_4[0][0]                      \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 60, 90, 256)  147712      concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 60, 90, 64)   147520      conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 60, 90, 640)  0           add_4[0][0]                      \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 60, 90, 256)  164096      concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 60, 90, 64)   147520      conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 60, 90, 640)  0           add_4[0][0]                      \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 60, 90, 512)  2949632     concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 60, 90, 512)  0           conv2d_41[0][0]                  \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 60, 90, 256)  131328      add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 60, 90, 64)   147520      conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 60, 90, 576)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 60, 90, 256)  147712      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 60, 90, 64)   147520      conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 60, 90, 640)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 60, 90, 256)  164096      concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 60, 90, 64)   147520      conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 60, 90, 640)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 60, 90, 512)  2949632     concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 60, 90, 512)  0           conv2d_48[0][0]                  \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 60, 90, 128)  589952      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 60, 90, 64)   73792       conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 60, 90, 1)    65          conv2d_26[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,805,057\n",
      "Trainable params: 19,805,057\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('checkpoints/best/weights_best.h5'):\n",
    "    dsn_model = keras.models.load_model('checkpoints/best/weights_best.h5', custom_objects={'dsn_loss': dsn_loss,\n",
    "                                                                                           'mae': mae,\n",
    "                                                                                           'mse': mse})\n",
    "else:\n",
    "    dsn_model = DenseScaleNetF(input_shape=(input_shape[1], input_shape[0], 3), \n",
    "                               pretrained=False)\n",
    "    dsn_model = dsn_model.build_model()\n",
    "    dsn_model.compile(loss=dsn_loss,\n",
    "                     optimizer=optimizer,\n",
    "                     metrics=[mae, mse])\n",
    "    dsn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add reg\n",
    "dsn_model = add_l2_reg(dsn_model, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Fitting the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_steps = (0.9 * len(train_gen)) // train_batch_size\n",
    "# validation_steps = int(0.1 * len(train_gen))\n",
    "training_steps = len(train_gen) // train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fury/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/1\n",
      "1200/1200 [==============================] - 468s 390ms/step - loss: 1.5107 - mae: 2.5570 - mse: 70.6198\n",
      "\n",
      "Epoch 00001: saving model to checkpoints/all/weights.2.557007-01.hdf5\n",
      "\n",
      "Epoch 00001: mae improved from 9999.00000 to 2.55701, saving model to checkpoints/best/weights_best.h5\n",
      "Aggregated: 2.557, 70.6198\n",
      "200\n",
      "Report sent\n"
     ]
    }
   ],
   "source": [
    "dsn_model.fit_generator(train_iter,\n",
    "                       steps_per_epoch = training_steps,\n",
    "                       epochs = 1,\n",
    "                       callbacks=calback_list\n",
    "                       )\n",
    "report_to_overwatch('VM:ML:P', 'Atlas', 'Training of model done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.1 Load the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn_model = keras.models.load_model('checkpoints/custom_archive/weights_best.h5', custom_objects={'dsn_loss': dsn_loss,\n",
    "                                                                                           'mae': mae,\n",
    "                                                                                           'mse': mse})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.2 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn_model.evaluate_generator(test_iter, steps=len(test_gen), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still get totally different results from the ones on the paper... Have to look at those loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - 91s 271ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = dsn_model.predict_generator(test_iter, steps=len(test_gen), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

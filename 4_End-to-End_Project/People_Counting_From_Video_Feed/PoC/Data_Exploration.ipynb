{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Image Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIRAT_S_000002_fr_0.2\n"
     ]
    }
   ],
   "source": [
    "!ls ./example_dataset/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_file_extension(filename):\n",
    "    \n",
    "    file_extension = 'mp4'\n",
    "    \n",
    "    if file_extension in filename:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_videos_from_folder(path):\n",
    "    \n",
    "    all_items = os.listdir(path)\n",
    "    only_videos = [video_name for video_name in filter(filter_file_extension, all_items)]\n",
    "    video_filenames = [video_name.split('.')[0] for video_name in only_videos]\n",
    "    videos_file_paths = [os.path.join(path, video_fp) for video_fp in only_videos]\n",
    "    \n",
    "    return videos_file_paths, video_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_paths, video_filenames = get_videos_from_folder('./example_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame(sec, video_object):\n",
    "    '''\n",
    "        Based on the given sec, extract the resulting image from the video_object.\n",
    "    '''\n",
    "    video_object.set(cv2.CAP_PROP_POS_MSEC, sec*1000)\n",
    "    success, image = video_object.read()\n",
    "    return success, image\n",
    "    \n",
    "\n",
    "def write_images_from_video(video_path, video_filename, frame_rate):\n",
    "    '''\n",
    "        Splits the video by the frame_rate, shown in the video_path variable into different frames,\n",
    "        then writes the resulting images in a folder with the  video's filename.\n",
    "        Creates the folder if it exits.\n",
    "    \n",
    "    '''\n",
    "    video_object = cv2.VideoCapture(video_path)\n",
    "    sec = 0\n",
    "    count = 0\n",
    "    frame_rate = 1 / frame_rate # looking at the actual time from second to the next\n",
    "    success = 1\n",
    "    \n",
    "    while success:\n",
    "        \n",
    "        success, image = get_frame(sec, video_object)\n",
    "        \n",
    "        # create the folder if not exists\n",
    "        video_file_name = f'{video_filename}_fr_{frame_rate}'\n",
    "        folder_path = f'./poc_dataset/images/{video_file_name}'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        # write the image\n",
    "        cv2.imwrite(os.path.join(folder_path, f'{video_filename}_frame_{np.round(count,2)}.png'), image)\n",
    "        \n",
    "        sec = sec + frame_rate\n",
    "        count += 1\n",
    "    \n",
    "    print(f'File {video_filename} processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File VIRAT_S_000002 processed.\n"
     ]
    }
   ],
   "source": [
    "write_images_from_video(video_file_paths[0], video_filenames[0], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "Various helper functions that are used in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_datasets(path):\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    data = {}\n",
    "    for file in files:\n",
    "        filename = file.split('.')[0]\n",
    "        filepath = os.path.join(path, file)\n",
    "        data[filename] = pd.read_csv(filepath)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People Counting with Object Recognition Methods\n",
    "\n",
    "[Reference](https://machinelearningmastery.com/object-recognition-with-deep-learning/?fbclid=IwAR3QHzwV3iteB2tE7EJo3GkNTT_v7loLqHtCqYuH5nopySIkzvTmHiUa-H0)\n",
    "\n",
    "### 1. Faster R-CNN\n",
    "* [Reference of Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf)  \n",
    "* [Reference of implementation](https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a)\n",
    "* [Reference Jupyter Notebook for the implementation](https://github.com/RockyXu66/Faster_RCNN_for_Open_Images_Dataset_Keras/blob/master/frcnn_train_vgg.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Getting The Data:\n",
    "\n",
    "#### 1.1.1 Downloading the dataset information with the corresponding bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-26 21:53:32--  https://datasets.figure-eight.com/figure_eight_datasets/open-images/train-annotations-bbox.csv\n",
      "Resolving datasets.figure-eight.com (datasets.figure-eight.com)... 52.200.149.96, 3.227.119.162, 3.227.227.96\n",
      "Connecting to datasets.figure-eight.com (datasets.figure-eight.com)|52.200.149.96|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1194033454 (1,1G) [text/csv]\n",
      "Saving to: ‘train-annotations-bbox.csv’\n",
      "\n",
      "train-annotations-b 100%[===================>]   1,11G   448KB/s    in 72m 55s \n",
      "\n",
      "2019-11-26 23:06:28 (267 KB/s) - ‘train-annotations-bbox.csv’ saved [1194033454/1194033454]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://datasets.figure-eight.com/figure_eight_datasets/open-images/train-annotations-bbox.csv && \\\n",
    "!wget https://datasets.figure-eight.com/figure_eight_datasets/open-images/train-images-boxable.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_csv_datasets('./training_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train-images-boxable',\n",
       " 'train-annotations-bbox',\n",
       " 'class-descriptions-boxable']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(datasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Downloading the actual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldesc_df = datasets['class-descriptions-boxable']\n",
    "cldesc_df.columns = ['label_id', 'class']\n",
    "# get the Label Name that I need - person\n",
    "label_id = cldesc_df.loc[cldesc_df['class'] == 'Person', 'label_id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trannbb_df = datasets['train-annotations-bbox']\n",
    "trimb_df = datasets['train-images-boxable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_url</th>\n",
       "      <th>ImageID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e39871fd9fd74f55.jpg</td>\n",
       "      <td>https://requestor-proxy.figure-eight.com/figur...</td>\n",
       "      <td>e39871fd9fd74f55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f18b91585c4d3f3e.jpg</td>\n",
       "      <td>https://requestor-proxy.figure-eight.com/figur...</td>\n",
       "      <td>f18b91585c4d3f3e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ede6e66b2fb59aab.jpg</td>\n",
       "      <td>https://requestor-proxy.figure-eight.com/figur...</td>\n",
       "      <td>ede6e66b2fb59aab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ed600d57fcee4f94.jpg</td>\n",
       "      <td>https://requestor-proxy.figure-eight.com/figur...</td>\n",
       "      <td>ed600d57fcee4f94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ff47e649b23f446d.jpg</td>\n",
       "      <td>https://requestor-proxy.figure-eight.com/figur...</td>\n",
       "      <td>ff47e649b23f446d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_name                                          image_url  \\\n",
       "0  e39871fd9fd74f55.jpg  https://requestor-proxy.figure-eight.com/figur...   \n",
       "1  f18b91585c4d3f3e.jpg  https://requestor-proxy.figure-eight.com/figur...   \n",
       "2  ede6e66b2fb59aab.jpg  https://requestor-proxy.figure-eight.com/figur...   \n",
       "3  ed600d57fcee4f94.jpg  https://requestor-proxy.figure-eight.com/figur...   \n",
       "4  ff47e649b23f446d.jpg  https://requestor-proxy.figure-eight.com/figur...   \n",
       "\n",
       "            ImageID  \n",
       "0  e39871fd9fd74f55  \n",
       "1  f18b91585c4d3f3e  \n",
       "2  ede6e66b2fb59aab  \n",
       "3  ed600d57fcee4f94  \n",
       "4  ff47e649b23f446d  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an image id from the name\n",
    "trimb_df['ImageID'] = trimb_df['image_name'].apply(lambda x: x.split('.')[0])\n",
    "trimb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 100000\n",
    "person_bb_df = trannbb_df[trannbb_df['LabelName'] == label_id] # getting a filtered dataset of only images that contain persons\n",
    "image_ids = list(trannbb_df.loc[trannbb_df['LabelName'] == label_id, 'ImageID'].unique())[:num_images] # get a list with the image ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered df based on the image ids\n",
    "filtered_trimb_df = trimb_df[trimb_df['ImageID'].isin(image_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_write_img(url, filename, path):\n",
    "    r = requests.get(url)\n",
    "    filepath = os.path.join(path, filename + '.jpg')\n",
    "    if r.status_code == 200:\n",
    "        with open(filepath, 'wb') as w:\n",
    "            w.write(r.content)\n",
    "            \n",
    "def download_images(df, path):\n",
    "    # hardcoding column names is not a good idea... \n",
    "    for imid, url in zip(df['ImageID'].values, df['image_url'].values):\n",
    "        pull_write_img(url, imid, path)\n",
    "    \n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "download_images(filtered_trimb_df, './training_dataset/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Implemented Faster RCNN\n",
    "Faster RCNN is the third itteration of the RCNN implementation, with the advantage of being faster. This is due to implementing a RPN (region proposal network) instead of using SS (selective search). More details can be found in the following papers:\n",
    "* [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n",
    "* [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n",
    "* [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. YOLO\n",
    "The YOLO (You Only Look Once) Algorithm for Object Detection. You can find more details by reading the paper:\n",
    "* [YOLOv3: An Incremental Improvement](https://pjreddie.com/media/files/papers/YOLOv3.pdf).  \n",
    "For the purpose of the PoC a pretrained model will be used, or more specificaly [keras-yolo3](https://github.com/experiencor/keras-yolo3). \n",
    "\n",
    "#### 2.1 Cloning the Repo and Downloading the pretrained model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into './yolo3'...\n",
      "remote: Enumerating objects: 168, done.\u001b[K\n",
      "remote: Total 168 (delta 0), reused 0 (delta 0), pack-reused 168\u001b[K\n",
      "Receiving objects: 100% (168/168), 81.77 KiB | 581.00 KiB/s, done.\n",
      "Resolving deltas: 100% (91/91), done.\n"
     ]
    }
   ],
   "source": [
    "# clone the keras-yolo3\n",
    "!git clone https://github.com/experiencor/keras-yolo3.git ./yolo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-30 19:14:55--  https://pjreddie.com/media/files/yolov3.weights\n",
      "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
      "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: ‘./yolo3/weights/yolov3.weights’\n",
      "\n",
      "yolov3.weights      100%[===================>] 236,52M   810KB/s    in 7m 5s   \n",
      "\n",
      "2019-11-30 19:22:01 (569 KB/s) - ‘./yolo3/weights/yolov3.weights’ saved [248007048/248007048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the model weights\n",
    "!wget -P ./yolo3/weights/ https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing the module\n",
    "from yolo3.yolo3_one_file_to_detect_them_all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fury/anaconda3/envs/opencv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "loading weights of convolution #0\n",
      "loading weights of convolution #1\n",
      "loading weights of convolution #2\n",
      "loading weights of convolution #3\n",
      "no convolution #4\n",
      "loading weights of convolution #5\n",
      "loading weights of convolution #6\n",
      "loading weights of convolution #7\n",
      "no convolution #8\n",
      "loading weights of convolution #9\n",
      "loading weights of convolution #10\n",
      "no convolution #11\n",
      "loading weights of convolution #12\n",
      "loading weights of convolution #13\n",
      "loading weights of convolution #14\n",
      "no convolution #15\n",
      "loading weights of convolution #16\n",
      "loading weights of convolution #17\n",
      "no convolution #18\n",
      "loading weights of convolution #19\n",
      "loading weights of convolution #20\n",
      "no convolution #21\n",
      "loading weights of convolution #22\n",
      "loading weights of convolution #23\n",
      "no convolution #24\n",
      "loading weights of convolution #25\n",
      "loading weights of convolution #26\n",
      "no convolution #27\n",
      "loading weights of convolution #28\n",
      "loading weights of convolution #29\n",
      "no convolution #30\n",
      "loading weights of convolution #31\n",
      "loading weights of convolution #32\n",
      "no convolution #33\n",
      "loading weights of convolution #34\n",
      "loading weights of convolution #35\n",
      "no convolution #36\n",
      "loading weights of convolution #37\n",
      "loading weights of convolution #38\n",
      "loading weights of convolution #39\n",
      "no convolution #40\n",
      "loading weights of convolution #41\n",
      "loading weights of convolution #42\n",
      "no convolution #43\n",
      "loading weights of convolution #44\n",
      "loading weights of convolution #45\n",
      "no convolution #46\n",
      "loading weights of convolution #47\n",
      "loading weights of convolution #48\n",
      "no convolution #49\n",
      "loading weights of convolution #50\n",
      "loading weights of convolution #51\n",
      "no convolution #52\n",
      "loading weights of convolution #53\n",
      "loading weights of convolution #54\n",
      "no convolution #55\n",
      "loading weights of convolution #56\n",
      "loading weights of convolution #57\n",
      "no convolution #58\n",
      "loading weights of convolution #59\n",
      "loading weights of convolution #60\n",
      "no convolution #61\n",
      "loading weights of convolution #62\n",
      "loading weights of convolution #63\n",
      "loading weights of convolution #64\n",
      "no convolution #65\n",
      "loading weights of convolution #66\n",
      "loading weights of convolution #67\n",
      "no convolution #68\n",
      "loading weights of convolution #69\n",
      "loading weights of convolution #70\n",
      "no convolution #71\n",
      "loading weights of convolution #72\n",
      "loading weights of convolution #73\n",
      "no convolution #74\n",
      "loading weights of convolution #75\n",
      "loading weights of convolution #76\n",
      "loading weights of convolution #77\n",
      "loading weights of convolution #78\n",
      "loading weights of convolution #79\n",
      "loading weights of convolution #80\n",
      "loading weights of convolution #81\n",
      "no convolution #82\n",
      "no convolution #83\n",
      "loading weights of convolution #84\n",
      "no convolution #85\n",
      "no convolution #86\n",
      "loading weights of convolution #87\n",
      "loading weights of convolution #88\n",
      "loading weights of convolution #89\n",
      "loading weights of convolution #90\n",
      "loading weights of convolution #91\n",
      "loading weights of convolution #92\n",
      "loading weights of convolution #93\n",
      "no convolution #94\n",
      "no convolution #95\n",
      "loading weights of convolution #96\n",
      "no convolution #97\n",
      "no convolution #98\n",
      "loading weights of convolution #99\n",
      "loading weights of convolution #100\n",
      "loading weights of convolution #101\n",
      "loading weights of convolution #102\n",
      "loading weights of convolution #103\n",
      "loading weights of convolution #104\n",
      "loading weights of convolution #105\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = make_yolov3_model()\n",
    "# load the model weights\n",
    "weight_reader = WeightReader('./yolo3/weights/yolov3.weights')\n",
    "# set the model wieghts into the model\n",
    "weight_reader.load_weights(model)\n",
    "# save the model to file\n",
    "model.save('./yolo3/weights/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 OpenCV",
   "language": "python",
   "name": "opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 Example\n",
    "`INTRO Placeholder`\n",
    "\n",
    "### What is CIFAR 10?\n",
    "Taken straight from the [website](https://www.cs.toronto.edu/~kriz/cifar.html):\n",
    "\"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\"\n",
    "\n",
    "### What is the goal?\n",
    "As the explanation says - use the test set and see what scores I can get using a ConvNet (via PyTorch).\n",
    "\n",
    "### Project Structure:\n",
    "(to be updated)\n",
    "1. Loading the data\n",
    "...\n",
    "\n",
    "### References\n",
    "* [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Loading the data.\n",
    "Refering to the dataset's [website](https://www.cs.toronto.edu/~kriz/cifar.html) it is quite easy to figure out the structure:\n",
    "The dataset is separated into 5 batches with an additional test set. The author even includes a tiny script to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries to help with the loading of the dataset\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict1 = pickle.load(fo, encoding='bytes')\n",
    "    return dict1\n",
    "\n",
    "def load_data(dataset_path):\n",
    "    # get all the items in the folder\n",
    "    batch_list = os.listdir(dataset_path)\n",
    "    # since we dont need the html and the metal files - skip them\n",
    "    batch_list = [item for item in batch_list if item != 'readme.html' if item != 'batches.meta']\n",
    "    # create a master dict to store all of the data\n",
    "    data_dict = {}\n",
    "    # print the items, so we may have a reference further down\n",
    "    print(batch_list)\n",
    "    # load all of the items\n",
    "    for item in batch_list:\n",
    "        data_dict[item] = unpickle(dataset_path + item)\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_batch_2', 'data_batch_3', 'data_batch_1', 'data_batch_5', 'test_batch', 'data_batch_4']\n"
     ]
    }
   ],
   "source": [
    "data = load_data('./datasets/cifar-10-python/cifar-10-batches-py/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

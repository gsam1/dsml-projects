{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 Example\n",
    "Since my experience until now has been mostly with Tensorflow/Keras, this notebook is intended to play around with PyTorch and run through their tutorial for training on the CIFAR10 dataset. In addition - it will be all done on the gpu.\n",
    "\n",
    "### What is CIFAR 10?\n",
    "Taken straight from the [website](https://www.cs.toronto.edu/~kriz/cifar.html):\n",
    "\"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\"\n",
    "\n",
    "### What is the goal?\n",
    "As an introduction to PyTorch - use the tutorial, get its results and improve them.\n",
    "\n",
    "### Project Structure:\n",
    "\n",
    "1. Loading the data & Loading the libraries.\n",
    "2. Showing a sample of images.\n",
    "3. Defining and training a convolutional neural network.\n",
    "    * The plan here is to explore the implementation of different architectures.\n",
    "4. Evaluating the results.\n",
    "5. Improving the results.\n",
    "    * Different optimizers.\n",
    "    * Transfer Learning.\n",
    "\n",
    "...\n",
    "\n",
    "### References\n",
    "* [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky\n",
    "* [Training a classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data & Loading the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to the GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will try to mimic the tutorial from Torch's website then implement it my way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Image batch dimensions: torch.Size([32, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./datasets', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./datasets', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in trainloader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Showing a sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse  deer  ship  frog\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[0:4]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the provided CNN architecture I will design a couple of my own architectures to get the hang of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Simple Net\n",
    "Just two convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=128 * 14 * 14, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # 1\n",
    "        output = self.conv1(inputs)\n",
    "        output = self.relu1(output)\n",
    "        # 2\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        # 3 pool\n",
    "        output = self.pool(output)\n",
    "        # 4 view\n",
    "        output = output.view(-1, 128 * 14 * 14)\n",
    "        # 5\n",
    "        output = F.relu(self.fc1(output))\n",
    "        logits = self.fc2(output)\n",
    "        probas = F.softmax(logits, 1)\n",
    "        \n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 30, 30]           1,792\n",
      "              ReLU-2           [-1, 64, 30, 30]               0\n",
      "            Conv2d-3          [-1, 128, 28, 28]          73,856\n",
      "              ReLU-4          [-1, 128, 28, 28]               0\n",
      "         MaxPool2d-5          [-1, 128, 14, 14]               0\n",
      "            Linear-6                   [-1, 64]       1,605,696\n",
      "            Linear-7                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 1,681,994\n",
      "Trainable params: 1,681,994\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.60\n",
      "Params size (MB): 6.42\n",
      "Estimated Total Size (MB): 9.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "simple_net = SimpleNet().to(device)\n",
    "summary(simple_net, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Deeper Net\n",
    "No fancy stuff though (Dropout, BatchNorm, etc, etc), just Deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeeperNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256 * 7 * 7, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=10)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # 1\n",
    "        output = self.conv1(inputs)\n",
    "        output = self.relu1(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        # 2 pool\n",
    "        output = self.pool1(output)\n",
    "        # 3 \n",
    "        output = self.conv3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.conv4(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.conv5(output)\n",
    "        output = self.relu5(output)\n",
    "        # 4 pool\n",
    "        output = self.pool2(output)\n",
    "        # 4 view\n",
    "        output = output.view(-1, 256 * 7 * 7)\n",
    "        # 5\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = F.relu(self.fc3(output))\n",
    "        logits = self.fc4(output)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        \n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 30, 30]           1,792\n",
      "              ReLU-2           [-1, 64, 30, 30]               0\n",
      "            Conv2d-3          [-1, 128, 28, 28]          73,856\n",
      "              ReLU-4          [-1, 128, 28, 28]               0\n",
      "         MaxPool2d-5          [-1, 128, 14, 14]               0\n",
      "            Conv2d-6           [-1, 64, 14, 14]           8,256\n",
      "              ReLU-7           [-1, 64, 14, 14]               0\n",
      "            Conv2d-8          [-1, 128, 14, 14]           8,320\n",
      "              ReLU-9          [-1, 128, 14, 14]               0\n",
      "           Conv2d-10          [-1, 256, 14, 14]          33,024\n",
      "             ReLU-11          [-1, 256, 14, 14]               0\n",
      "        MaxPool2d-12            [-1, 256, 7, 7]               0\n",
      "           Linear-13                  [-1, 128]       1,605,760\n",
      "           Linear-14                  [-1, 128]          16,512\n",
      "           Linear-15                   [-1, 64]           8,256\n",
      "           Linear-16                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 1,756,426\n",
      "Trainable params: 1,756,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.04\n",
      "Params size (MB): 6.70\n",
      "Estimated Total Size (MB): 10.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "deeper_net = DeeperNet().to(device)\n",
    "summary(deeper_net, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Fancy Net\n",
    "Nothing really fancy about it, but just implementing batch normalization and making a singular unit of convolution and batch-norm - no big idea behind it, just for experimenting with pytorch. Also trying to experiment I will try to make the network more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a unit of the architecture containing a single convolution, batch normalization and activator\n",
    "class Unit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(Unit, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.conv(inputs)\n",
    "        output = self.bn(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(FancyNet, self).__init__()\n",
    "        self.unit1 = Unit(in_channels=3, out_channels=32, kernel_size=1)\n",
    "        self.unit2 = Unit(in_channels=32, out_channels=32, kernel_size=1)\n",
    "        self.unit3 = Unit(in_channels=32, out_channels=32, kernel_size=1)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.unit4 = Unit(in_channels=32, out_channels=64)\n",
    "        self.unit5 = Unit(in_channels=64, out_channels=64)\n",
    "        self.unit6 = Unit(in_channels=64, out_channels=64)\n",
    "        self.unit6 = Unit(in_channels=64, out_channels=64)\n",
    "        \n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.net = nn.Sequential(self.unit1, self.unit2, self.unit3, self.pool1, \\\n",
    "                                self.unit4, self.unit5, self.unit6, self.pool2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64*5*5, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = self.net(inputs)\n",
    "        output = output.view(-1, 64*5*5)\n",
    "        logits = self.fc1(output)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        \n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             128\n",
      "            Conv2d-2           [-1, 32, 32, 32]             128\n",
      "       BatchNorm2d-3           [-1, 32, 32, 32]              64\n",
      "       BatchNorm2d-4           [-1, 32, 32, 32]              64\n",
      "              ReLU-5           [-1, 32, 32, 32]               0\n",
      "              ReLU-6           [-1, 32, 32, 32]               0\n",
      "              Unit-7           [-1, 32, 32, 32]               0\n",
      "              Unit-8           [-1, 32, 32, 32]               0\n",
      "            Conv2d-9           [-1, 32, 32, 32]           1,056\n",
      "           Conv2d-10           [-1, 32, 32, 32]           1,056\n",
      "      BatchNorm2d-11           [-1, 32, 32, 32]              64\n",
      "      BatchNorm2d-12           [-1, 32, 32, 32]              64\n",
      "             ReLU-13           [-1, 32, 32, 32]               0\n",
      "             ReLU-14           [-1, 32, 32, 32]               0\n",
      "             Unit-15           [-1, 32, 32, 32]               0\n",
      "             Unit-16           [-1, 32, 32, 32]               0\n",
      "           Conv2d-17           [-1, 32, 32, 32]           1,056\n",
      "           Conv2d-18           [-1, 32, 32, 32]           1,056\n",
      "      BatchNorm2d-19           [-1, 32, 32, 32]              64\n",
      "      BatchNorm2d-20           [-1, 32, 32, 32]              64\n",
      "             ReLU-21           [-1, 32, 32, 32]               0\n",
      "             ReLU-22           [-1, 32, 32, 32]               0\n",
      "             Unit-23           [-1, 32, 32, 32]               0\n",
      "             Unit-24           [-1, 32, 32, 32]               0\n",
      "        MaxPool2d-25           [-1, 32, 16, 16]               0\n",
      "        MaxPool2d-26           [-1, 32, 16, 16]               0\n",
      "           Conv2d-27           [-1, 64, 14, 14]          18,496\n",
      "           Conv2d-28           [-1, 64, 14, 14]          18,496\n",
      "      BatchNorm2d-29           [-1, 64, 14, 14]             128\n",
      "      BatchNorm2d-30           [-1, 64, 14, 14]             128\n",
      "             ReLU-31           [-1, 64, 14, 14]               0\n",
      "             ReLU-32           [-1, 64, 14, 14]               0\n",
      "             Unit-33           [-1, 64, 14, 14]               0\n",
      "             Unit-34           [-1, 64, 14, 14]               0\n",
      "           Conv2d-35           [-1, 64, 12, 12]          36,928\n",
      "           Conv2d-36           [-1, 64, 12, 12]          36,928\n",
      "      BatchNorm2d-37           [-1, 64, 12, 12]             128\n",
      "      BatchNorm2d-38           [-1, 64, 12, 12]             128\n",
      "             ReLU-39           [-1, 64, 12, 12]               0\n",
      "             ReLU-40           [-1, 64, 12, 12]               0\n",
      "             Unit-41           [-1, 64, 12, 12]               0\n",
      "             Unit-42           [-1, 64, 12, 12]               0\n",
      "           Conv2d-43           [-1, 64, 10, 10]          36,928\n",
      "           Conv2d-44           [-1, 64, 10, 10]          36,928\n",
      "      BatchNorm2d-45           [-1, 64, 10, 10]             128\n",
      "      BatchNorm2d-46           [-1, 64, 10, 10]             128\n",
      "             ReLU-47           [-1, 64, 10, 10]               0\n",
      "             ReLU-48           [-1, 64, 10, 10]               0\n",
      "             Unit-49           [-1, 64, 10, 10]               0\n",
      "             Unit-50           [-1, 64, 10, 10]               0\n",
      "        MaxPool2d-51             [-1, 64, 5, 5]               0\n",
      "        MaxPool2d-52             [-1, 64, 5, 5]               0\n",
      "           Linear-53                   [-1, 10]          16,010\n",
      "================================================================\n",
      "Total params: 206,346\n",
      "Trainable params: 206,346\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 7.87\n",
      "Params size (MB): 0.79\n",
      "Estimated Total Size (MB): 8.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fancy_net = FancyNet().to(device)\n",
    "summary(fancy_net, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- ResNet\n",
    "Implementing the [ResNet-101](http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html) in pytorch. Not going to lie - huge help from [Sebastian Rashka's repo](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb). In addition [this reference](https://github.com/rasbt/deeplearning-models/blob/fb41fbf2b96ac959dfead38db4823617660d5bec/pytorch_ipynb/cnn/resnet-ex-1.ipynb) was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the residual block first\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=channels[0],\n",
    "                                      out_channels=channels[1],\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(2, 2),\n",
    "                                      padding=1)\n",
    "        self.conv1_bn = torch.nn.BatchNorm2d(channels[1])\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=channels[1],\n",
    "                                      out_channels=channels[2],\n",
    "                                      kernel_size=(1,1),\n",
    "                                      stride=(1,1),\n",
    "                                      padding=0)\n",
    "        self.conv2_bn = torch.nn.BatchNorm2d(channels[2])\n",
    "        \n",
    "        self.conv_shortcut1 = torch.nn.Conv2d(in_channels=channels[0],\n",
    "                                                out_channels=channels[2],\n",
    "                                                kernel_size=(1,1),\n",
    "                                                stride=(2,2),\n",
    "                                                padding=0)\n",
    "        self.conv_shortcut1_bn = torch.nn.BatchNorm2d(channels[2])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # set aside the skip connection\n",
    "        shortcut = inputs\n",
    "        \n",
    "        # first conv connection\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.conv1_bn(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # second conv\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv2_bn(out)\n",
    "        \n",
    "        # now for the shortcut - linearly combine the shortcut and the regular layers - no non-linearity (relu)\n",
    "        shortcut = self.conv_shortcut1(shortcut)\n",
    "        shortcut = self.conv_shortcut1_bn(shortcut)\n",
    "                \n",
    "        out += shortcut\n",
    "        \n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.residual_block_1 = ResidualBlock(channels=[3, 8, 16])\n",
    "        self.residual_block_2 = ResidualBlock(channels=[16, 32, 64])\n",
    "    \n",
    "        self.linear_1 = torch.nn.Linear(8*8*64, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        out = self.residual_block_1.forward(inputs)\n",
    "        out = self.residual_block_2.forward(out)\n",
    "        logits = self.linear_1(out.view(-1, 8*8*64))\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 16, 16]             224\n",
      "       BatchNorm2d-2            [-1, 8, 16, 16]              16\n",
      "            Conv2d-3           [-1, 16, 16, 16]             144\n",
      "       BatchNorm2d-4           [-1, 16, 16, 16]              32\n",
      "            Conv2d-5           [-1, 16, 16, 16]              64\n",
      "       BatchNorm2d-6           [-1, 16, 16, 16]              32\n",
      "            Conv2d-7             [-1, 32, 8, 8]           4,640\n",
      "       BatchNorm2d-8             [-1, 32, 8, 8]              64\n",
      "            Conv2d-9             [-1, 64, 8, 8]           2,112\n",
      "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
      "           Conv2d-11             [-1, 64, 8, 8]           1,088\n",
      "      BatchNorm2d-12             [-1, 64, 8, 8]             128\n",
      "           Linear-13                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 49,642\n",
      "Trainable params: 49,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.31\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 0.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNet(num_classes=10).to(device)\n",
    "summary(resnet, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defining a Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss\n",
    "\n",
    "def define_optimizer(optimizer, lr, momentum, network):\n",
    "    return optimizer(network.parameters(), lr, momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stick with Stochastic Gradient Descent for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple net\n",
    "sn_optimizer = define_optimizer(optim.SGD, 0.01, 0.9, simple_net)\n",
    "# deeper net\n",
    "dn_optimizer = define_optimizer(optim.SGD, 0.01, 0.9, deeper_net)\n",
    "# fancy net\n",
    "fn_optimizer = define_optimizer(optim.SGD, 0.01, 0.9, fancy_net)\n",
    "# resnet\n",
    "rn_optimizer = define_optimizer(optim.SGD, 0.01, 0.9, resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_network(network, optimizer, criterion, epochs, loader):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for i, data in enumerate(loader):\n",
    "            # inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # clear the gradients\n",
    "            # predict the parameters\n",
    "            logits, probas = network(inputs)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            # propagate the loss backwards\n",
    "            loss.backward()\n",
    "            \n",
    "            # adjust the parameters according to the gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print stats\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(probas, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            if i == (len(loader) - 1):\n",
    "                print(f'[epoch:{epoch + 1}] - loss: {running_loss / len(loader):.2f}; accuracy - {100 * correct / total:.2f}')\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.37; accuracy - 50.86\n",
      "[epoch:2] - loss: 0.94; accuracy - 66.90\n",
      "[epoch:3] - loss: 0.74; accuracy - 74.02\n",
      "[epoch:4] - loss: 0.55; accuracy - 80.48\n",
      "[epoch:5] - loss: 0.38; accuracy - 86.65\n",
      "[epoch:6] - loss: 0.24; accuracy - 91.45\n",
      "[epoch:7] - loss: 0.17; accuracy - 93.96\n",
      "[epoch:8] - loss: 0.13; accuracy - 95.41\n",
      "[epoch:9] - loss: 0.12; accuracy - 95.76\n",
      "[epoch:10] - loss: 0.10; accuracy - 96.58\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# simple net fit\n",
    "fit_network(simple_net, sn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 2.18; accuracy - 15.88\n",
      "[epoch:2] - loss: 1.51; accuracy - 44.41\n",
      "[epoch:3] - loss: 1.15; accuracy - 58.69\n",
      "[epoch:4] - loss: 0.92; accuracy - 67.48\n",
      "[epoch:5] - loss: 0.76; accuracy - 73.33\n",
      "[epoch:6] - loss: 0.64; accuracy - 77.62\n",
      "[epoch:7] - loss: 0.52; accuracy - 81.61\n",
      "[epoch:8] - loss: 0.44; accuracy - 84.73\n",
      "[epoch:9] - loss: 0.35; accuracy - 87.85\n",
      "[epoch:10] - loss: 0.28; accuracy - 90.05\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# deeper net fit\n",
    "fit_network(deeper_net, dn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.48; accuracy - 47.23\n",
      "[epoch:2] - loss: 1.14; accuracy - 59.61\n",
      "[epoch:3] - loss: 0.99; accuracy - 65.41\n",
      "[epoch:4] - loss: 0.88; accuracy - 69.19\n",
      "[epoch:5] - loss: 0.81; accuracy - 71.72\n",
      "[epoch:6] - loss: 0.75; accuracy - 73.78\n",
      "[epoch:7] - loss: 0.70; accuracy - 75.33\n",
      "[epoch:8] - loss: 0.66; accuracy - 77.18\n",
      "[epoch:9] - loss: 0.62; accuracy - 78.62\n",
      "[epoch:10] - loss: 0.58; accuracy - 79.68\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# fancy net fit\n",
    "fit_network(fancy_net, fn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 0.64; accuracy - 77.43\n",
      "[epoch:2] - loss: 0.62; accuracy - 78.11\n",
      "[epoch:3] - loss: 0.60; accuracy - 78.68\n",
      "[epoch:4] - loss: 0.58; accuracy - 79.29\n",
      "[epoch:5] - loss: 0.57; accuracy - 79.59\n",
      "[epoch:6] - loss: 0.55; accuracy - 80.24\n",
      "[epoch:7] - loss: 0.54; accuracy - 80.65\n",
      "[epoch:8] - loss: 0.53; accuracy - 81.06\n",
      "[epoch:9] - loss: 0.52; accuracy - 81.15\n",
      "[epoch:10] - loss: 0.51; accuracy - 81.61\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "fit_network(resnet, rn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the Results\n",
    "\n",
    "As defined in the tutorial there are two functions we are generally interested in - the ConvNet accuracy and the class accuracy. Let's use the code from the tutorial and convert it to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_accuracy(nnet, name=\"default\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits, probas = nnet(images)\n",
    "            _, predicted = torch.max(probas, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the {name} network on the 10000 test {100 * correct / total}')\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_class_accuracy(nnet, name='default'):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits, probas = nnet(images)\n",
    "            _, predicted = torch.max(probas, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    print(f'Accuracy of classes of the {name} network:')\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... so how accurates are the models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the simple network on the 10000 test 68.06\n",
      "Accuracy of the deeper network on the 10000 test 72.31\n",
      "Accuracy of the fancy network on the 10000 test 70.79\n",
      "Accuracy of the resnet network on the 10000 test 63.36\n"
     ]
    }
   ],
   "source": [
    "sn_accuracy = net_accuracy(simple_net, 'simple')\n",
    "dn_accuracy = net_accuracy(deeper_net, 'deeper')\n",
    "fn_accuracy = net_accuracy(fancy_net, 'fancy')\n",
    "rn_accuracy = net_accuracy(resnet, 'resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classes of the simple network:\n",
      "Accuracy of plane : 70 %\n",
      "Accuracy of   car : 88 %\n",
      "Accuracy of  bird : 51 %\n",
      "Accuracy of   cat : 49 %\n",
      "Accuracy of  deer : 61 %\n",
      "Accuracy of   dog : 65 %\n",
      "Accuracy of  frog : 74 %\n",
      "Accuracy of horse : 71 %\n",
      "Accuracy of  ship : 80 %\n",
      "Accuracy of truck : 80 %\n",
      "Accuracy of classes of the deeper network:\n",
      "Accuracy of plane : 81 %\n",
      "Accuracy of   car : 76 %\n",
      "Accuracy of  bird : 58 %\n",
      "Accuracy of   cat : 54 %\n",
      "Accuracy of  deer : 60 %\n",
      "Accuracy of   dog : 61 %\n",
      "Accuracy of  frog : 87 %\n",
      "Accuracy of horse : 74 %\n",
      "Accuracy of  ship : 87 %\n",
      "Accuracy of truck : 79 %\n",
      "Accuracy of classes of the fancy network:\n",
      "Accuracy of plane : 76 %\n",
      "Accuracy of   car : 80 %\n",
      "Accuracy of  bird : 60 %\n",
      "Accuracy of   cat : 41 %\n",
      "Accuracy of  deer : 62 %\n",
      "Accuracy of   dog : 60 %\n",
      "Accuracy of  frog : 79 %\n",
      "Accuracy of horse : 72 %\n",
      "Accuracy of  ship : 83 %\n",
      "Accuracy of truck : 81 %\n",
      "Accuracy of classes of the resnet network:\n",
      "Accuracy of plane : 64 %\n",
      "Accuracy of   car : 78 %\n",
      "Accuracy of  bird : 40 %\n",
      "Accuracy of   cat : 47 %\n",
      "Accuracy of  deer : 56 %\n",
      "Accuracy of   dog : 57 %\n",
      "Accuracy of  frog : 65 %\n",
      "Accuracy of horse : 68 %\n",
      "Accuracy of  ship : 68 %\n",
      "Accuracy of truck : 72 %\n"
     ]
    }
   ],
   "source": [
    "net_class_accuracy(simple_net, 'simple')\n",
    "net_class_accuracy(deeper_net, 'deeper')\n",
    "net_class_accuracy(fancy_net, 'fancy')\n",
    "net_class_accuracy(resnet, 'resnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving the results.\n",
    "Let's create a plan for what we want to explore and how it changes the results:\n",
    "1. Changing the optimizer to Adam.\n",
    "2. Adjusting Hyperparameters (lr).\n",
    "3. Dynamic LR adjustment.\n",
    "4. Transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Changing the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the networks\n",
    "simple_net = SimpleNet().to(device)\n",
    "deeper_net = DeeperNet().to(device)\n",
    "fancy_net = FancyNet().to(device)\n",
    "resnet = ResNet(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a new network, anet for short\n",
    "# Create the optimizer with the same learning rate\n",
    "a_sn_optimizer = optim.Adam(simple_net.parameters(), lr=0.01)\n",
    "a_dn_optimizer = optim.Adam(deeper_net.parameters(), lr=0.01)\n",
    "a_fn_optimizer = optim.Adam(fancy_net.parameters(), lr=0.01)\n",
    "a_rn_optimizer = optim.Adam(resnet.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.60; accuracy - 45.32\n",
      "[epoch:2] - loss: 1.28; accuracy - 56.45\n",
      "[epoch:3] - loss: 1.17; accuracy - 60.73\n",
      "[epoch:4] - loss: 1.09; accuracy - 63.76\n",
      "[epoch:5] - loss: 1.03; accuracy - 66.16\n",
      "[epoch:6] - loss: 0.97; accuracy - 68.47\n",
      "[epoch:7] - loss: 0.97; accuracy - 69.19\n",
      "[epoch:8] - loss: 0.89; accuracy - 71.77\n",
      "[epoch:9] - loss: 0.86; accuracy - 72.99\n",
      "[epoch:10] - loss: 0.82; accuracy - 74.26\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# simple net fit\n",
    "fit_network(simple_net, a_sn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.63; accuracy - 40.87\n",
      "[epoch:2] - loss: 1.40; accuracy - 50.76\n",
      "[epoch:3] - loss: 1.41; accuracy - 50.73\n",
      "[epoch:4] - loss: 1.34; accuracy - 52.79\n",
      "[epoch:5] - loss: 1.33; accuracy - 53.31\n",
      "[epoch:6] - loss: 1.35; accuracy - 53.02\n",
      "[epoch:7] - loss: 1.63; accuracy - 43.56\n",
      "[epoch:8] - loss: 1.53; accuracy - 45.85\n",
      "[epoch:9] - loss: 1.68; accuracy - 40.49\n",
      "[epoch:10] - loss: 1.72; accuracy - 39.54\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# deeper net fit\n",
    "fit_network(deeper_net, a_dn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.22; accuracy - 57.17\n",
      "[epoch:2] - loss: 1.03; accuracy - 64.36\n",
      "[epoch:3] - loss: 0.95; accuracy - 67.29\n",
      "[epoch:4] - loss: 0.89; accuracy - 69.22\n",
      "[epoch:5] - loss: 0.84; accuracy - 70.87\n",
      "[epoch:6] - loss: 0.79; accuracy - 72.54\n",
      "[epoch:7] - loss: 0.75; accuracy - 74.13\n",
      "[epoch:8] - loss: 0.71; accuracy - 75.27\n",
      "[epoch:9] - loss: 0.69; accuracy - 76.28\n",
      "[epoch:10] - loss: 0.66; accuracy - 77.01\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "fit_network(fancy_net, a_fn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.14; accuracy - 60.70\n",
      "[epoch:2] - loss: 1.05; accuracy - 63.81\n",
      "[epoch:3] - loss: 0.99; accuracy - 65.82\n",
      "[epoch:4] - loss: 0.96; accuracy - 66.81\n",
      "[epoch:5] - loss: 0.93; accuracy - 67.76\n",
      "[epoch:6] - loss: 0.90; accuracy - 68.64\n",
      "[epoch:7] - loss: 0.88; accuracy - 69.29\n",
      "[epoch:8] - loss: 0.86; accuracy - 70.44\n",
      "[epoch:9] - loss: 0.84; accuracy - 70.93\n",
      "[epoch:10] - loss: 0.83; accuracy - 71.54\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "fit_network(resnet, a_rn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the simple network on the 10000 test 57.47\n",
      "Accuracy of the deeper network on the 10000 test 47.34\n",
      "Accuracy of the fancy network on the 10000 test 68.8\n",
      "Accuracy of the resnet network on the 10000 test 61.51\n"
     ]
    }
   ],
   "source": [
    "sn_accuracy = net_accuracy(simple_net, 'simple')\n",
    "dn_accuracy = net_accuracy(deeper_net, 'deeper')\n",
    "fn_accuracy = net_accuracy(fancy_net, 'fancy')\n",
    "rn_accuracy = net_accuracy(resnet, 'resnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to perform worse than the simple SGD optimizer, but that is probably due to the learning rate. Let's adjust that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Adjusting the hyper-parameters.\n",
    "Let's try to adjust the learning rate to a more reasonable 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the networks\n",
    "simple_net = SimpleNet().to(device)\n",
    "deeper_net = DeeperNet().to(device)\n",
    "fancy_net = FancyNet().to(device)\n",
    "resnet = ResNet(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_sn_optimizer = optim.Adam(simple_net.parameters(), lr=0.001)\n",
    "a2_dn_optimizer = optim.Adam(deeper_net.parameters(), lr=0.001)\n",
    "a2_fn_optimizer = optim.Adam(fancy_net.parameters(), lr=0.001)\n",
    "a2_rn_optimizer = optim.Adam(resnet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.26; accuracy - 54.94\n",
      "[epoch:2] - loss: 0.90; accuracy - 68.38\n",
      "[epoch:3] - loss: 0.76; accuracy - 73.47\n",
      "[epoch:4] - loss: 0.64; accuracy - 77.65\n",
      "[epoch:5] - loss: 0.53; accuracy - 81.64\n",
      "[epoch:6] - loss: 0.43; accuracy - 84.89\n",
      "[epoch:7] - loss: 0.34; accuracy - 87.79\n",
      "[epoch:8] - loss: 0.28; accuracy - 90.18\n",
      "[epoch:9] - loss: 0.22; accuracy - 92.11\n",
      "[epoch:10] - loss: 0.18; accuracy - 93.40\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# simple net fit\n",
    "fit_network(simple_net, a2_sn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeper net fit\n",
    "fit_network(deeper_net, a2_dn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_network(fancy_net, a2_fn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_network(resnet, a2_rn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_accuracy = net_accuracy(simple_net, 'simple')\n",
    "dn_accuracy = net_accuracy(deeper_net, 'deeper')\n",
    "fn_accuracy = net_accuracy(fancy_net, 'fancy')\n",
    "rn_accuracy = net_accuracy(resnet, 'resnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net_class_accuracy(simple_net, 'simple')\n",
    "net_class_accuracy(deeper_net, 'deeper')\n",
    "net_class_accuracy(fancy_net, 'fancy')\n",
    "net_class_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Dynamic LR Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Using a pre-trained model.\n",
    "A way of increasing the accuracy of a CNN is to use the architecture and weights of a pre-trained model for our specific use case. I would have liked to test at least three of them, but let's start with VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers\n",
    "for parameter in model_vgg16.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "# remove the last layer\n",
    "num_features = model_vgg16.classifier[6].in_features\n",
    "features = list(model_vgg16.classifier.children())[:-1]\n",
    "# out features are 10 by design\n",
    "features.extend([nn.Linear(num_features, 10)])\n",
    "# replace the first layer as VGG accepts 224x224 images\n",
    "# features[0] = nn.Linear(512, 4096)\n",
    "# model_vgg16.classifier = nn.Sequential(*features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n",
      "Linear(in_features=25088, out_features=4096, bias=True)\n",
      "ReLU(inplace=True)\n",
      "Dropout(p=0.5, inplace=False)\n",
      "Linear(in_features=4096, out_features=4096, bias=True)\n",
      "ReLU(inplace=True)\n",
      "Dropout(p=0.5, inplace=False)\n",
      "Linear(in_features=4096, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for layer in model_vgg16.modules():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the loss and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimizer and the criterion itself\n",
    "criterion_vgg16 = nn.CrossEntropyLoss()\n",
    "optimizer_vgg16 = optim.Adam(model_vgg16.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the classifier trainable\n",
    "for name, child in model_vgg16.named_children():\n",
    "    if name == 'classifier':\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [32 x 25088], m2: [512 x 4096] at /opt/conda/conda-bld/pytorch_1570710718161/work/aten/src/THC/generic/THCTensorMathBlas.cu:290",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-48814c060856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_vgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# send to the gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_vgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-e07c70fd87b4>\u001b[0m in \u001b[0;36mfit_network\u001b[0;34m(network, optimizer, criterion, epochs, loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# predict the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# propagate the loss backwards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [32 x 25088], m2: [512 x 4096] at /opt/conda/conda-bld/pytorch_1570710718161/work/aten/src/THC/generic/THCTensorMathBlas.cu:290"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "model_vgg16.cuda() # send to the gpu\n",
    "model_vgg16 = fit_network(model_vgg16, optimizer_vgg16, criterion_vgg16, epochs=10, loader=trainloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [32 x 25088], m2: [512 x 4096] at /opt/conda/conda-bld/pytorch_1570710718161/work/aten/src/THC/generic/THCTensorMathBlas.cu:290",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1a135f4201e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_vgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# send to the gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_vgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Separator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'####-----####'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-e07c70fd87b4>\u001b[0m in \u001b[0;36mfit_network\u001b[0;34m(network, optimizer, criterion, epochs, loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# predict the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# propagate the loss backwards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [32 x 25088], m2: [512 x 4096] at /opt/conda/conda-bld/pytorch_1570710718161/work/aten/src/THC/generic/THCTensorMathBlas.cu:290"
     ]
    }
   ],
   "source": [
    "# Separator\n",
    "print('####-----####')\n",
    "# Evaluate\n",
    "net_accuracy(model_vgg16)\n",
    "net_class_accuracy(model_vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64% accuracy, a little less than my model. And even trained for longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion. For now.\n",
    "It seems that the maximum accuracy I got from CIFAR10 is 67%. What surprised me is that VGG16 didn't do much better than the custom model I came up with. In any case it is obvious that all models examined tend to overfit at some point. Some further steps that could be done are to:\n",
    "1. Implement dropout and regularization in my model.\n",
    "2. Implement batch normalization in both - my model and VGG16.\n",
    "3. Augment the input data by random transforms. \n",
    "4. Try out other pre-trained models.\n",
    "\n",
    "In any case I intend to revisit the problem and try out these steps, at least, in the near future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 Pytorch",
   "language": "python",
   "name": "torchpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

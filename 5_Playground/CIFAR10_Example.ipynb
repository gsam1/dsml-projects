{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 Example\n",
    "Since my experience until now has been mostly with Tensorflow/Keras, this notebook is intended to play around with PyTorch and run through their tutorial for training on the CIFAR10 dataset. In addition - it will be all done on the gpu.\n",
    "\n",
    "### What is CIFAR 10?\n",
    "Taken straight from the [website](https://www.cs.toronto.edu/~kriz/cifar.html):\n",
    "\"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\"\n",
    "\n",
    "### What is the goal?\n",
    "As an introduction to PyTorch - use the tutorial, get its results and improve them.\n",
    "\n",
    "### Project Structure:\n",
    "\n",
    "1. Loading the data & Loading the libraries.\n",
    "2. Showing a sample of images.\n",
    "3. Defining and training a convolutional neural network.\n",
    "    * The plan here is to explore the implementation of different architectures.\n",
    "4. Evaluating the results.\n",
    "5. Improving the results.\n",
    "    * Different optimizers.\n",
    "    * Transfer Learning.\n",
    "\n",
    "...\n",
    "\n",
    "### References\n",
    "* [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky\n",
    "* [Training a classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data & Loading the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to the GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will try to mimic the tutorial from Torch's website then implement it my way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./datasets', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./datasets', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Showing a sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " deer plane  frog  bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAB5CAYAAAAgYXpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO19aZBk2VXed3PPrKqsfemq6um9Z18ZjQaEsdA6khUMDghCmMByWBHzB9vgwAHC/MAK4wgIO8B2hI1DAQLhUCBkISyFjEHDaNACzNKzT3dP72t1dde+ZFXuef3jnPPOycqs6uruUVendb+Iisy67+V9975333vnnO8sznuPgICAgIDOQ2ynBxAQEBAQcHMID/CAgICADkV4gAcEBAR0KMIDPCAgIKBDER7gAQEBAR2K8AAPCAgI6FDc0gPcOfeUc+6Ec+60c+4z79agAgICAgKuD3ezfuDOuTiAkwA+DOAygJcB/Kz3/ti7N7yAgICAgM2QuIXfPgHgtPf+LAA4574E4GkAmz7Ac7mc7+vru4VDBgQEBPzgYXp6es57P7yx/VYe4BMALpn/LwN471Y/6OvrwzPPPHMLhwwICAj4wcNnP/vZC+3av+8kpnPuGefcEefckfX19e/34QICAgJ+YHArD/ApALvN/5Pc1gTv/ee894977x/P5XK3cLiAgICAAItbeYC/DOCQc26fcy4F4JMAvv7uDCsgICAg4Hq4aRu4977mnPsXAP4KQBzA5733R2+0n//w738TABBL6lD27N8HANi3b1/UtrCwAACYmiIhf2VlJdpWq1YBAPVazY6v5VixGL+vGr5lH/meTCajtng83tJWKpWa+rTbGo0GAMA5F7VVKhUaoxmb3b5xHLKtAW2L87jjCT1Hn/mVX23q41/+8r+JvtdqVe5L389ySO/Nsf1W40ALZLMdG2Ku6QfebIv5Om3yjZa+Np4DCzmPdj+7v4OcI92vEW2n47uYkU14SHI9AWB1fhEA8Mef/8OW47/8yksAgPvvvTdqu/9u+n706NtRW71O85udneMD6DHTGVoXmWxGh8HnIZ7QNVOt029kfUufABBzNPBUQuee4DVg1930lWsAgIsX6d7o7e2NtnV3dwMA/vaFl6O2Eq+Pn3j641GbA7Uhk6Jz8OILOpcsHfODP/WxqG1peoZ+V9TxHhzT8wUAvWtXou/5fJ66T6ejNrke68Wi9ru6BAAYGRsFAOzdsz/alkzSb8vlatRWKdN9ValVora1YgEAsLy8zPuXsRH2/GWzZBlIxrUtxeP0vBanr16Ntq2sUL/d3T1Rm6zP+fkFHUeB5rW2RqbjWk3XayaTBQAMDg9FbQkeU7Wnv2W8m+FWSEx47/8CwF/cSh8BAQEBATeHW3qAvxuoN/gNXlcpY2aG3u72LTkyMgIA6O+nt1PMSFgiFZfMmzySZI1EU2Zp2HNbwki0Ivm0k8pTqVTUtlGitmOUbVbSk3HY8W6UPq3EKbBj22o/3VZr2c+5Vsm+STHxTR9NG2M8hyZpWDZbId5vkMDtAXyrpqNTd2a3Rutvo23Sh+4fE6m/afdGc1O9VYqH0QS2in9IpWnupZKS7hcungUAXL6qjlfFIm3P8RIY61PJqcKXL57Q9Xf1KknKY2OTUdv4+BgAYG5+FgCwtl6ItqVZK/Xm3pDrUa2qFOr4fPT0dPE2lUZrLJlO7h6L2tbYmSBuTmB+iCTByUfuBgAsOJ374hyNbWBU+yhcIUmzUmqVbvXYuiZFCrbjTouUa65FPN687otG45U1EDeSciJJbVYCl3Mkxy8ZCTzWZp3KMesxXR8VHqf0Ifc2AHR1kVYj2g0ArK2vAWi+t1Npem6srxd5m85L7m87jiI/vxI3IIGHUPqAgICADkV4gAcEBAR0KHbchCJoGPW2zGrTpYsXo7alJSI3RG0RUwqghGZ3j5IKomYLgQAACVZRytxm1Ze0IVcEog5lMpaIot+IKtjV1WWOSe9Da/6Q/a0P/EZirh2Jac02Yt6pGjVuI0olNR/J/ladU1OEnV/0bcP/gJdDWbMKz88aH4TQ3MCH0v5CKLY9pjlWiznD7tQ67kgNbtfXpj0oGQwAlXIzGW3R20vrKJXW67i+TmtsdFDXWLlBxNzqHBFcfd26/7KnY1Xr1mxDo3r7beX6U9lzAHT99fVp/4mIdNc+5Bqsrq5GbXEmT0fHKFBvaWkx2pbvpftleEKD+Eq8/pfn56K2K0v0PbWfTCmHH7s/2jZ3hUya89PzUdvSVfo+nB/AZrBruJ0LsczFmhdbTYet94Zd12IirVTUNCOmCPm0ZtR0NtsyNtmvaUHx1zKbiKzzQpb7sG2FwhqPQ+/RSoWO2+DFa81DYqK0+4u5SCno6yNI4AEBAQEdijtHAjdShryVGkbsKs+RhCCS7+CAvvl7+ym/SsK8VeXNncioZJ0tkRSwyFLRWkEJI5EQrDQg41jjtysADAwONO1vxy1jsxKCSFbWlanFha4NeWhhyaDN0GjUzfcGz6Wdq14blz6RrJsI3DaEKUuVzWL85mNqxxNG19Rua/YAvC5qDXEVbHUtjI7dNA6W4hFvaWuHUpkk1Nk5leqSfE77s9rH7Cpdl2qDzsvioq4nWWMzsyop16t8u3ldp6USra1Mltr6+vLRthQT5GdOno7aZE3anEIiEcr1rhlCT4jY7n6VgLN50hrjDT0Hpy+SJlB++UUAwMF7D5q5U7+X3zql/c6RKx1ymxNuVvMSbSJh3IUrdTq/TWuXXU9LLPmuGnfheo221Y1WIwRhsWzaSjTeZJrOUa+Zu2jw9vytrNJcrlzVOETRmCtVusblmo6xzm6MxoMSJV4qa7pkIhfHBj9mY2aNlkp0jarm3q7Vr3+fb0SQwAMCAgI6FOEBHhAQENChuGNMKO108cYW5E3FmCRGxsg/NZtQ9Vb8s21UWjFBapGr1luOKISEJTciX02jkFvS0o4HaPZxFUh/tl/ZTz7b+Y1bbOX/LfBNPs7i097O99xEL7IZwbXpP5qzMTWIqttk/YjMGK0RpDqeduO1/uIb21opyHbnxdftP9e3v1iLko143IhMiq77HMcjAIBjs0RuSNfTGmv3PsZ+40YDLhZpfZZLas5IJ0mV7+5WlT7VRede1lo8bsxCbINo1HVdLS6S/7U19cUT9D3J6z9roj+rFfFn1j6iyF7Tx6HD5P+dGiFzzOnjJ3QybCoqnNLzcdfIMPe7+dqsmftBTBG5HvWd9jUab90QeZ7PkcvQeU50q6nUsV91D5uMAGAwRSbKZEb7lejWKBakjTlwaFAjIOu8fWJ5KWoTE06dzRr23o6iYU1EbYlNLhfOnY/api5fBgAscKRupaikp5j8zOVGLL6FPXITBAk8ICAgoEOx4xK45qww7kL8GWtyF6I3Yb3GhJQhPhbmyaXJ0ilCVli3pBznPAATEjZiTVyJLLGZ7yFCqb9XCQ8hitY58ipupMVUPMVzslGDTN4YKVskE+krbnJoeJ6zdVFykZvf5m/oeqOVDIkbCdy3+bYVayjSsGsSt1vHIYKsSOxNPUYRkJsepulY7YnTDX1BJUcrWbktjiHjrRuSyEqkGzGaJ1e+0qq6Zs7OkfQ5tWjc9zIUHSyXdmxSXfXcHInnDRNF6Tzn1jFce5o1s0i7MeNqgNq6u/UHnld5rkslTllj5QqtmWxWt7HQilJZ5xJj98hiXU/a+aNEUD4ySSn9R4d2RdtOniG3x7U1XZNzBZI4s6XNU0Tv2b83+p7K0L2Xyqr20VjjMSX1Oo7sm6DPybsAAAPGWUG0ahv9rBGNZu3wd7mHZmZno02i7cYzpo8Y9XFgbFy74FPTTlMTzdWbe26didCxcT1vhRVaK+8cpRo3J46/Y7ax+mYWrn2mbRdBAg8ICAjoUIQHeEBAQECHYsdNKJkuJi2aksAQMbG+puaMRr3Zt9kbH9a1Vd7PaFF1dsxMplRVkojKVe53dU39uyP/1KaIP3q/pYzKVuBj1RvUfyJhVTEmT0z6zxqrWVbdSkQpWJnEsWwcH7MpHSrPNRnf/HI1J7PiYzaRmPxpfhPxfvylmexxLfvr+94Qm765/3YGGku46RgbLfu5NsTpxn3swWxbrI0ve/TbKBLO+IFvoa5qSlDta2CIzCXJhJozdh94EACQTrPfcVzXkwQFO+j6WGI/cfH9BoBUroenRH1YsqzG/textJJ2uRjdG9mc+ovHIKS1rEmzTiTizxChFfY9Pm8inY+/RWlypxenAQD/8MM/Hm17+J4HAABT6ctR21qBTAbFLSJad+2eiL6Xqw3+1O39g2Sa7BtQ09PgOJkxUuwsYGMZJNrSniO5pvWymkOrTBZGvtyraubpylG/pWV9tsSY/E04XZM1Jl3bRYvK2i2bcZQqRHratNjybDt8H6XZtTEeZ06dpLGW1M/9JiwoQQIPCAgI6FRcVwJ3zn0ewCcAzHjvH+C2AQB/CmAvgPMAfsZ7v7hZH1uhi8lG62YnaWFtdJ1Iz9FbuGJJKPpeKWlbmd/CNj/KKkvPIiHbqLDIpcqrRNGIkQS2ZqQMkcC7OJl7LGvcuboSvM9y1Far0ZvZG7KixsRIPCJgzFSY0PRGQk2lSYqLu83ft97m3Ii0E32lt0sZu5ESbSJJJcWr2V5nFafJvTMS7VulYkE7N0jrmhmlnW3bB2sHRuPy0bUyezaaU9I29cFtsTZt7ZDMsvRXVSlNLt8P/9iHo7Z7H3kPAODU6eMAgJ6krr9rf/d3AIBUSiOBa3WS7F1Mjy1Em6wFS9zHWNpOdqm07dk9cWVZ11hXNrnpnNoVKlldI3Jt5to13ZHXz9IFIvxe/9bfR5sefOghAMDoqLreFTiHbvN92Ixi2UQvenEz1fnlOYI6k1MNY/aKFE7gq2WWZJqLTSTMXMApdO2zYnGBHkWrqyTdrq8p8Yw6nb9kUqVh0Zwb6yoNV3n9J9kls8ukjm1whOXKnBZvmOfiDSsreqx1Tg2c5P4zZi0MD5HWYYfm+H7dPONRK7Yjgf8RgKc2tH0GwHPe+0MAnuP/AwICAgJuI64rgXvvv+Oc27uh+WkA7+fvXwDwNwB+FbcAm2mvFhVcMEE1bP91nM+iYRLlV9le1S77Xt3mMOAUe74qbkAqsXSzbawnZwJ1ku2yEfIYKxzcYCVafh3G4/qmFZvpekHtcGIPlzwISZOvJbIHJ/TdKuWo6tXNcyX4dtJom+3tsvRFboqxNhubAn9EyjVtLAOoBGTzqbSOU2yaNi+E9Ct9tQsNaZenxfYhGkvk4mXzy4hkb663lehbjsXXr1rTwA653uMTWsf7zaNvAgDeeOMVAMDe0cFo24kT5JZn3UfrDVp/1hba00c23zLbcK30FecAobKRcmNsKI3VVCtMxUgi5eR3TS6oqyu07mJJPVcigff2aVDS9BRLvsK3eJOTh6XzSrk1o+aa4ZE2IpVWl8ESn7+ECVYR18Y5U4bs2hVyCZ6dpuCXclXnIsUvBgbUrbeHM0cOjYxGbaLJSUm6LnN/xfie686awDqWqNesOMzBXOBrUKirNnb1InEBV6e0zNr8Ms3l4mXlCQrMEwwOkutnf5/NM0gXK2n4ijRzHZVt5gQCbt4GPuq9n+bvVwGMbrVzQEBAQMC7j1smMT2JOpu+M5xzzzjnjjjnjtic2AEBAQEBt4abdSO85pzb5b2fds7tAjCz2Y7e+88B+BwAjI+Ptzzol7lQQxMxxipmzLxfJAJTohYtKZNM0LZM1pgueLslR4UoWi+T2lczJokMq1lW5a2wymtVqxKnlhVSK1HVU5hIkakl4fSYSQ67S/WoGSY9TL9J8TFX11UNrUWmDuOKWJa0m1vkRLlevhQh8ow7lJoU2OxQt2aK1neyVoH3Ta3gEQMbqsfLSWpTjKFdHzKeRgu9ukmOGGPKiTFJJoe0529jqlk62OZ66jITUjVjfpM8I6+9rNXdrywR4VevFHnbhWjb2iqRZT15ve4HDxwAABQrmsdnrdhcK9KSk0kx3RkxK8+q/cFxjVD07Mo6zW6KbdMZ1/SY4no3OanmoMIqmSqKHG05Mq4ugCl2xa1UVQDrERK/jYvoxuMAQI1NPkWTw2iF76UrV9QU8fbrFLV49TIp+CnjpnuZ3Q6HBjTmet9eitjsNxGek3ftBQC4XtqvYkxKYoZMGdNFNcm5kTLmWcHPmQJXoJ86ez7a9uqLL1Db5StR21KBnhULS2p2K6zTc+PAYUrN29VtC1yQuSSdVLOKuAkvrm3fn/BmJfCvA/gUf/8UgK/dZD8BAQEBATeJ7bgR/gmIsBxyzl0G8BsAfgvAl51znwZwAcDP3OwAJGDFykhRqbFGq+RUZmnUe32Ti9RsSTgpoGCzB/ayS9DuEXprW+J0ZZHenLYEm5AsSUMAZfl7rEH959MqyaX4rb1ksroXi+wWGFNpZHiEpJvde2gcF6eU+Jjncliloo5NKl7H2tUQYzSRtVHODysN02eTO14UwNOG4JQ2K7FLXzEbEMOtvF+xaKUdOkfd3XoN6m0EXxmTugBaMrrB428jazQRkRske6sJbBwrrpONkLPdpU1AVpldW996842oTQojwDE57jXfyNgYScjLK8YVkaupDw2ptDh34gwPl45lMxVKvpZqRftd5rWVMBJ1pjvP+zP5b8oDplnLqxR0PaWYWE93qXbw4IOUjVCk4n6TxdOzhlStGBLdlXjOm69Jq8VJMJwtulJicra/35Zlo9/MXCOpPG3yBDXWSBruN+6/fazh5o0Ceo3PaZbJ/65+JT3LCSElTQEZ1u777eOwyM8Z1kwSa3q+U3xdaut6bZevEhFrSV05vxPjRBFO7NZcKzF2UkiZIiN1YaHXVAu7HrbjhfKzm2z64LaPEhAQEBDwriNEYgYEBAR0KHY8F4qgx0RMRpXfK6rmxF1zFWdbY9K6rAr62Ocyn1dVsMqmhekrU9y/mjqE3OgyROhQP/3WFCdHV5L22zNOatmBUY3QGuoj0uTKgqqrf//aaW5T1Ur8yks8B2v+EBNEwyTKF1NPo7a52m+T8zu+rG3Tz1p/6npz9GKzGSvW9AkAnq9B0ZBCR14+0nSsKWMOKrGK+aEPfiBqm5qic28LbRy+m9R38Ym1i7KdqUOOFUtsTlS2m3tTEYQ2tUcFQlrnTIRgjgmx5VUl3MqzdP12TXCK11E1f6RZNb54Sa/76bOXaP9xXeujw318LDLHLBsSU2pz2qIQFZ7XUlHNGa5CEYQ1L2mS9ZwlOMXx+pqaYfq5HuTQsJoW5ua5fmONrtlAn5q9qqza16qm7iSb+Hw7gphh17U4E1iCOsmkfy6n99Ceu4hYvfAO+dE3zH0+2Eemlgfuuz9qu/vwPQCazXpzC3SNspz/Zc6YOsq8BtaMA0OR62r2GHNNltnw6iqZVnty+ly4+/BhAM2kf70i5j/tNz9OkauTeyYBABPjxoTC40jFdLU3eK2fnNu+CSVI4AEBAQEdih2XwHMsjaaNW6AQAZWqlZA54xoTUTZDYIbdf5yJ8lrjjINLS5qiJcqY5zgKKqVvfnh6c/ZktY+H7ybyYTSvxMuTD+4BANyzn9+mVY0ik1JcPjESte2fpOi8L35Dc0ussUSwsEBRZ0UbaccZBFMmi6LjwhJb+dFbyW2Fk8VbN0khNq1EW+XoP6l6b7WaeS6SsbKq0ss61wy7dFmrd3/3u98FoJpR1kStjo3Q3C9fuBS1zcxQ/o0REzn32GOPAQC6OHtbOq3nW3LZ5HJGumWSzrqSyrwkItNGO4rGkzWluMolnetGSL+S7wYACpxXI5fTYy4vcXk/jhY8fFjd8tJ8a73w4smo7dy5swCAvfvfG7UNsCYiUZe2QvviIrkpNmW8TAihqNJwtP5rUqKvNd42YaTLOh8jndZrJVXaV9htzhY2KTKhl82oVB5LcMbQokr2G2HXa5yJ05i5ZqkMXY+evN6H+/btBwBc2HsOAJA0z4CDk0T+W/LwnWNEKlcbqumsFjiKkx0e0imdZ8zR8eu2FByvlRUTnVmuiSspu3ma8z3UTxGh3V26JnePkbSd7NE+YgN0bRupeNM5AIABJp7zeV2TSd7vW68dw3YRJPCAgICADkV4gAcEBAR0KHbchCLJ5xcX1BQh6jgMMSFkWoLTQNokMKIwWjVRVOiUUcezOVKlJKLQ1EDAcDepMj/10Ueitvc/SQTJaI+qfX2sjTVYtVpc1P7z3aQy1Q3x9yPvPQQAOGeIif/zbVKrxf9ZTB4A0JC0m6Z+YzuTwUacO3M++n71KpE4NRPxJ4l9rNuuk/qAUlW9pOrwlSsUZTbDFbWpX1LpFxY02mz3JNUAFCLImrbyrCZeuqgmlAzXIiwVdc4vv0RmGIlk7elRcm1oaJA/NZWpEH5N5oYlWj8SyWiLGoh5YGRECwfkTXzARkh91HjCEkxcZ9RcAqmvMT/H5+iwRi/KHN73PjWXnDxL/sm7dul+E6Nkbnv11Vd5m9ZUlDkI8QsAdU6Hmk7rQGJxz3MiU4RV9yVJVr5XzRSS7OqySbwk8A1au5dMlGE3p7M1JWTR3cs+1lucx/4h9e+usq911UYM8/06PaOB3FO8drMcs9FnyMPZ80Rsrp5TEn10hAjkTLfec4cPkinrzDky19VNzQnxc68YojfTRfd+Mq8mke4U9Tc4QOvu5Klz0ba5Aq3/Y0e1baCPzCp9gzrnBSafpy7R+s8m1ZTjRskEW6vrPWdT3G4XQQIPCAgI6FDsuAQub3ArMcn3uJHmhACQ5OtVQ0IJ6WVLkwkhZ93JquwGtc6/TdeV+Pjgj5Ok9NEnD0Ztg3nOH2IIklqF3ZDYtaoaU2mx6EnKWVpSbcIxUXPgwF7d7y/f4PHS/5ZYdE6kYRUbRCLcKhfKvYf2R98P7yUJxCgkcFG+EVP4oSVosTVys25cpU6eIgno2DElWZSApOu4aHJBxFhEtZGYEtVaLCkRdekSSTIHDpI7YV+fSspCsDbnWJHCDzq2YpGuUUHINxOkKSX6rFtgZQvyTQjfWSMZ9nM0nyWGV5bpmHlOaXr+rEqt8d108t/3o09GbcO7iLjtyik5KoUChGgtFm00J0lpFy9ov5I6uavLkKkrtN56HI0xmdR51pmALFda3VjnTRrXDBOUqRRJoTGn1ycRFw1Q+5WozJTJH7IRd99/X/R9naXR5VXNK1RhN8OCIc9Vm6BjLl1SLWHuNLnk/th7HovaHjhELn1vnTkTtb1+grS7194gyfesId1rvMYbRkuOsQNDf0rX2KOH9gEA7tlLbeMZdUzI9JKUfbSgVebnVugajXfpte0doO8ZJovt86zGEcaW6N0ircymCBJ4QEBAQIciPMADAgICOhQ7bkIR80G3qTknKqyN2qowIVdm/801o4rJ/pJ8CtBqN7ZWZIJr0kkllyfvV8LoRx8h/+5EaT5qc0yW2QQ105wUqAxSNRvGR/csR3gW11TNrjRov7eOTkdtc0x6SQWapKk63j9IpEn9mvYhZgT5bIfSiqr7UvEnZuwIEdFrIxT53NQ3pJUFNL1pzZCpWUfq3kDOVLZZJ2IzlaW+MtBzJZahpKmxU2NVenVhNmpbmqPq6LNdZD6av6rV0lcLdJ2t+ahel9qcrQm8bMXyraCWkOGWbV1MBmazesw0r521ZV13Fy5IVCZ9XjyvhNsoJ2hqqh4zQ/2umgRXGCOSdniYxjE7q+elr5eu1bVrem27OS2xrYo0ydXfq2zuymX1XoqxSW5h0Zj12HyQM9WnshlS9xtshkwmdS5yHSslVffLzAz29KnJYCPyfZr2tYvXWM5EXEslnpRJTpVlknuWE3jZGqG9PXSupi7qPbq8dJTaFjXe4xzX+rzGjhHrxuwVZ3IU5pjVGpmBiiW9l/0xukbFBepr1PiqI07HH8vq2smwmdBGpsY5dmGQSfReY0rs4e9RPVMAORkb1DRzPQQJPCAgIKBDseMSuEQQLhnySwz7SRMpFmdCTHKW1I2kJZJYzNSRzDBxZVNaCjk6kCOp5OmPqovXriF6wxaLOo7eBL0lS4Ycfec8Sdn1JEkD9z2s5GGJo7cWTWm91RLN4e13lIwR1y7hB73JhyAaSd1InJK3o12RBcGxt17U/WNMsJbV78vxuzphmJIq+4VVOYLPkqlColaM71iDI1jLxj1xcZmIIsnt0Ggat7RZSbnObXr9apyzYnmec9TUdJ6idViJU/qwxKYewzWNB9B6qraauY+zBDTUKoFHuVaMG6vMq2bOUaFAZNkKS9TTV3SMYyOkSd3/8HuitjVOBXvtqrpmjo/R8WOOxpjNqDZ26BAR6h/6kOaSuXSZikYI8QtoVfc4+ziumqjcFLuulQ05KhJ6Nqv3l7hJ9rIrrE29OzdH0m3VzL2vn9wIlwt6v2yETYkcMebGfc/xtY0ZzTLJGm1OKtYb3j5bY83BFDwoMJE80K/S/gWO9pVcRlkjPc8s01zWjMtsjFPM5oxrZh10rSqcbydlpGe/Sud3l3k+dffS82B1UI+1FOdUyDU6D70Jk1K6l/pLmvS61jV6uwgSeEBAQECHYscl8HaSm0jKVsLSMmv8RjRvK5H0Uk0lkWh/W1FeEvR//EfIXe2p9/9QtK26SrbHE5fUljZX47Js5k07t87BFXO0f3e/BpiM9HNV8oRKJWXOS1F2Or8JzukwdYUkhaSxxxXYtl83UkmC3SS3ksBfe0NzrTiQhOyN5BsDZ4OzJnBsnt0wKq5gJF+wZN9Uyoyvn3xaKaKL3c6cLSIRHVPbRC6JSrCZVSlCi5WGvZciICbZf1T4QSqSGzcxHpt1O6zFNnfJlAAru/4KXOgjYbS8gUGSQiV/iL063/07qlSf79ecL5KBscsEp0xfIft2sch2cbNOVljSGx7R4JDLUySB5/MqcV6bpj4mxinrXd3YfBsxkv4mJjR4qFIVDkG1IBejua5zgFXG5AUZHBQ7vl6YFGux1YaJktmAWJv712aB7GIbfLcJBupnqdnXiZ/KjipP5XPENSyfUO1j310UQLNgClwMcWDOj3/kQwCAkXHt4+o83bfX5lULEi7o4ORk1NTLtukzR98EABya1EyCK9foPMxMqy2+izM7xsb0uvSO0PqY4OCscdPHELuU2jwt2KI4xma4rgTunL64QM8AAB7LSURBVNvtnHveOXfMOXfUOfeL3D7gnHvWOXeKP/uv11dAQEBAwLuH7ZhQagB+2Xt/H4AnAfyCc+4+AJ8B8Jz3/hCA5/j/gICAgIDbhO2UVJsGMM3fV51zxwFMAHgaVCsTAL4A4G8A/OqNDkCIFFs4QFKpNiv4EkHFORXKhsTkz9q6kiZJVrkTJsH6ww9SdNUnPsimE0NOHj9Haui5Od2/q0LmjAfu1Sis81fIPPLiG6TKXpxSNerTP/cxAEC5pmplXz+5TeVzOr81zsnSx0RN3ZttXP/Smi5E9fd+c5NH2VtVlggaZ5V6IQ/tjzjqTVRda6IRcsUWShAy0KrBkbkLYkrR/bvZ9BQzwxA3P4k4lRkC6s64ValL+i19VoypoMqEWZQo35BD0ubM2LbSVsVt1KawlbqrMdPv/n3kenr5EkXh1WqWaKWxPf/830Rtcr0fe/ShqG3fHlKvk0km6EydytMceXjkyMtR2733UnSjzUEiuV4i4tbMbWCAzC8xk8p0tUDmIJteV9IHLy6SCcVY39DLkYd1Qy5LumYbMbwRaVObU+5vS2jL+R0bG9Nj9VDbLLsATp1R8t+tM8G/ombOoQkiDYdMRHSZXT33cFGXjEmvm+P8LCPdOvcME71Dxq2yh11lBxPkpGD4R9SK9M+e/L6oLbWb1kJll5q7hvZTRPQg58WxRWsSbOZJJHQccfd9zoXinNsL4FEALwIY5Yc7QI6wo5v85hnn3BHn3JGt8lkHBAQEBNwYtk1iOue6AfwZgF/y3q9Ygsd7713EQDXDe/85AJ8DgPHx8ZZ9hMCzLmySdS9jyD2w9CT7lRJWMqTvvT36Bn30vgcAAPsnlGR8iAs07B4n163v/f1r0baX3jgPACg3DKkwTaTJpMkQd3WBJI+LV0kSvzKnL6VHHqVcIQ/co4RRtU5SyEOHD0RtU9OUec7zOawZkVMykjWMVC4J+DNGYtqImtE0YkzQxY3EGWetJmakxERckv3TeRZ3QgBwEtARN8QVSwhxZzJB8mUTycqS0YWySNuGzGKXSe9tsQnuRDJO2kyTPAcr6WmeFj1Wg7UHKS1nq6ULKWnPR2wLYUfy6FSNhJ/h6u4wgU2ZLEmLIkmurKj/qBx/fV0Dm2Tc7xzXQI377t4LAJjgrI6Li6rRSb8TE5MtbYsmcEW0mq0cApbM2MpcQMRqXPJdSFpn7z3W23KmgMFaeZXntLkq0zD9x7gPZxL0dLFEms6opF4pckCMVI9f1eteWqdxj+zT+yueovtvdFApuOIkSfS1ApHABZN3p8KabcLUSWRuHuWqeTzxGsh0k+aSNA4SfRW6Vv0j6kK8zPltqgOmxCIHZw1wNsek0UgafD96c59vVZ5uM2xLAnfOJUEP7y9677/Kzdecc7t4+y4AM5v9PiAgICDg3cd2vFAcgD8AcNx7/ztm09cBfIq/fwrA19794QUEBAQEbIbtmFDeB+DnAbzlnHud2/4tgN8C8GXn3KcBXADwMzczAFEJG20iD11THT/OgcIEkyWpRO2z+VRGh0il2jM+GLWNcF6Ko6fIF/TbL6gqe+IctRXKqsZ05Ui3euzhw1FbI05mjL5BMsdUTCrMuTlS1VwiH7W98Rrlajh86O6o7cU3iJy6cI1zQWTV9JNm32lbqVvqQma3MKF0mdwVCbYPGF4YTioRpK3/NdeP5FOfSWgfVYluTVpzBvuSG5OPnC25fq6p4gFtjbeJMLN+7mJWkWNZcrLOZh0fs77n1K8zPtlxricY1cI0zGndM8FpCzRA4gpaIWYHm1dlfZ3GlDMFQopsHhH/bmtCiYhZYxqU9fnAAw9EbcfZnNJgk9L+/XuibXL9LPl1iYsD3H+/VmYXAlLum/Pnz0fbVriWZ8xE+4pP/eqqNfkwgczX/a49Wt9zlaMdC2srLfsPDur9tRENY1WVq9fkG87mwjhM/dcKV6rP0LnatVsJzjn2OS85HXeyTNeju6c3atvPWWxPHGd/ca/XrMHXJZPT85HkVLo+pvfy0hrdm3t27wUApPt0nst1jtruUuI01k9t/YM6jhzfk2IisulkG1GaZuNWcOMWlG15oXxvi64/eOOHDAgICAh4N7DjkZjVKkcNmrbIXa5hyS/JoMauaeZNXmPpb9VUUD93jgjFLqjUsGecXHyOnSPnmWReCc5akggPZ9itbB+9ucsNk6vE01s0zZpDr5GO9u2jV/9fPaduX9MSrWXJUZZMuruZFDLSUYrJm0RC90+IVLuF71vKSOCeiUpblq2BZqIL0HPOXGazNMrinzfno8yRezZPhpRQc0xs2ihNdQ80+S+i8VgCjaX9OkvWRvOKRxK97Zf3N+dNJX9JMKNzF5fMRkzHIQRouxtAtAlbtV0IUJs5T0jMcY70W1nWtVYsErlmtQmReG108Nge0uSEOF1a0jwm3exSZ4s8RBGvRpoTDUAk/JERdXt94YUXaJ4pHffEBEUEXrhwIWpTiZruEeuCmuTfrs2aIg98/HRq84IO7aJhnV1/vtmNFdBcKPl8b8u4GyXSBM5d0QINS7xmJkxE48R+0r6XC3ROp89pQYxKgc5loqj9+gGeX0Y1rqH9RJQO7b4LALBY0uu46KiPsWF9fkzuI40lkzOR31LIIdHKmGsNlXb5fLaPkAslICAgoEMRHuABAQEBHYodN6GIOcFWlE8kWtOnxjckTWoyBbCqFo+pn2qSE8Pv3qf+1+evEFG5Kv7OJpl6kmsMNky0mWe1eXFZfXmrVRkvbfvA+38s2nZ1hoiPV97Q+nyxGPU7MqB+u1lWs8QcVKnbeVK/hTUlarKsqleqSrJsRMEUs0CD9rO+zpIy0xbJiPvm2pIx8z6Xudv8WQ0hPU1km5g7JIisUbemEfqUdLGA+g23q++Z4rVgozl9lFhK95PxNoyZJNFoJm5tIQqZVrVmUuN6Q7ZuQGTyqehcxGRWr5uIYTZb9bGv8OMP6Vp75GEiKr/9vZeitiNvnAAAfPPZv47aPvZxWj/VKhF03pCvd2XI/3t4VFX1LvbFnr16LWo7f55MIfsP0fHvuUcJ8yeeeAIAMH1NC0Xke8nEMGNqfq6skPlHIjet/3oySfOrmPTEVS7MUTNtLbAhtWJasxG10acxobCJykMSuKlZYWiQCM3pXj0fZ6aO0++mdX77K2TWvP89jwIAJk0ir5lzFNnZnVGysaePTE+pIePrvY/MTPOc9OryvJq28pNkVhk5pNdbfOStic1GLDfPWNezrVG7lU/9ZggSeEBAQECHYsclcJFCrVugEDo1k/xdXtz79lP+gT171N3qrbfJVW/BVNlu8NtsvaFvwUtcAuuhJx4HAMy/8mq0TdKFlgzpFPMkYRWMu1WxQJLJ/rtIOto1pu5Ff/7l52j/snFhqxe5X5WeJWi1i0nMhJH6G1xAIZ3Wd2s6I9GFm6eTdYYoScRo3I2GIfLE9c5oLuLhJoJS3EgAdT6WlQm8a00xW+NjCGFqoyPFpa/RprRbo03eFZG8a0ZSlnMVs/lXmLC07oyeJxO5wxnJTaR4m07WY/Nz2c9S6KKRWuu8FpuiZnm8XZzXItWjUn2JJdiPfliLMQwMEWF55sL5qE2KMQx3D3Ff6j569RpJf3lTrGB+iSTlkWF1r+Pi7rjEFdynr2j5vqEh6ndxUSXIoWEahyU75Z6rswaVsGXOJHdPr45jdpqOEd+ilHqjYtPVSkpfs92LRqfX1onrKX+mk4Yk5WIT+RGd+8JbdO+/duJU1FbopvW/e4Wipffv1WfF4eEHqd+0zqXB661e1zTQqxeIKL18mTSd1biSkwcfIMIynVd3YYkYdiZyOfLBkNTMZu7yz61K0EECDwgICOhQhAd4QEBAQIdix00oeVZDhke0NuHFi1SV3Br4ReX4yEc+AgD4xz/5k9GmZ58l08VXv/rVqK1aI5/VI29ptGVPN5k7cjlSHcfGlIQ4fIBMHXOLqjbHOS2r4VeRZFXwrglS415+8XvRtgqbERKmynyNK3nXLanmxDxBn5b4KHNlkXjCkoGk4iXim18uW+9ROEabxrXO5hHrkx1jElNSpTZRKKz2VY3pRwIZrSmnJtujhFF6soScaiJn5GsbolJMNE21AaNoPh23mE5iZsQyJknI1WiKQuUqPUZVL61vTgiLj3XKRF2Kj26touadDKd+HRklwuvaVfU3/vb3KFFaT6+q2Z94+uMAgLHTSsIJmfvww0S4WQL3DJOT09fU/HHyJEUXZh/Sfge4ruc6+4tLtCYAvPPOSRqHqRB/7/1EsNrIXplzuVziues1yPfQse6+R/vYNUa+76sFG33aDBuFqpWSDJEn66Np5UlK3Fa/cTHr2Ao+Uvv2mvGVL3Kq22t8ra4ZAnKEnzdJ43KdTLHJ0cx5uUTr4yL75XftMsdkk1naVpUSP/c2Of1k7dj4iajiVZONcnOz3mYIEnhAQEBAh2LHJfB5rqC9WtG0rBXOQSGFHQCgyulEy+wuZ+s9CknRZ6Sd4UEiGg7s2xu1HTt2DAAwM0sSfq2ib+2H7rsHALCyqtJRIk5Sc7yiUkaMy2QvMUEyPqZ17n7k/e8HALz6lroRvvP22wCaU2YmV7lKOvv5VYx0nub8EClDmkhOjvX1zaWduHkXi+QdMxJ7XI5lCUKWauNtqiY4JpjiVqJggrKds5NIwzHTR+SW6Fol5URTjUtO1C9snPG+8uy6aIlNGaftI8Yit0jgsZR14aJtlYqmJm00Ns8rI9KiLUiwxAR52hRGKBdpbLNSBMTkkhnetRcAcOLk0ajtpZdeBAAcPKRpSBeWSVoUkrFQ0GjHaoX6f+2NIzqOBdIQD+/XPobv2s3jpnlKFCMAlMut2puk5hXXQUCjPUe5BuVjj2nRiTXO49+VVcJ+mEnUi0bav3DuIiystiecstWuxAXVSqFRTh1JLWw0S9nN9iHEe6munSxwCtpVblszkdSza7Q+Esa1VUZZNY4UV7nO6TKvmffu3Rtti5ad0U7FLdVqDLKOIqUzZjVRnruxMty4/B0k8ICAgICOxY5L4CKZ9gyq+9SMBNysqYTsWQqV4IMzZ05H2/7qm98EABw7/lbUtnecXKX27zJ2uwE61txVsiPOmtwOly+Ti2E6q6dEKl53JfU9VyixnZFdtZ549NFoW65bSoiptDg2TMfPmITw/Vy9ftc+CriYmdck/lOXye5piyaMc+6Ks2dUsm+BkXYkwMUZUTbKe2J+EgnGkovESMrxWJuq9PVWG7W474mg7mxytThvM5Ky5nuwtlCCSM+lhp4/cSdrMqNLIQyruXAOjVxWgqNsJkG6zjbThK9tvvRFCoyZcUeZLo1rnOQ7iXLyGyl3aJzWTrZHNanXX6dK9StLqkntuouCQo4fI1t1qaxaQoWDZRavasDNnt1czTymcz9xjLS8Vc79kTeZ+aTq+atvavESkfJrpoCHlGWTXCizc1q1vcK25Lff1PWX5zJrpj4IkhsCUWwBA9cmF4rubqO0mt1X2/FgNghM7ewmbxLnEZpZWedPE5TEuVXS5n4UMbZuXE/jLCH3cVm2kSHl6HpZ048Z191ofk0pQF3TZ5O5m+dg+aQQyBMQEBDwA4TwAA8ICAjoUFzXhOKcywD4DoA07/8V7/1vOOf2AfgSgEEArwD4ee/9FokR2iPHdSwnd2sC+QLnHrm6qJFRMVY1/vpb5DJ46tTJaNsVjgqrGyW5xm6EUxeOR20/9Ai5T7158jztU1ISZ3aO1NS6dTUrEpHxngcPRm2ek35c4AitEyd0HLsmSQ09f0pNOY7DsQa7teazKE0VLmpgSZwamxiqxiQyw/kYmvwZN6BWt7UXWb02KWzFdFGp6iWSenwpnnOiiWxkMqkpGQp9L5v8FwkpxiDpZM2Y6mLGiBmTCJslqrXWdK9yIpJmWVaYaLbmnXQ6x/M0x+KIUM/91o15QOqA2vSmla0yd8qUzQFK7FbWl7NmAfpMsBuaXTvrNboe+X41Zxw4QCaz4rqaBo8dIzfXlPRhzAPTHFk52KOEa99BIhkry+ruWivRZBbnyTQzM6PbikxYnjxxNmob2/UmAKCnR0l/UeXfeYfytRw4qNGLOS44ctzU8ty9h1xw181aOLTnLjQhaUwMkVvg1mYCcW2VdVcz1ylyFa1aMxY7NZjrXee0s9JmTRPiyVc1qaolP5AtPCJpcnvitNYGu7V4g5CvdeOnm5DcQdZVcIOZxI7DtcnpdDPYzq/LAD7gvX8YwCMAnnLOPQngtwH8rvf+IIBFAJ++pZEEBAQEBNwQtlORxwMQti/Jfx7ABwD8E27/AoB/B+D3bnQAMa4un+1SKSPfT5LB/KwSKRJAs7hEWf2WFkx2P84pMj6huR3uvZckhKxX98QMz3ZyiAipSxc1oX2+h6t3r5lcHiyKpU1SecmWJjkPXntVXbzumqTMcpPDKtlcmSLJPmtIk4tXKOBj+iy5XdVt1kUWWmyF+KkpystgCwFsRNyUHFMS07j0RRK1zX4m7+9WwtIzQZiMtxI19TZloGLx1oro4gJoXT5F4IgZ6UWm71j6sjlZqpyDJG4IwpiTQhEw+3FOFt/q6hiPs3Rr87TENl/6qRStxUrZFIAQiSmjbm0jvZT1rsBuditlJcXjrCWsF3X95QeICOvqU+2jxrlyLp4md7ypaSUsR3kd/YP3qga4f1S0JZV8u/neiTFJf3FejznHborlqp6Rv/0e5QDKm1weZdYw7nvgEADgvUPqMnj+HI1toF+1yHSa76GrSmy2SOBWRZKl1o7EbE6Q0tTWFBjG915T7hRZw00uejxG1mpsoFA7Mj8Zk+ynJhcQk6MxdmWOG0JW+rAEpEjeVsuL8r/Iflab5XXanJPn+xTI45yLcz3MGQDPAjgDYMn7KJ/nZQATm/z2GefcEefckfX19Xa7BAQEBATcBLb1APfe1733jwCYBPAEgHu2ewDv/ee894977x+XAsYBAQEBAbeOG/ID994vOeeeB/DDAPqccwmWwicBTG396/aocp3FWkNNBuksmRu6ejT/gEQaJkRFseowkw9rJY2MWlgh1THVq6aZVa5rt3+CVMH5OU2KL+TK8rqqUTnOR5I3anOK/ZLFJLK+pqacdSaW3vdDGsV2cYxU4pgxfyy9TkRRiZm0mvE/rXN0l/P6bk3xb6s2n8oGxK0fbJtzlGBTVcKYcqpVIQ85h4up8i4V3xNoYgppm4nOjK4fpO6k7i4+4k3Rd6xWJlqS3QNV9jeuGTI6mZEaobp/nY9ZNcUjhOxybkN1egCe52BTzNas/WUDpIBGqaRmih6uO5nI2MIjiaaxOVMAopvzahQKqnVKYYukMckNcarYPvZd7jM1VrNd1G/aRJWK+ahoxlbj89DDaV+zBd02Mkxmm3zPQNR2mlOvnjl9LmrL5fhe4/TLb72p5P/cHMUpzM3oXK5x7ELBFH7YiHjDmvCayclNIYUO2mwSM4WtESrRntZdXOqj1tsVDWFTiDV/SBCnmHMBoMomvhiT1klj4pXnjTXvxMQo027gzrVsisjONrlTbgTXlcCdc8POuT7+ngXwYQDHATwP4Kd5t08B+NotjSQgICAg4IawHQl8F4AvOBJtYgC+7L3/hnPuGIAvOed+E8BrAP7gZgYgpFOlrlJDkokD+26qiEsQR9zZt5/krKibwginzjIplNL9ljkZ/lNPHAYA3LNP85jU2P1sflE1gcEBkrr6TTRdnsuwFdboWIN5lWyG+kh6yiZ1HIf2E7HznZdeidoKayS1lFmgNmkctJK8cXNybQjCjbBVyhtRdj8juUUl0lSKl4IEnt/jNndFnCVw61olPFHDuHHFk9RYYem2qXgDSx5V4wsmUZkNk6kucrMSN0lDplZ5P1uqrcHnxkrgIhVFElzMSudMcBppZ6uCDouLTJQvqRtrmjMTxuJ6npc5ws+xtN/VrdLzYB9JtPmsrp1Sic6bLbVVLHCmvDq5+3Wnde6H7yFLZbWkhUpWy/xbp+bIC9dIk+zNU1tPVgk3qQCXGdOI5Dq7ko6OaXTh2bMkjT/77PO8TXMCHThAa7hirnujRuehUto8qyNMQRaRND3sWmjNhbLRVbZdAYiqyQgp6yhufrex7KJ132sXASnaWpObH38V7dFeM9m/uUCD3F+GxJeMgxs+N2u7mUjM7XihvAng0TbtZ0H28ICAgICAHUCIxAwICAjoUOx4MisxGVTLNs0pvVesD3JVsuaIr3DCmgzo0yb4L7E5YG1dkwMV18jve6Kf1NoH71YTysQA+bXGqppYatcgEx4mDeloL6nGMTZP3H2Xqpp59kefnVPVu8ZRiKfOKscrVeglfWraJNb3rAquGZdLiabbync5Fle1ucYnJJ7Qfut1SSuq+0XkivikGpJPyMhERpOMpThlZqOo/s4pJvB8jdP9mkhPGa+tbVpm4rZdsv9EUqI0jbmEyVzTFCXpiplIvwakJqfYpXR/OZb1A8cWVennOJGTNVmJil5rGH9gXgPSbdWow0L8lUtqdoCn/fN5jerr6iJf7ESO526CmatMeqZTai65tsAmi4T2u+ZoPTsm6ccHdf8qX9srpthEVzeti8EhNatk2HHgzBkypcRjen68p23d3bqeCrwm3RZmvaKJZYhSqhpTR4MvarVu4wTYdBdrfQbI9bBFJMpsTmlysZa6qG3HxuRkLN7S1lRzNkrJLCYrs0n6bTJ5tEZbbuiquZhFRGy61h1vAEECDwgICOhQuK0qnb/bGB8f988888xtO15AQEDA/w/47Gc/+4r3/vGN7UECDwgICOhQhAd4QEBAQIciPMADAgICOhThAR4QEBDQobitJKZzbhbAGoC56+17h2MInT2HTh8/0Plz6PTxA50/h04a/x7v/fDGxtv6AAcA59yRdmxqJ6HT59Dp4wc6fw6dPn6g8+fQ6eMHggklICAgoGMRHuABAQEBHYqdeIB/bgeO+W6j0+fQ6eMHOn8OnT5+oPPn0Onjv/028ICAgICAdwfBhBIQEBDQobitD3Dn3FPOuRPOudPOuc/czmPfDJxzu51zzzvnjjnnjjrnfpHbB5xzzzrnTvFn//X62klwUerXnHPf4P/3Oede5Ovwp8651PX62Ek45/qcc19xzr3jnDvunPvhDrwG/5rX0NvOuT9xzmXu5OvgnPu8c27GOfe2aWt7zh3hv/I83nTOPbZzI1dsMof/yOvoTefcn0u1Md72azyHE865j+7MqG8Mt+0BzhV9/huAjwG4D8DPOufuu13Hv0nUAPyy9/4+AE8C+AUe82cAPOe9PwTgOf7/TsYvgsrgCX4bwO967w8CWATw6R0Z1fbxXwD8pff+HgAPg+bSMdfAOTcB4F8BeNx7/wCAOIBP4s6+Dn8E4KkNbZud848BOMR/zwD4vds0xuvhj9A6h2cBPOC9fwjASQC/BgB8X38SwP38m//unIvjDsftlMCfAHDae3/We18B8CUAT9/G498wvPfT3vtX+fsq6MExARr3F3i3LwD4yZ0Z4fXhnJsE8I8A/D7/7wB8AMBXeJc7ffy9AH4MXLLPe1/x3i+hg64BIwEg65xLAMgBmMYdfB28998BsLChebNz/jSAP/aEF0AFz3fdnpFujnZz8N5/02tdwRdABdkBmsOXvPdl7/05AKfRARXHbucDfALAJfP/ZW7rCDjn9oJKy70IYNR7P82brgIY3aFhbQf/GcCvAJDs9oMAlswivtOvwz4AswD+kM1Av++c60IHXQPv/RSA/wTgIujBvQzgFXTWdQA2P+edem//cwD/l7935BwCibkNOOe6AfwZgF/y3q/YbZ7ceO5IVx7n3CcAzHjvX7nuzncuEgAeA/B73vtHQakYmswld/I1AAC2FT8NehmNA+hCq2rfUbjTz/n14Jz7dZCJ9Is7PZZbwe18gE8B2G3+n+S2OxrOuSTo4f1F7/1XufmaqIj8ObNT47sO3gfgJ5xz50Emqw+A7Ml9rMoDd/51uAzgsvf+Rf7/K6AHeqdcAwD4EIBz3vtZ730VwFdB16aTrgOw+TnvqHvbOffPAHwCwM959aPuqDkIbucD/GUAh5h5T4EIg6/fxuPfMNhe/AcAjnvvf8ds+jqAT/H3TwH42u0e23bgvf817/2k934v6Hx/y3v/cwCeB/DTvNsdO34A8N5fBXDJOXc3N30QwDF0yDVgXATwpHMux2tK5tAx14Gx2Tn/OoB/yt4oTwJYNqaWOwrOuadAJsWf8N6vm01fB/BJ51zaObcPRMi+tBNjvCF472/bH4CPg5jfMwB+/XYe+ybH+6MgNfFNAK/z38dBduTnAJwC8NcABnZ6rNuYy/sBfIO/7wctztMA/heA9E6P7zpjfwTAEb4O/xtAf6ddAwCfBfAOgLcB/E8A6Tv5OgD4E5C9vgrSgj692TkHlfz9b3xfvwXytrlT53AaZOuW+/l/mP1/nedwAsDHdnr82/kLkZgBAQEBHYpAYgYEBAR0KMIDPCAgIKBDER7gAQEBAR2K8AAPCAgI6FCEB3hAQEBAhyI8wAMCAgI6FOEBHhAQENChCA/wgICAgA7F/wP2GC15aiBW1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[0:4]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the provided CNN architecture I will design a couple of my own architectures to get the hang of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Simple Net\n",
    "Just two convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=128 * 14 * 14, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # 1\n",
    "        output = self.conv1(inputs)\n",
    "        output = self.relu1(output)\n",
    "        # 2\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        # 3 pool\n",
    "        output = self.pool(output)\n",
    "        # 4 view\n",
    "        output = output.view(-1, 128 * 14 * 14)\n",
    "        # 5\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 30, 30]           1,792\n",
      "              ReLU-2           [-1, 64, 30, 30]               0\n",
      "            Conv2d-3          [-1, 128, 28, 28]          73,856\n",
      "              ReLU-4          [-1, 128, 28, 28]               0\n",
      "         MaxPool2d-5          [-1, 128, 14, 14]               0\n",
      "            Linear-6                   [-1, 64]       1,605,696\n",
      "            Linear-7                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 1,681,994\n",
      "Trainable params: 1,681,994\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.60\n",
      "Params size (MB): 6.42\n",
      "Estimated Total Size (MB): 9.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "simple_net = SimpleNet().to(device)\n",
    "summary(simple_net, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Deeper Net\n",
    "No fancy stuff though (Dropout, BatchNorm, etc, etc), just Deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeeperNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256 * 7 * 7, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=10)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # 1\n",
    "        output = self.conv1(inputs)\n",
    "        output = self.relu1(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        # 2 pool\n",
    "        output = self.pool1(output)\n",
    "        # 3 \n",
    "        output = self.conv3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.conv4(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.conv5(output)\n",
    "        output = self.relu5(output)\n",
    "        # 4 pool\n",
    "        output = self.pool2(output)\n",
    "        # 4 view\n",
    "        output = output.view(-1, 256 * 7 * 7)\n",
    "        # 5\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = F.relu(self.fc3(output))\n",
    "        output = self.fc4(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 30, 30]           1,792\n",
      "              ReLU-2           [-1, 64, 30, 30]               0\n",
      "            Conv2d-3          [-1, 128, 28, 28]          73,856\n",
      "              ReLU-4          [-1, 128, 28, 28]               0\n",
      "         MaxPool2d-5          [-1, 128, 14, 14]               0\n",
      "            Conv2d-6           [-1, 64, 14, 14]           8,256\n",
      "              ReLU-7           [-1, 64, 14, 14]               0\n",
      "            Conv2d-8          [-1, 128, 14, 14]           8,320\n",
      "              ReLU-9          [-1, 128, 14, 14]               0\n",
      "           Conv2d-10          [-1, 256, 14, 14]          33,024\n",
      "             ReLU-11          [-1, 256, 14, 14]               0\n",
      "        MaxPool2d-12            [-1, 256, 7, 7]               0\n",
      "           Linear-13                  [-1, 128]       1,605,760\n",
      "           Linear-14                  [-1, 128]          16,512\n",
      "           Linear-15                   [-1, 64]           8,256\n",
      "           Linear-16                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 1,756,426\n",
      "Trainable params: 1,756,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.04\n",
      "Params size (MB): 6.70\n",
      "Estimated Total Size (MB): 10.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "deeper_net = DeeperNet().to(device)\n",
    "summary(deeper_net, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Fancy Net\n",
    "Nothing really fancy about it, but just implementing batch normalization and dropout. Also trying to experiment I will try to make the network more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a unit of the architecture containing a single convolution, batch normalization and activator\n",
    "class Unit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(Unit, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.conv(inputs)\n",
    "        output = self.bn(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(FancyNet, self).__init__()\n",
    "        self.unit1 = Unit(in_channels=3, out_channels=32, kernel_size=1)\n",
    "        self.unit2 = Unit(in_channels=32, out_channels=32, kernel_size=1)\n",
    "        self.unit3 = Unit(in_channels=32, out_channels=32, kernel_size=1)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.unit4 = Unit(in_channels=32, out_channels=64)\n",
    "        self.unit5 = Unit(in_channels=64, out_channels=64)\n",
    "        self.unit6 = Unit(in_channels=64, out_channels=64)\n",
    "        self.unit6 = Unit(in_channels=64, out_channels=64)\n",
    "        \n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.net = nn.Sequential(self.unit1, self.unit2, self.unit3, self.pool1, \\\n",
    "                                self.unit4, self.unit5, self.unit6, self.pool2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64*5*5, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = self.net(inputs)\n",
    "        output = output.view(-1, 64*5*5)\n",
    "        output = self.fc1(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             128\n",
      "            Conv2d-2           [-1, 32, 32, 32]             128\n",
      "       BatchNorm2d-3           [-1, 32, 32, 32]              64\n",
      "       BatchNorm2d-4           [-1, 32, 32, 32]              64\n",
      "              ReLU-5           [-1, 32, 32, 32]               0\n",
      "              ReLU-6           [-1, 32, 32, 32]               0\n",
      "              Unit-7           [-1, 32, 32, 32]               0\n",
      "              Unit-8           [-1, 32, 32, 32]               0\n",
      "            Conv2d-9           [-1, 32, 32, 32]           1,056\n",
      "           Conv2d-10           [-1, 32, 32, 32]           1,056\n",
      "      BatchNorm2d-11           [-1, 32, 32, 32]              64\n",
      "      BatchNorm2d-12           [-1, 32, 32, 32]              64\n",
      "             ReLU-13           [-1, 32, 32, 32]               0\n",
      "             ReLU-14           [-1, 32, 32, 32]               0\n",
      "             Unit-15           [-1, 32, 32, 32]               0\n",
      "             Unit-16           [-1, 32, 32, 32]               0\n",
      "           Conv2d-17           [-1, 32, 32, 32]           1,056\n",
      "           Conv2d-18           [-1, 32, 32, 32]           1,056\n",
      "      BatchNorm2d-19           [-1, 32, 32, 32]              64\n",
      "      BatchNorm2d-20           [-1, 32, 32, 32]              64\n",
      "             ReLU-21           [-1, 32, 32, 32]               0\n",
      "             ReLU-22           [-1, 32, 32, 32]               0\n",
      "             Unit-23           [-1, 32, 32, 32]               0\n",
      "             Unit-24           [-1, 32, 32, 32]               0\n",
      "        MaxPool2d-25           [-1, 32, 16, 16]               0\n",
      "        MaxPool2d-26           [-1, 32, 16, 16]               0\n",
      "           Conv2d-27           [-1, 64, 14, 14]          18,496\n",
      "           Conv2d-28           [-1, 64, 14, 14]          18,496\n",
      "      BatchNorm2d-29           [-1, 64, 14, 14]             128\n",
      "      BatchNorm2d-30           [-1, 64, 14, 14]             128\n",
      "             ReLU-31           [-1, 64, 14, 14]               0\n",
      "             ReLU-32           [-1, 64, 14, 14]               0\n",
      "             Unit-33           [-1, 64, 14, 14]               0\n",
      "             Unit-34           [-1, 64, 14, 14]               0\n",
      "           Conv2d-35           [-1, 64, 12, 12]          36,928\n",
      "           Conv2d-36           [-1, 64, 12, 12]          36,928\n",
      "      BatchNorm2d-37           [-1, 64, 12, 12]             128\n",
      "      BatchNorm2d-38           [-1, 64, 12, 12]             128\n",
      "             ReLU-39           [-1, 64, 12, 12]               0\n",
      "             ReLU-40           [-1, 64, 12, 12]               0\n",
      "             Unit-41           [-1, 64, 12, 12]               0\n",
      "             Unit-42           [-1, 64, 12, 12]               0\n",
      "           Conv2d-43           [-1, 64, 10, 10]          36,928\n",
      "           Conv2d-44           [-1, 64, 10, 10]          36,928\n",
      "      BatchNorm2d-45           [-1, 64, 10, 10]             128\n",
      "      BatchNorm2d-46           [-1, 64, 10, 10]             128\n",
      "             ReLU-47           [-1, 64, 10, 10]               0\n",
      "             ReLU-48           [-1, 64, 10, 10]               0\n",
      "             Unit-49           [-1, 64, 10, 10]               0\n",
      "             Unit-50           [-1, 64, 10, 10]               0\n",
      "        MaxPool2d-51             [-1, 64, 5, 5]               0\n",
      "        MaxPool2d-52             [-1, 64, 5, 5]               0\n",
      "           Linear-53                   [-1, 10]          16,010\n",
      "================================================================\n",
      "Total params: 206,346\n",
      "Trainable params: 206,346\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 7.87\n",
      "Params size (MB): 0.79\n",
      "Estimated Total Size (MB): 8.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fancy_net = FancyNet().to(device)\n",
    "summary(fancy_net, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --Dense Net\n",
    "Implementing the [ResNet-101](http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html) in pytorch. Not going to lie - huge help from [Sebastian Rashka's repo](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def conv3x3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defining a Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def define_optimizer(optimizer, lr, momentum, network):\n",
    "    return optimizer(network.parameters(), lr, momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stick with Stochastic Gradient Descent for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple net\n",
    "sn_optimizer = define_optimizer(optim.SGD, 0.01, 0.9, simple_net)\n",
    "# deeper net\n",
    "dn_optimizer = define_optimizer(optim.SGD, 0.01, 0.9, deeper_net)\n",
    "# fancy net\n",
    "fn_optimizer = define_optimizer(optim.SGD, 0.01, 0.9, fancy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_network(network, optimizer, criterion, epochs, loader):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for i, data in enumerate(loader):\n",
    "            # inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # predict the parameters\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # propagate the loss backwards\n",
    "            loss.backward()\n",
    "            \n",
    "            # adjust the parameters according to the gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print stats\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            if i == (len(loader) - 1):\n",
    "                print(f'[epoch:{epoch + 1}] - loss: {running_loss / len(loader)}; accuracy - {100 * correct / total}')\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.381451873693875; accuracy - 50.182\n",
      "[epoch:2] - loss: 0.9437513516754655; accuracy - 66.974\n",
      "[epoch:3] - loss: 0.7400993279211809; accuracy - 74.118\n",
      "[epoch:4] - loss: 0.5411859389468407; accuracy - 81.1\n",
      "[epoch:5] - loss: 0.3746554716010545; accuracy - 86.808\n",
      "[epoch:6] - loss: 0.23952458614408398; accuracy - 91.722\n",
      "[epoch:7] - loss: 0.1620904068609727; accuracy - 94.36\n",
      "[epoch:8] - loss: 0.13455130357679007; accuracy - 95.408\n",
      "[epoch:9] - loss: 0.1021667761598271; accuracy - 96.546\n",
      "[epoch:10] - loss: 0.08533319208821043; accuracy - 97.148\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# simple net fit\n",
    "fit_network(simple_net, sn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 2.1063875619677193; accuracy - 19.722\n",
      "[epoch:2] - loss: 1.4415092452626463; accuracy - 47.068\n",
      "[epoch:3] - loss: 1.1178361290704724; accuracy - 60.002\n",
      "[epoch:4] - loss: 0.9122733922433335; accuracy - 67.798\n",
      "[epoch:5] - loss: 0.7547979567459738; accuracy - 73.466\n",
      "[epoch:6] - loss: 0.6357197202479923; accuracy - 77.688\n",
      "[epoch:7] - loss: 0.5238276930741599; accuracy - 81.742\n",
      "[epoch:8] - loss: 0.4311988955781922; accuracy - 84.874\n",
      "[epoch:9] - loss: 0.35518188063372275; accuracy - 87.578\n",
      "[epoch:10] - loss: 0.28665935686209687; accuracy - 89.948\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# deeper net fit\n",
    "fit_network(deeper_net, dn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.3027506271998088; accuracy - 53.272\n",
      "[epoch:2] - loss: 1.0652381318239394; accuracy - 62.518\n",
      "[epoch:3] - loss: 0.9319708014404019; accuracy - 67.41\n",
      "[epoch:4] - loss: 0.8429714564856092; accuracy - 70.408\n",
      "[epoch:5] - loss: 0.7766394508014638; accuracy - 72.914\n",
      "[epoch:6] - loss: 0.7224202010201402; accuracy - 74.706\n",
      "[epoch:7] - loss: 0.6770998550887605; accuracy - 76.286\n",
      "[epoch:8] - loss: 0.63387260630355; accuracy - 77.884\n",
      "[epoch:9] - loss: 0.5991088078617668; accuracy - 79.214\n",
      "[epoch:10] - loss: 0.5614088105224907; accuracy - 80.29\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# fancy net fit\n",
    "fit_network(fancy_net, fn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the results.\n",
    "As defined in the tutorial there are two functions we are generally interested in - the ConvNet accuracy and the class accuracy. Let's use the code from the tutorial and convert it to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_accuracy(nnet, name=\"default\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = nnet(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the {name} network on the 10000 test {100 * correct / total}')\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_class_accuracy(nnet, name='default'):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = nnet(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    print(f'Accuracy of classes of the {name} network:')\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... so how accurates are the models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the simple network on the 10000 test 66.33\n",
      "Accuracy of the deeper network on the 10000 test 72.15\n",
      "Accuracy of the fancy network on the 10000 test 71.25\n"
     ]
    }
   ],
   "source": [
    "sn_accuracy = net_accuracy(simple_net, 'simple')\n",
    "dn_accuracy = net_accuracy(deeper_net, 'deeper')\n",
    "fn_accuracy = net_accuracy(fancy_net, 'fancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classes of the simple network:\n",
      "Accuracy of plane : 71 %\n",
      "Accuracy of   car : 91 %\n",
      "Accuracy of  bird : 55 %\n",
      "Accuracy of   cat : 48 %\n",
      "Accuracy of  deer : 53 %\n",
      "Accuracy of   dog : 49 %\n",
      "Accuracy of  frog : 70 %\n",
      "Accuracy of horse : 75 %\n",
      "Accuracy of  ship : 77 %\n",
      "Accuracy of truck : 58 %\n",
      "Accuracy of classes of the deeper network:\n",
      "Accuracy of plane : 79 %\n",
      "Accuracy of   car : 89 %\n",
      "Accuracy of  bird : 64 %\n",
      "Accuracy of   cat : 52 %\n",
      "Accuracy of  deer : 47 %\n",
      "Accuracy of   dog : 76 %\n",
      "Accuracy of  frog : 76 %\n",
      "Accuracy of horse : 75 %\n",
      "Accuracy of  ship : 71 %\n",
      "Accuracy of truck : 87 %\n",
      "Accuracy of classes of the fancy network:\n",
      "Accuracy of plane : 84 %\n",
      "Accuracy of   car : 86 %\n",
      "Accuracy of  bird : 58 %\n",
      "Accuracy of   cat : 46 %\n",
      "Accuracy of  deer : 62 %\n",
      "Accuracy of   dog : 60 %\n",
      "Accuracy of  frog : 66 %\n",
      "Accuracy of horse : 76 %\n",
      "Accuracy of  ship : 78 %\n",
      "Accuracy of truck : 85 %\n"
     ]
    }
   ],
   "source": [
    "net_class_accuracy(simple_net, 'simple')\n",
    "net_class_accuracy(deeper_net, 'deeper')\n",
    "net_class_accuracy(fancy_net, 'fancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net_class_accuracy(simple_net, 'simple')\n",
    "net_class_accuracy(deeper_net, 'deeper')\n",
    "net_class_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving the results.\n",
    "Let's create a plan for what we want to explore and how it changes the results:\n",
    "1. Changing the optimizer to Adam.\n",
    "2. Adjusting Hyperparameters (lr).\n",
    "3. Dynamic LR adjustment.\n",
    "4. Transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Changing the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a new network, anet for short\n",
    "# Create the optimizer with the same learning rate\n",
    "a_sn_optimizer = optim.Adam(simple_net.parameters(), lr=0.01)\n",
    "a_dn_optimizer = optim.Adam(deeper_net.parameters(), lr=0.01)\n",
    "a_fn_optimizer = optim.Adam(fancy_net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 2.325702106197599; accuracy - 9.978\n",
      "[epoch:2] - loss: 2.304057924852719; accuracy - 9.84\n",
      "[epoch:3] - loss: 2.3040443671794555; accuracy - 9.782\n",
      "[epoch:4] - loss: 2.3040482132997715; accuracy - 9.986\n",
      "[epoch:5] - loss: 2.3041873225323757; accuracy - 9.778\n",
      "[epoch:6] - loss: 2.3041448672459968; accuracy - 9.638\n",
      "[epoch:7] - loss: 2.3040147103793487; accuracy - 9.872\n",
      "[epoch:8] - loss: 2.3040915031274465; accuracy - 9.896\n",
      "[epoch:9] - loss: 2.303845198583084; accuracy - 10.054\n",
      "[epoch:10] - loss: 2.3040436087551592; accuracy - 9.966\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# simple net fit\n",
    "fit_network(simple_net, a_sn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.922760345504136; accuracy - 26.386\n",
      "[epoch:2] - loss: 1.6740413017175324; accuracy - 36.836\n",
      "[epoch:3] - loss: 1.6540988255065752; accuracy - 38.6\n",
      "[epoch:4] - loss: 1.6078812356797534; accuracy - 40.85\n",
      "[epoch:5] - loss: 1.60830838636031; accuracy - 41.346\n",
      "[epoch:6] - loss: 1.6758789142888093; accuracy - 39.048\n",
      "[epoch:7] - loss: 1.6301930939167337; accuracy - 40.702\n",
      "[epoch:8] - loss: 1.621927967074584; accuracy - 40.614\n",
      "[epoch:9] - loss: 1.647375161649322; accuracy - 39.8\n",
      "[epoch:10] - loss: 1.6254153376729994; accuracy - 40.584\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# deeper net fit\n",
    "fit_network(deeper_net, a_dn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.6216792235066322; accuracy - 42.122\n",
      "[epoch:2] - loss: 1.2886818621834348; accuracy - 54.108\n",
      "[epoch:3] - loss: 1.1567225565684582; accuracy - 59.102\n",
      "[epoch:4] - loss: 1.078126943633866; accuracy - 62.186\n",
      "[epoch:5] - loss: 1.0196908874459856; accuracy - 64.302\n",
      "[epoch:6] - loss: 0.982000963339345; accuracy - 65.54\n",
      "[epoch:7] - loss: 0.944906348077746; accuracy - 67.072\n",
      "[epoch:8] - loss: 0.9182418670436165; accuracy - 68.06\n",
      "[epoch:9] - loss: 0.8909664741900206; accuracy - 68.964\n",
      "[epoch:10] - loss: 0.8656358273076614; accuracy - 69.746\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "fit_network(fancy_net, a_fn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the simple network on the 10000 test 10.0\n",
      "Accuracy of the deeper network on the 10000 test 38.85\n",
      "Accuracy of the fancy network on the 10000 test 66.86\n"
     ]
    }
   ],
   "source": [
    "sn_accuracy = net_accuracy(simple_net, 'simple')\n",
    "dn_accuracy = net_accuracy(deeper_net, 'deeper')\n",
    "fn_accuracy = net_accuracy(fancy_net, 'fancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the bad results, I think the learning rate is at fault as it is too high and the better results shown by the fancy and the deeper net are likely due to the random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Adjusting the hyper-parameters.\n",
    "Let's try to adjust the learning rate to a more reasonable 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_sn_optimizer = optim.Adam(simple_net.parameters(), lr=0.001)\n",
    "a2_dn_optimizer = optim.Adam(deeper_net.parameters(), lr=0.001)\n",
    "a2_fn_optimizer = optim.Adam(fancy_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 2.3034452304425184; accuracy - 11.59\n",
      "[epoch:2] - loss: 2.303434254950769; accuracy - 11.59\n",
      "[epoch:3] - loss: 2.303444357156296; accuracy - 11.59\n",
      "[epoch:4] - loss: 2.303450617360062; accuracy - 11.59\n",
      "[epoch:5] - loss: 2.3034469181348785; accuracy - 11.59\n",
      "[epoch:6] - loss: 2.3034496301271483; accuracy - 11.59\n",
      "[epoch:7] - loss: 2.3034475857984233; accuracy - 11.59\n",
      "[epoch:8] - loss: 2.30344886544875; accuracy - 11.59\n",
      "[epoch:9] - loss: 2.3034496559062525; accuracy - 11.59\n",
      "[epoch:10] - loss: 2.3034460994576462; accuracy - 11.59\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# simple net fit\n",
    "fit_network(simple_net, a2_sn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 2.306331143498192; accuracy - 10.0\n",
      "[epoch:2] - loss: 2.3063293383507415; accuracy - 10.0\n",
      "[epoch:3] - loss: 2.306323309701296; accuracy - 10.0\n",
      "[epoch:4] - loss: 2.306330741100104; accuracy - 10.0\n",
      "[epoch:5] - loss: 2.306319151028409; accuracy - 10.0\n",
      "[epoch:6] - loss: 2.306323222448943; accuracy - 10.0\n",
      "[epoch:7] - loss: 2.3063218009372743; accuracy - 10.0\n",
      "[epoch:8] - loss: 2.306328225730706; accuracy - 10.0\n",
      "[epoch:9] - loss: 2.3063202915630963; accuracy - 10.0\n",
      "[epoch:10] - loss: 2.3063245995717407; accuracy - 10.0\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# deeper net fit\n",
    "fit_network(deeper_net, a2_dn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 2.4121558527418685; accuracy - 10.354\n",
      "[epoch:2] - loss: 2.4102963426520407; accuracy - 10.466\n",
      "[epoch:3] - loss: 2.4113037560280515; accuracy - 10.32\n",
      "[epoch:4] - loss: 2.411476687445369; accuracy - 10.556\n",
      "[epoch:5] - loss: 2.413053119480038; accuracy - 10.482\n",
      "[epoch:6] - loss: 2.411336243038214; accuracy - 10.396\n",
      "[epoch:7] - loss: 2.4120331027152364; accuracy - 10.378\n",
      "[epoch:8] - loss: 2.4126745871985027; accuracy - 10.376\n",
      "[epoch:9] - loss: 2.4120550340211935; accuracy - 10.548\n",
      "[epoch:10] - loss: 2.412192959245473; accuracy - 10.522\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "fit_network(fancy_net, a2_fn_optimizer, criterion, epochs=10, loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the simple network on the 10000 test 11.16\n",
      "Accuracy of the deeper network on the 10000 test 10.0\n",
      "Accuracy of the fancy network on the 10000 test 10.89\n"
     ]
    }
   ],
   "source": [
    "sn_accuracy = net_accuracy(simple_net, 'simple')\n",
    "dn_accuracy = net_accuracy(deeper_net, 'deeper')\n",
    "fn_accuracy = net_accuracy(fancy_net, 'fancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's just horrible - it seems that the random init sent the gradients into a local minimum, from which Adam has a hard time leaving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Dynamic LR Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Using a pre-trained model.\n",
    "A way of increasing the accuracy of a CNN is to use the architecture and weights of a pre-trained model for our specific use case. I would have liked to test at least three of them, but let's start with VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/fury/.cache/torch/checkpoints/vgg16-397923af.pth\n",
      "100%|| 528M/528M [01:15<00:00, 7.37MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers\n",
    "for parameter in model_vgg16.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "# remove the last layer\n",
    "num_features = model_vgg16.classifier[6].in_features\n",
    "features = list(model_vgg16.classifier.children())[:-1]\n",
    "# out features are 10 by design\n",
    "features.extend([nn.Linear(num_features, 10)])\n",
    "# replace the first layer as VGG accepts 224x224 images\n",
    "features[0] = nn.Linear(512, 4096)\n",
    "model_vgg16.classifier = nn.Sequential(*features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the loss and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimizer and the criterion itself\n",
    "criterion_vgg16 = nn.CrossEntropyLoss()\n",
    "optimizer_vgg16 = optim.Adam(model_vgg16.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the classifier trainable\n",
    "for name, child in model_vgg16.named_children():\n",
    "    if name == 'classifier':\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-49696f2ea37e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_vgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# send to the gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_vgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Separator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'####-----####'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_net' is not defined"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "model_vgg16.cuda() # send to the gpu\n",
    "model_vgg16 = train_net(model_vgg16, optimizer_vgg16, criterion_vgg16, epochs=25)\n",
    "# Separator\n",
    "print('####-----####')\n",
    "# Evaluate\n",
    "net_accuracy(model_vgg16)\n",
    "net_class_accuracy(model_vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64% accuracy, a little less than my model. And even trained for longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion. For now.\n",
    "It seems that the maximum accuracy I got from CIFAR10 is 67%. What surprised me is that VGG16 didn't do much better than the custom model I came up with. In any case it is obvious that all models examined tend to overfit at some point. Some further steps that could be done are to:\n",
    "1. Implement dropout and regularization in my model.\n",
    "2. Implement batch normalization in both - my model and VGG16.\n",
    "3. Augment the input data by random transforms. \n",
    "4. Try out other pre-trained models.\n",
    "\n",
    "In any case I intend to revisit the problem and try out these steps, at least, in the near future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 Pytorch",
   "language": "python",
   "name": "torchpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 Example\n",
    "Since my experience until now has been mostly with Tensorflow/Keras, this notebook is intended to play around with PyTorch and run through their tutorial for training on the CIFAR10 dataset. In addition - it will be all done on the gpu.\n",
    "\n",
    "### What is CIFAR 10?\n",
    "Taken straight from the [website](https://www.cs.toronto.edu/~kriz/cifar.html):\n",
    "\"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\"\n",
    "\n",
    "### What is the goal?\n",
    "As an introduction to PyTorch - use the tutorial, get its results and improve them.\n",
    "\n",
    "### Project Structure:\n",
    "\n",
    "1. Loading the data & Loading the libraries.\n",
    "2. Showing a sample of images.\n",
    "3. Defining and training a convolutional neural network.\n",
    "4. Evaluating the results.\n",
    "5. Improving the results.\n",
    "\n",
    "...\n",
    "\n",
    "### References\n",
    "* [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky\n",
    "* [Training a classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data & Loading the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to the GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will try to mimic the tutorial from Torch's website then implement it my way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./datasets', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./datasets', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Showing a sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dog  ship   cat horse\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# maybe define an object as input - architecture.\n",
    "class Net(nn.Module):\n",
    "    # nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "    # nn.Linear(in_features, out_features, bias=True)\n",
    "    # nn.MaxPool2d(kernel_size, stride=None)\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defining a Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(nnet, optimizer, criterion, epochs=2):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = nnet(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i == (len(trainloader) - 1):\n",
    "                print(f'[epoch:{epoch + 1}] - loss: {running_loss / len(trainloader)}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "    \n",
    "    return nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.6870145456838608\n",
      "[epoch:2] - loss: 1.3141306379413604\n",
      "[epoch:3] - loss: 1.190185233669281\n",
      "[epoch:4] - loss: 1.111506900010109\n",
      "[epoch:5] - loss: 1.0453986904978751\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = train_net(net, optimizer, criterion, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the results.\n",
    "As defined in the tutorial there are two functions we are generally interested in - the ConvNet accuracy and the class accuracy. Let's use the code from the tutorial and convert it to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_accuracy(nnet):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = nnet(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_class_accuracy(nnet):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = nnet(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... so how accurates are the models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 60 %\n"
     ]
    }
   ],
   "source": [
    "net_accuracy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 62 %\n",
      "Accuracy of   car : 78 %\n",
      "Accuracy of  bird : 46 %\n",
      "Accuracy of   cat : 46 %\n",
      "Accuracy of  deer : 50 %\n",
      "Accuracy of   dog : 51 %\n",
      "Accuracy of  frog : 60 %\n",
      "Accuracy of horse : 66 %\n",
      "Accuracy of  ship : 74 %\n",
      "Accuracy of truck : 71 %\n"
     ]
    }
   ],
   "source": [
    "net_class_accuracy(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than the tutorial - 63%, but still far from enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving the results.\n",
    "Let's create a plan for what we want to explore and how it changes the results:\n",
    "1. Changing the optimizer to Adam.\n",
    "2. Training for longer (f.e. insted of 5 epochs - 10 epochs).\n",
    "3. A different (deeper) architecture with more filters.\n",
    "4. Using a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Changing the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a new network, anet for short\n",
    "anet = Net().to(device)\n",
    "# Create the optimizer with the same learning rate\n",
    "adam_optimizer = optim.Adam(anet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.4879309157180787\n",
      "[epoch:2] - loss: 1.2393201390063762\n",
      "[epoch:3] - loss: 1.1426151813077927\n",
      "[epoch:4] - loss: 1.0810983619809151\n",
      "[epoch:5] - loss: 1.041029339210391\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "anet = train_net(anet, adam_optimizer, criterion, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 60 %\n",
      "Accuracy of plane : 64 %\n",
      "Accuracy of   car : 64 %\n",
      "Accuracy of  bird : 56 %\n",
      "Accuracy of   cat : 35 %\n",
      "Accuracy of  deer : 48 %\n",
      "Accuracy of   dog : 53 %\n",
      "Accuracy of  frog : 63 %\n",
      "Accuracy of horse : 65 %\n",
      "Accuracy of  ship : 82 %\n",
      "Accuracy of truck : 71 %\n"
     ]
    }
   ],
   "source": [
    "net_accuracy(anet)\n",
    "net_class_accuracy(anet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually slightly worse results than the default setup, but according to the tutorial web page the default accuracy should be 54%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training for longer\n",
    "Let's see for both with the SGD and the ADAM optimizers if we train for 10 epochs what will be the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.7005764370560645\n",
      "[epoch:2] - loss: 1.3071436790537834\n",
      "[epoch:3] - loss: 1.178251290719509\n",
      "[epoch:4] - loss: 1.089817810304165\n",
      "[epoch:5] - loss: 1.0250604792380333\n",
      "[epoch:6] - loss: 0.9694220644283295\n",
      "[epoch:7] - loss: 0.9206617661905289\n",
      "[epoch:8] - loss: 0.8862931032204628\n",
      "[epoch:9] - loss: 0.8487826156067848\n",
      "[epoch:10] - loss: 0.823209721262455\n",
      "Finished Training\n",
      "####-----####\n",
      "Accuracy of the network on the 10000 test images: 62 %\n",
      "Accuracy of plane : 65 %\n",
      "Accuracy of   car : 82 %\n",
      "Accuracy of  bird : 60 %\n",
      "Accuracy of   cat : 36 %\n",
      "Accuracy of  deer : 58 %\n",
      "Accuracy of   dog : 48 %\n",
      "Accuracy of  frog : 71 %\n",
      "Accuracy of horse : 66 %\n",
      "Accuracy of  ship : 69 %\n",
      "Accuracy of truck : 60 %\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# with sgd optimizer\n",
    "sgd_net = Net().to(device)\n",
    "# the optimizer and the criterion itself\n",
    "criterion_sgd = nn.CrossEntropyLoss()\n",
    "optimizer_sgd = optim.SGD(sgd_net.parameters(), lr=0.001, momentum=0.9)\n",
    "# train finally\n",
    "sgd_net = train_net(sgd_net, optimizer_sgd, criterion_sgd, epochs=10)\n",
    "# Separator\n",
    "print('####-----####')\n",
    "# Evaluate\n",
    "net_accuracy(sgd_net)\n",
    "net_class_accuracy(sgd_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.5191178158402443\n",
      "[epoch:2] - loss: 1.2854367004466056\n",
      "[epoch:3] - loss: 1.1915172613608838\n",
      "[epoch:4] - loss: 1.127001931077242\n",
      "[epoch:5] - loss: 1.0794019720938801\n",
      "[epoch:6] - loss: 1.0408295844863356\n",
      "[epoch:7] - loss: 1.0094187392425538\n",
      "[epoch:8] - loss: 0.9858514883223176\n",
      "[epoch:9] - loss: 0.961853191379495\n",
      "[epoch:10] - loss: 0.9454264050481096\n",
      "Finished Training\n",
      "####-----####\n",
      "Accuracy of the network on the 10000 test images: 59 %\n",
      "Accuracy of plane : 61 %\n",
      "Accuracy of   car : 60 %\n",
      "Accuracy of  bird : 39 %\n",
      "Accuracy of   cat : 33 %\n",
      "Accuracy of  deer : 61 %\n",
      "Accuracy of   dog : 60 %\n",
      "Accuracy of  frog : 69 %\n",
      "Accuracy of horse : 66 %\n",
      "Accuracy of  ship : 66 %\n",
      "Accuracy of truck : 79 %\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# with adam optimizer\n",
    "adam_net = Net().to(device)\n",
    "# the optimizer and the criterion itself\n",
    "criterion_adam = nn.CrossEntropyLoss()\n",
    "optimizer_adam = optim.Adam(adam_net.parameters(), lr=0.001)\n",
    "# train finally\n",
    "adam_net = train_net(adam_net, optimizer_adam, criterion_adam, epochs=10)\n",
    "# Separator\n",
    "print('####-----####')\n",
    "# Evaluate\n",
    "net_accuracy(adam_net)\n",
    "net_class_accuracy(adam_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary**:\n",
    "\n",
    "It seems that the SGD net starts to overfit, lowering its score to 61%, while the ADAM-based net improves the score by 1%. Still Adam underperforms by comparison. A conclusion one might draw is that, while with the increase in epochs the SGD-based network starts to overfit, the ADAM-based one improves. Thus, given enough epochs, the ADAM-based network's loss one might converge to a 0 (if it doesn't overfit though...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 A different (deeper) architecture with more filters.\n",
    "What if we try a different architecture all together - a deeper fully connected layer with more convolutional layers to identify patterns in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class of the deeper network\n",
    "# Let's start with 5 conv layers\n",
    "class DNet(nn.Module):\n",
    "    '''\n",
    "    Reminder:\n",
    "    nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "    nn.Linear(in_features, out_features, bias=True)\n",
    "    nn.MaxPool2d(kernel_size, stride=None)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(DNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3)\n",
    "        self.conv3 = nn.Conv2d(12, 24, 3)\n",
    "        self.conv4 = nn.Conv2d(24, 48, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(48 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 48)\n",
    "        self.fc4 = nn.Linear(48, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 48 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1] - loss: 1.593034551138878\n",
      "[epoch:2] - loss: 1.215692552409172\n",
      "[epoch:3] - loss: 1.0668661633825303\n",
      "[epoch:4] - loss: 0.9762291455727815\n",
      "[epoch:5] - loss: 0.9142012245258689\n",
      "[epoch:6] - loss: 0.8613344285660982\n",
      "[epoch:7] - loss: 0.8153844744030945\n",
      "[epoch:8] - loss: 0.7845257074354589\n",
      "[epoch:9] - loss: 0.753998375293538\n",
      "[epoch:10] - loss: 0.7306487673864513\n",
      "Finished Training\n",
      "####-----####\n",
      "Accuracy of the network on the 10000 test images: 67 %\n",
      "Accuracy of plane : 69 %\n",
      "Accuracy of   car : 87 %\n",
      "Accuracy of  bird : 59 %\n",
      "Accuracy of   cat : 39 %\n",
      "Accuracy of  deer : 59 %\n",
      "Accuracy of   dog : 63 %\n",
      "Accuracy of  frog : 66 %\n",
      "Accuracy of horse : 79 %\n",
      "Accuracy of  ship : 77 %\n",
      "Accuracy of truck : 72 %\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Net\n",
    "dnet = DNet().to(device)\n",
    "# Train\n",
    "# the optimizer and the criterion itself\n",
    "criterion_dnet = nn.CrossEntropyLoss()\n",
    "optimizer_dnet = optim.Adam(dnet.parameters(), lr=0.001)\n",
    "# train finally\n",
    "dnet = train_net(dnet, optimizer_dnet, criterion_dnet, epochs=10)\n",
    "# Separator\n",
    "print('####-----####')\n",
    "# Evaluate\n",
    "net_accuracy(dnet)\n",
    "net_class_accuracy(dnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary:** An improvement on the basis of the initial network of 3%. Maybe a few decimals could be shaved off of the loss if we train for longer, but lets move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Using a pre-trained model.\n",
    "A way of increasing the accuracy of a CNN is to use the architecture and weights of a pre-trained model for our specific use case. I would have liked to test at least three of them, but let's start with VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers\n",
    "for parameter in model_vgg16.parameters():\n",
    "    parameter.require_grad = False\n",
    "    \n",
    "# remove the last layer\n",
    "num_features = model_vgg16.classifier[6].in_features\n",
    "features = list(model_vgg16.classifier.children())[:-1]\n",
    "# out features are 10 by design\n",
    "features.extend([nn.Linear(num_features, 10)])\n",
    "# replace the first layer as VGG accepts 224x224 images\n",
    "features[0] = nn.Linear()\n",
    "model_vgg16.classifier = nn.Sequential(*features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the loss and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimizer and the criterion itself\n",
    "criterion_vgg16 = nn.CrossEntropyLoss()\n",
    "optimizer_vgg16 = optim.Adam(model_vgg16.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=25088, out_features=4096, bias=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f35f8e031d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [4 x 512], m2: [25088 x 4096] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b5a463bd9adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_vgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# send to the gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_vgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_vgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Separator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'####-----####'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ac2dec33255c>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(nnet, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [4 x 512], m2: [25088 x 4096] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "model_vgg16.cuda() # send to the gpu\n",
    "model_vgg16 = train_net(model_vgg16, optimizer_vgg16, criterion_vgg16, epochs=10)\n",
    "# Separator\n",
    "print('####-----####')\n",
    "# Evaluate\n",
    "net_accuracy(model_vgg16)\n",
    "net_class_accuracy(model_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

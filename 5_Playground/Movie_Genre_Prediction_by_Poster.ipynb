{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Genre Prediction by Poster\n",
    "The goal of this notebook is to try to predict a movie's genre by looking at the poster it has.\n",
    "To achieve this goal I will:\n",
    "1. Use [The Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset) to get 45k movie titles.\n",
    "2. Use the [omdb_api](http://www.omdbapi.com/) to get the posters for the movies and store them localy.\n",
    "3. Use Tensorflow and Keras to create a ConvNet that will classify the images.\n",
    "\n",
    "## Project Structure\n",
    "1. Data loading, acquisition and cleaning\n",
    "2. Model creation.\n",
    "3. Model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Loading\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Loading the \n",
    "OMDB_KEY = json.loads(open('apikeys/apikey.json').read())['key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading, acquisition and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading 'The Movies' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "the_movies = pd.read_csv('./datasets/The_Movies/movies_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defining the API functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie(imdbid):\n",
    "    '''Gets the movie by imdb id and returns json with the title, genre and imdb id.\n",
    "       Input args:\n",
    "       - imdbid: the imdb id of the character\n",
    "       Returns:\n",
    "       - If the API responds succesfully return the genres, otherwise return NA\n",
    "    - \n",
    "    '''\n",
    "    url = 'http://www.omdbapi.com/?apikey=' + OMDB_KEY + '&i=' + str(imdbid)\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        movie_json = r.json()\n",
    "        return movie_json['Genre']\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "def get_poster(imdb_id, path, genre, dataset_type, image_heigth, item_id):\n",
    "    '''Gets the movie poster as a jpg and saves it to the path.\n",
    "       Input args:\n",
    "       - imbd_id: the imdb id of the movie,\n",
    "       - path: where the dataset will be stored,\n",
    "       - genre: the genre of the movie,\n",
    "       - dataset_type: whether we are creating the training or testing,\n",
    "       - image_height: the height of the poster - api query\n",
    "       - item_id: identifier to attach to the filename\n",
    "       Returns:\n",
    "       - None\n",
    "    '''\n",
    "    # construct the requests url\n",
    "    url = 'http://img.omdbapi.com/?apikey=' + OMDB_KEY + '&i=' + str(imdb_id) + '&h=' + str(image_heigth)\n",
    "    # create the dir where we store the posters based on the genre and the dataset type\n",
    "    path = os.path.join(os.getcwd(), path, dataset_type, genre + '/')\n",
    "    # check if the folder is already_created\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    r = requests.get(url)\n",
    "    filename = os.path.join(os.getcwd(), path, genre + '.' + str(item_id) + '.jpg')\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as w:\n",
    "            w.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Downloading the movie posters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a movie can have multiple genres I will make an assumption here - if a genre is listed first - that will be the genre of the movie. Let's create a function that will get only the main genre of the movie from the dataset. If it doesn't find one - download it straight from imdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def get_main_genre(dataset_genre, imdb_id):\n",
    "    '''\n",
    "        Gets the main genre of the movie. If there is none listed - pull one directly from imdb.\n",
    "        Input args:\n",
    "        - genre_row: the dataset value of the genres for the movie,\n",
    "        - imdb_id: the imdb_id of the movie\n",
    "        Returns:\n",
    "        - main_genre: the main genre of the movie \n",
    "    '''\n",
    "    dataset_genre = ast.literal_eval(dataset_genre)\n",
    "    if len(dataset_genre) == 0:\n",
    "        try: \n",
    "            main_genre = get_movie(imdb_id).replace(' ', '').split(',')[0]\n",
    "        except KeyError:\n",
    "            main_genre = 'NA'\n",
    "    else:\n",
    "        main_genre = dataset_genre[0]['name']\n",
    "        if main_genre == 'N/A':\n",
    "            main_genre = 'NA'\n",
    "    return main_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER: Slow Code!\n",
    "the_movies['main_genre'] = the_movies.apply(lambda x: get_main_genre(x['genres'], x['imdb_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_movies['main_genre'].unique().shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick fix for N/As\n",
    "the_movies[the_movies['main_genre'] == 'N/A'] = 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all we got 31 classes. Let's create the train - test split now, as we will need it for movie poster download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "movies_train, movies_test = train_test_split(the_movies, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally downloading the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the posters for the train set\n",
    "for idx, row in movies_train.iterrows():\n",
    "    # get_poster(imdb_id, path, genre, dateset_type, image_heigth, item_id):\n",
    "    get_poster(row['imdb_id'], 'datasets/The_Movies/posters', row['main_genre'], 'train', 600, idx)\n",
    "# downloading the posters for the test set\n",
    "for idx, row in movies_test.iterrows():\n",
    "    get_poster(row['imdb_id'], 'datasets/The_Movies/posters', row['main_genre'], 'test', 600, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has been moved to a script so it can stay overnight and download all of the movies' posters. \n",
    "Since that has been done lets move on to what genres we have actually downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fantasy',\n",
       " 'Drama',\n",
       " 'Biography',\n",
       " 'Animation',\n",
       " 'Science Fiction',\n",
       " 'Romance',\n",
       " 'Crime',\n",
       " 'Adventure',\n",
       " 'History',\n",
       " 'War',\n",
       " 'Foreign',\n",
       " 'Documentary',\n",
       " 'Thriller',\n",
       " 'Music',\n",
       " 'Action',\n",
       " 'Western',\n",
       " 'Short',\n",
       " 'Family',\n",
       " 'Horror',\n",
       " 'Comedy',\n",
       " 'Mystery',\n",
       " 'Musical',\n",
       " 'TV Movie']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/media/fury/data/Scripts/the_movies_data_scraper/datasets/The_Movies/posters/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list, displayed above we can see somethings we can edit/remove. For example \n",
    "- Sci-Fi and Science Fiction are the same thing so both dirs can be merged to a common category 'Sci-Fi'\n",
    "- 'Aniplex', 'Odyssey Media' and 'Carousel Productions' are actually names of companies. Will see how much samples they contribute and if it is not significant (~100 or more)they will be deleted.\n",
    "- NA - The same treatment - check the number of samples and delete them.\n",
    "\n",
    "#### Results\n",
    "- Sci-Fi - it seems that there are only 5 posters in Sci-Fi - thus I can merge it into 'Science Fiction'\n",
    "- 'Odyssey Media' and 'Carousel Productions' and 'NA' contained no information whatsover.\n",
    "- 'Removed also 'Adult' as it contained no posters.\n",
    "\n",
    "#### Aditional Checks performed\n",
    "- Whether both dirs contain the same genres and the same number of genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model creation.\n",
    "\n",
    "### 2.1 Importing the neccesarry libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Loading the data.\n",
    "With the help of Keras that should be quite straightforward. The ImageDataGenerator can be used to load the data and get the labels simultaneously in a neat 2D one-hot encoded format. In addition it allows us to generate additional train/validation/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23273 images belonging to 23 classes.\n",
      "Found 12494 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_location = '/media/fury/data/Scripts/the_movies_data_scraper/datasets/The_Movies/posters/train'\n",
    "test_location = '/media/fury/data/Scripts/the_movies_data_scraper/datasets/The_Movies/posters/test'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# The generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_location,\n",
    "    target_size = (300, 300),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 8,\n",
    "    class_mode = 'categorical',\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_location,\n",
    "    target_size = (300, 300),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 8,\n",
    "    class_mode = 'categorical',\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le model\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (3,3), input_shape = (300, 300, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 23, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 298, 298, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 149, 149, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 710432)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               90935424  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 23)                2967      \n",
      "=================================================================\n",
      "Total params: 90,939,287\n",
      "Trainable params: 90,939,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Why is ADAM the optimizer?](https://arxiv.org/pdf/1609.04747.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le compile\n",
    "classifier.compile(optimizer = 'adam', \n",
    "                   loss = 'categorical_crossentropy', \n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1734s 217ms/step - loss: 1.0653 - acc: 0.3639 - val_loss: 1.1921e-07 - val_acc: 0.1100\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1732s 217ms/step - loss: 1.1921e-07 - acc: 0.1050 - val_loss: 1.1921e-07 - val_acc: 0.1102\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1734s 217ms/step - loss: 1.1921e-07 - acc: 0.1054 - val_loss: 1.1921e-07 - val_acc: 0.1106\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 1739s 217ms/step - loss: 1.1921e-07 - acc: 0.1049 - val_loss: 1.1921e-07 - val_acc: 0.1096\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 1737s 217ms/step - loss: 1.1921e-07 - acc: 0.1051 - val_loss: 1.1921e-07 - val_acc: 0.1107\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 1739s 217ms/step - loss: 1.1921e-07 - acc: 0.1052 - val_loss: 1.1921e-07 - val_acc: 0.1105\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 1745s 218ms/step - loss: 1.1921e-07 - acc: 0.1052 - val_loss: 1.1921e-07 - val_acc: 0.1100\n",
      "Epoch 8/10\n",
      "6036/8000 [=====================>........] - ETA: 6:02 - loss: 1.1921e-07 - acc: 0.1053"
     ]
    }
   ],
   "source": [
    "classifier.fit_generator(train_generator,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 10,\n",
    "                         validation_data = test_generator,\n",
    "                         validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very low accuracy, a little bit above random picking... Bias needs to be improved. But first - let's create a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offtopic: Creating the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_posters(dataset, location):\n",
    "    num_items = 0\n",
    "    for item in os.listdir(location):\n",
    "        num_posters =len(os.listdir(os.path.join(location, item)))\n",
    "        num_items += num_posters\n",
    "\n",
    "    print(\"%s: %i\" % (dataset, num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 23273\n",
      "test: 12494\n"
     ]
    }
   ],
   "source": [
    "get_total_posters('train', train_location)\n",
    "get_total_posters('test', test_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_data_to_validation(perc, location, target_location):\n",
    "    '''\n",
    "    Move a certain percentage of the target dataset to a new location.\n",
    "    The percentage is drawn from each genre.\n",
    "    '''\n",
    "    for genre in os.listdir(location):\n",
    "        dir_contents = os.listdir(os.path.join(location, genre))\n",
    "        dir_len = len(dir_contents)\n",
    "        num_items_to_take = int(np.ceil(perc * dir_len / 100))\n",
    "        # Randomize the draw with a integeres drawn from descrete uniform\n",
    "        items_idx_array = np.random.choice(dir_len, num_items_to_take, replace = False)    \n",
    "        \n",
    "        for idx in items_idx_array:\n",
    "            src = os.path.join(location, genre, dir_contents[idx])\n",
    "            dst_dir = os.path.join(target_location, genre)\n",
    "            if not os.path.exists(dst_dir): # make sure that the path exists\n",
    "                os.makedirs(dst_dir)\n",
    "            move(src, os.path.join(dst_dir, dir_contents[idx]))\n",
    "            \n",
    "    print('Done!') # just because"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# lets move data\n",
    "validation_location = '/media/fury/data/Scripts/the_movies_data_scraper/datasets/The_Movies/posters/validation'\n",
    "move_data_to_validation(20, train_location, validation_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_location = '/media/fury/data/Scripts/the_movies_data_scraper/datasets/The_Movies/posters/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 23273\n",
      "test: 12494\n",
      "validation: 5832\n"
     ]
    }
   ],
   "source": [
    "# Getting the number of items in each dir\n",
    "get_total_posters('train', train_location)\n",
    "get_total_posters('test', test_location)\n",
    "get_total_posters('validation', validation_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5832 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "# generating the validation dataset\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# The generators\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_location,\n",
    "    target_size = (300, 300),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 8,\n",
    "    class_mode = 'categorical',\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 cntd. Model Building.\n",
    "Since we saw that the model is underfitting there are a few steps we could try out:  \n",
    "* CNN Architecture - maybe if we made the model a little deeper...\n",
    "* Create more data - In my opinion there are two approaches that could be examined:  \n",
    "    A. Blindly creating more samples - just create more images, irregardles of their class.   \n",
    "    B. Try to balance out the classess by only cropping the images to generate additional ones.  \n",
    "* Train for longer - not clear on the results though as it seems between the epochs no significant improvement is made.\n",
    "* Tune the hyperparameters - maybe use grid search?\n",
    "\n",
    "Let's go over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Going deeper..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le model 2\n",
    "deep_clf = Sequential()\n",
    "deep_clf.add(Conv2D(32, (3,3), input_shape = (300, 300, 3), activation = 'relu'))\n",
    "deep_clf.add(MaxPooling2D(pool_size = (2,2)))\n",
    "deep_clf.add(Flatten())\n",
    "deep_clf.add(Dense(units = 92, activation = 'relu'))\n",
    "deep_clf.add(Dense(units = 92, activation = 'relu'))\n",
    "deep_clf.add(Dense(units = 23, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 298, 298, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 149, 149, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 710432)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 92)                65359836  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 92)                8556      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 23)                2139      \n",
      "=================================================================\n",
      "Total params: 65,371,427\n",
      "Trainable params: 65,371,427\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_clf.compile(optimizer = 'adam', \n",
    "                   loss = 'categorical_crossentropy', \n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 124s 124ms/step - loss: 7.4215 - acc: 0.2780 - val_loss: 2.3491 - val_acc: 0.2625\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 2.3311 - acc: 0.2689 - val_loss: 2.3110 - val_acc: 0.2831\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 2.2851 - acc: 0.2813 - val_loss: 2.3277 - val_acc: 0.2756\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 111s 111ms/step - loss: 2.0310 - acc: 0.3644 - val_loss: 2.6231 - val_acc: 0.2600\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 103s 103ms/step - loss: 1.9960 - acc: 0.3892 - val_loss: 2.5229 - val_acc: 0.2525\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 103s 103ms/step - loss: 1.7785 - acc: 0.4515 - val_loss: 3.2409 - val_acc: 0.2406\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 103s 103ms/step - loss: 0.7703 - acc: 0.7869 - val_loss: 4.2800 - val_acc: 0.2406\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 103s 103ms/step - loss: 0.7321 - acc: 0.8011 - val_loss: 4.1567 - val_acc: 0.2213\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 103s 103ms/step - loss: 0.6182 - acc: 0.8347 - val_loss: 5.9281 - val_acc: 0.2144\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 103s 103ms/step - loss: 0.2958 - acc: 0.9335 - val_loss: 6.0146 - val_acc: 0.2206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0f1580780>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_clf.fit_generator(train_generator,\n",
    "                         steps_per_epoch = 1000,\n",
    "                         epochs = 10,\n",
    "                         validation_data = validation_generator,\n",
    "                         validation_steps = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait.. what? Bring on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

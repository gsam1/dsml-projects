{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Genre Prediction by Poster\n",
    "The goal of this notebook is to try to predict a movie's genre by looking at the poster it has.\n",
    "To achieve this goal I will:\n",
    "1. Use [The Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset) to get 45k movie titles.\n",
    "2. Use the [omdb_api](http://www.omdbapi.com/) to get the posters for the movies and store them localy.\n",
    "3. Use Tensorflow and Keras to create a ConvNet that will classify the images.\n",
    "\n",
    "## Project Structure\n",
    "1. Data loading, acquisition and cleaning\n",
    "2. Model creation.\n",
    "3. Model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Loading\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Loading the \n",
    "OMDB_KEY = json.loads(open('apikeys/apikey.json').read())['key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading, acquisition and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading 'The Movies' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "the_movies = pd.read_csv('./datasets/The_Movies/movies_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defining the API functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie(imdbid):\n",
    "    '''Gets the movie by imdb id and returns json with the title, genre and imdb id.\n",
    "       Input args:\n",
    "       - imdbid: the imdb id of the character\n",
    "       Returns:\n",
    "       - If the API responds succesfully return the genres, otherwise return NA\n",
    "    - \n",
    "    '''\n",
    "    url = 'http://www.omdbapi.com/?apikey=' + OMDB_KEY + '&i=' + str(imdbid)\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        movie_json = r.json()\n",
    "        return movie_json['Genre']\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "def get_poster(imdb_id, path, genre, dataset_type, image_heigth, item_id):\n",
    "    '''Gets the movie poster as a jpg and saves it to the path.\n",
    "       Input args:\n",
    "       - imbd_id: the imdb id of the movie,\n",
    "       - path: where the dataset will be stored,\n",
    "       - genre: the genre of the movie,\n",
    "       - dataset_type: whether we are creating the training or testing,\n",
    "       - image_height: the height of the poster - api query\n",
    "       - item_id: identifier to attach to the filename\n",
    "       Returns:\n",
    "       - None\n",
    "    '''\n",
    "    # construct the requests url\n",
    "    url = 'http://img.omdbapi.com/?apikey=' + OMDB_KEY + '&i=' + str(imdb_id) + '&h=' + str(image_heigth)\n",
    "    # create the dir where we store the posters based on the genre and the dataset type\n",
    "    path = os.path.join(os.getcwd(), path, dataset_type, genre + '/')\n",
    "    # check if the folder is already_created\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    r = requests.get(url)\n",
    "    filename = os.path.join(os.getcwd(), path, genre + '.' + str(item_id) + '.jpg')\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as w:\n",
    "            w.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Downloading the movie posters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a movie can have multiple genres I will make an assumption here - if a genre is listed first - that will be the genre of the movie. Let's create a function that will get only the main genre of the movie from the dataset. If it doesn't find one - download it straight from imdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def get_main_genre(dataset_genre, imdb_id):\n",
    "    '''\n",
    "        Gets the main genre of the movie. If there is none listed - pull one directly from imdb.\n",
    "        Input args:\n",
    "        - genre_row: the dataset value of the genres for the movie,\n",
    "        - imdb_id: the imdb_id of the movie\n",
    "        Returns:\n",
    "        - main_genre: the main genre of the movie \n",
    "    '''\n",
    "    dataset_genre = ast.literal_eval(dataset_genre)\n",
    "    if len(dataset_genre) == 0:\n",
    "        try: \n",
    "            main_genre = get_movie(imdb_id).replace(' ', '').split(',')[0]\n",
    "        except KeyError:\n",
    "            main_genre = 'NA'\n",
    "    else:\n",
    "        main_genre = dataset_genre[0]['name']\n",
    "        if main_genre == 'N/A':\n",
    "            main_genre = 'NA'\n",
    "    return main_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER: Slow Code!\n",
    "the_movies['main_genre'] = the_movies.apply(lambda x: get_main_genre(x['genres'], x['imdb_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_movies['main_genre'].unique().shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick fix for N/As\n",
    "the_movies[the_movies['main_genre'] == 'N/A'] = 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all we got 31 classes. Let's create the train - test split now, as we will need it for movie poster download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "movies_train, movies_test = train_test_split(the_movies, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally downloading the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the posters for the train set\n",
    "for idx, row in movies_train.iterrows():\n",
    "    # get_poster(imdb_id, path, genre, dateset_type, image_heigth, item_id):\n",
    "    get_poster(row['imdb_id'], 'datasets/The_Movies/posters', row['main_genre'], 'train', 600, idx)\n",
    "# downloading the posters for the test set\n",
    "for idx, row in movies_test.iterrows():\n",
    "    get_poster(row['imdb_id'], 'datasets/The_Movies/posters', row['main_genre'], 'test', 600, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has been moved to a script so it can stay overnight and download all of the movies' posters. \n",
    "Since that has been done lets move on to what genres we have actually downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fantasy',\n",
       " 'Drama',\n",
       " 'Biography',\n",
       " 'Animation',\n",
       " 'Science Fiction',\n",
       " 'Romance',\n",
       " 'Crime',\n",
       " 'Adventure',\n",
       " 'History',\n",
       " 'War',\n",
       " 'Foreign',\n",
       " 'Documentary',\n",
       " 'Thriller',\n",
       " 'Music',\n",
       " 'Action',\n",
       " 'Western',\n",
       " 'Short',\n",
       " 'Family',\n",
       " 'Horror',\n",
       " 'Comedy',\n",
       " 'Mystery',\n",
       " 'Musical',\n",
       " 'TV Movie']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/media/fury/data/Scripts/the_movies_data_scraper/datasets/The_Movies/posters/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list, displayed above we can see somethings we can edit/remove. For example \n",
    "- Sci-Fi and Science Fiction are the same thing so both dirs can be merged to a common category 'Sci-Fi'\n",
    "- 'Aniplex', 'Odyssey Media' and 'Carousel Productions' are actually names of companies. Will see how much samples they contribute and if it is not significant (~100 or more)they will be deleted.\n",
    "- NA - The same treatment - check the number of samples and delete them.\n",
    "\n",
    "#### Results\n",
    "- Sci-Fi - it seems that there are only 5 posters in Sci-Fi - thus I can merge it into 'Science Fiction'\n",
    "- 'Odyssey Media' and 'Carousel Productions' and 'NA' contained no information whatsover.\n",
    "- 'Removed also 'Adult' as it contained no posters.\n",
    "\n",
    "#### Aditional Checks performed\n",
    "- Whether both dirs contain the same genres and the same number of genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model creation.\n",
    "\n",
    "### 2.1 Importing the neccesarry libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Loading the data.\n",
    "With the help of Keras that should be quite straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29105 images belonging to 23 classes.\n",
      "Found 12494 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_location = '/media/fury/data/Scripts/the_movies_data_scraper/datasets/The_Movies/posters/train'\n",
    "test_location = '/media/fury/data/Scripts/the_movies_data_scraper/datasets/The_Movies/posters/test'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# The generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_location,\n",
    "    target_size = (300, 300),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_location,\n",
    "    target_size = (300, 300),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le model\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (3,3), input_shape = (300, 300, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 23, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 298, 298, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 149, 149, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 710432)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               90935424  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 23)                2967      \n",
      "=================================================================\n",
      "Total params: 90,939,287\n",
      "Trainable params: 90,939,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le compile\n",
    "classifier.compile(optimizer = 'adam', \n",
    "                   loss = 'categorical_crossentropy', \n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1734s 217ms/step - loss: 1.0653 - acc: 0.3639 - val_loss: 1.1921e-07 - val_acc: 0.1100\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1732s 217ms/step - loss: 1.1921e-07 - acc: 0.1050 - val_loss: 1.1921e-07 - val_acc: 0.1102\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1734s 217ms/step - loss: 1.1921e-07 - acc: 0.1054 - val_loss: 1.1921e-07 - val_acc: 0.1106\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 1739s 217ms/step - loss: 1.1921e-07 - acc: 0.1049 - val_loss: 1.1921e-07 - val_acc: 0.1096\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 1737s 217ms/step - loss: 1.1921e-07 - acc: 0.1051 - val_loss: 1.1921e-07 - val_acc: 0.1107\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 1739s 217ms/step - loss: 1.1921e-07 - acc: 0.1052 - val_loss: 1.1921e-07 - val_acc: 0.1105\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 1745s 218ms/step - loss: 1.1921e-07 - acc: 0.1052 - val_loss: 1.1921e-07 - val_acc: 0.1100\n",
      "Epoch 8/10\n",
      "6036/8000 [=====================>........] - ETA: 6:02 - loss: 1.1921e-07 - acc: 0.1053"
     ]
    }
   ],
   "source": [
    "classifier.fit_generator(train_generator,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 10,\n",
    "                         validation_data = test_generator,\n",
    "                         validation_steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to improve the results:\n",
    "1. Split the training into 2 parts - 80% Training/ 20% validation (holdout) - the idea is to spotcheck whether the model under/over fits.\n",
    "2. Use the ImageDataGenerator to create more samples - In my opinion there are two approaches that could be examined:\n",
    "    2.1 Blindly creating more samples - just create more images, irregardles of their class.\n",
    "    2.2 Try to balance out the classess by only cropping the images to generate additional ones, but only for classes with less than 1000 samples - any other image may polute the data\n",
    "3. Hyperparameter tunning - grid search maybe?\n",
    "4. Different architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
